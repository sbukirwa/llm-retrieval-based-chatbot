SPEAKER 0
Sort of picks up where we left off last week and talked about biases in AI. And today, actually, I want to focus a little bit more on what are the sorts of things we can do about the biases that we've seen in, in, in AI and and in data in particular. And so the lecture will be consisting of three parts looking at some of the techniques, what we can do with bias data in terms of the data, and then talking about crowdsourcing in particular and how it relates to some of the problems we're seeing. And then finally, algorithmic solutions on transparency and on how and what we could do in a sort of technical way about de biasing data. So one thing to think about when you think about the and the problems we see with data for AI in particular is think about it as an ecosystem ecosystem consisting of people that generate the data or that collect the data in the first place. These can be labourers or crowdsourced or crop workers, and we'll talk a bit more about crowdsourcing today. They can also be researchers like us, like folks who are working at universities, for instance. And so it's important to kind of think about what are their incentives, Are they being paid to do this work? Are they doing it, you know, for research? Of course, researchers are being paid as well. But, you know, it's their job to do research, not just their job to collect data as such. So it's kind of important to think about about these these sorts of issues. Then who uses the data? And that's the other side of it. I talked about who generates or collects the data. Now think about who uses the data and for what purposes. These are. These are often not the same kind of people. So we have, for instance, called advertisers. People want to, you know, or offer advertising as a service for companies. They obviously have different types of incentives. What they want to get out of the data. Again, we have researchers who might use the data as part of their research, but we also may have governments who will be using the data to understand things about their well, thinks about the things about the people in their constituent constituencies and things like that. We also have past law enforcement, and we've seen some some bad examples of how law enforcement might might use data. And we have other kinds of service sector folks like, again, like sales. So companies might want to use data to personal data to sell products to, you know, data, its itself thinking about the data itself, then there's a whole bunch of things we, we we can take into consideration when we're thinking about some of the potential issues around this can largely be quantified in terms of the quality of the data. So how representative is the data? If it's data set about a certain population, is it even representative of the population, or is it, as we have often seen in the case of biased data, the problem stems from the fact that it's not representative of, say, a diverse sample of the population, but it represents a small sector of the population and therefore it doesn't easily apply to all of the population. Coverage is a similar kind of issue where perhaps the dataset doesn't actually cover the types of concerns that are important for for the particular problems that might be the data might apply to validity. So how valid even is the data and what it describes perhaps the way in which it was measured was faulty and therefore the data itself is invalid Sources Where does the data come from? And so we've, you know, alluded to the fact that it can be created by labels or, you know, people who are like creating labels, created crowd workers, work for creating these. But there are of course other types. A lot of datasets are really cobbled together, let's say taken from multiple sources and created created into a new dataset. And errors can occur in the process of that. What are the intended uses of the datasets? I think oftentimes we see problems stemming from the fact that data is being used for different purposes than it. Originally collected that is originally intended for. And it's important to think about as well and context. I'm not going to go into detail, into too much detail now about about these ones here. But the reliability of the data preprocessing and from the NLP pipeline, you already know that there's quite a few steps involved in pre-processing and that also errors can occur as part of that process. And different choices can be made as part of pre-processing as well and on the side of the systems. So this is, broadly speaking, algorithms or systems that make use of the data. And in an automated fashion, what techniques are these systems using? What kind of algorithms are they using? What's training data is it based on? So if it's, for instance, using datasets, it's using a different different kinds of training data, then the data it might be using to to solve, to apply for whatever problem it's looking at sort of test data. And are they compatible? What purposes are these algorithms deployed for? Again, it's kind of a question of what are the intentions of these systems and so on. So the point of this slide is to give you a broad understanding and just this is not a complete picture, but to give you a broader appreciation of the types of factors, if you want the types of different types of of entities involved in such a such an ecosystem. So in dealing with biased data now let's think about what we can do about it. We've already touched a little bit on that, but amongst yourselves, discuss for 3 minutes and then share with it share with the class. After all, you've heard now learnt about biased data. What can we do about it? And yeah, talk amongst yourselves for 3 minutes and then share with us. Okay, That's going to jump to the start again, isn't it? Keep doing that for me. Okay. So the first sort of idea I wanted to mention was very similar to your suggestion, which was basically an approach to to check the data quality. And there are there's this is this is research actually before I do that, that that sort of highlights issues of ecological validity of data sets. So what is ecological validity? Well, this definition is here on the slides. It's essentially the extent to which study findings or data generalises to real world settings. So it's essentially a way to think about whether the data that you have actually represents is representative of of a problem in the real world. And so this is a this is a concern because a lot of the time data is generated and that the way that data is generated doesn't always reflect the real issues that actually in this case, the researchers want to want of kind of the data to be about. All right. So here's an example. The example that that these authors looked at was real callers versus simulated callers. And so they essentially wanted to compare compare the differences in speech between real callers. So this is in the context of a call centre, right where they want to understand what are the issues that real callers bring up and encounter when they interact with a call centre automated system. And to help, you know, to help with that kind of problem solving, the call centre would also have simulated callers. So testers essentially, right? So like stuff like software testing, effectively they would have these callers would simulate going through a troubleshooting process. And what they found was differences in speech so that the participants that were recruited, so the simulated simulated callers, they would talk more and they would speak faster, whereas real users so people actually needed help. They asked for more help and they interrupt the system more than there were some significant differences. And it's just a point to show that whenever you encounter data sets, it matters the context in which the dataset was created. And in this case, it shows there are some significant differences between well, between the kind of the real the real context of of people asking and using a telephone call centre compared with a simulated one where people are recruited to test the system. So this is then more like the some of the suggestions we heard earlier. It's just this idea of data statements. The idea of data statements is to mitigate bias, not to get rid of it in the data, but to mitigated by being transparent about it by being open about it. A data statement is a characterisation of a data set that provides the context to allow developers and users to better understand different aspects of the data sets, including how the experimental results might generalise, how the software might be appropriately deployed. So if there is, for instance, a software system that is being presented as part of the work, then it's important to me to think through what are the, let's say, what are the intended use cases for that software and, and being quite open about it so that somebody else takes that software, They know the context, they understand, appreciate the context in which that should be deployed for what problems that that algorithm should be deployed for instance. And then also what biases might be reflected in systems built on that software. So the idea of data statements is that in this case, this is aimed at the research community. They create an algorithm, they provide a statement going through these these characterisations to help users of that systems better understand and use and hopefully in an inappropriate way. And there is data sheets. This is taking that and kind of formalising this idea of transport of data statements, formalising it in the context of an it's basically a template to suggest to people to complete with respect to the dataset. And here's an example. You can actually look at it if you want. It's on GitHub. This is about the. This is a datasheet for the hate speech Twitter dataset, which these researchers compiled and make it available on GitHub for research purposes. And the data sheet has these headings in capital letters here. So curation rationale. What what was the rationale to compile this dataset in this case to study the automatic detection of hate speech? It mentions what language variety is in the dataset. It's how and when it's from. So it's used the Twitter Search API In late 2015, information about which varieties of English are represented is not available, but at least Australian and US and mainstream English are both included. You can see that this kind of information could be important because it says something about the context that the dataset is about. And then there is this idea of A, B, c, p, four seven tag, which is about the best current practices for identifying languages as a link. If you want to understand more about that, some other things that are included in the datasheet are speaker demographic. So this in this case, speakers were not directly approached, so couldn't be asked for demographic information. Why? Because there's a Twitter dataset of course, and so there's not much information about what's available. There is then the annotated demographic, which the annotators are the the label is the crowd workers, right. That completed this dataset and this dataset was annotated by both crowd workers and experts at and so on. This data here about the number of Kraut workers, how they were recruited through a system called crowd flower, where forms were the label is from primarily Europe, South America and North America. The expert ants were recruited specifically for their understanding of intersectional feminism, and they ranged ranged in age from 20 to 40 years and included and so on. So you can see how this information is also important. Really, when you think about datasets who were actually the people that provided the information, the labels that are then being used. If you are attempting to automate, in this case, hate speech classification and then something called a speech situation, which which provides some some more contextual information about the data, in this case, the tweets that are part of the dataset, what was the sort of date range for first data and what are they? What are they about? And. So this just to summarise what the states are about, they are another approach to provide massive data for transparency. This is the reference, by the way, of data sheets for data sets and the motivation of of this is to. Yeah, I guess you've got we've got that from the example to understand why it was collected, who collected it and how was it funded and so on. Understand the composition. How many instances are there, how was it sampled? How was the data split? What is the collection process? Again, you've seen that from from the example. How was it collected again? How was the metadata assigned? What sort of IAB is Institutional Research Board? And that's a US term for ethics, for ethics approval and what timeline and so on. So you can see how attaching this kind of information doesn't help with, you know, getting rid of any bias that might be in the data. But it would hopefully identify, help identify that there is bias and sort of acknowledge in any reuses of the data. So in part two, I'm going to talk about crowdsourcing. So what is crowdsourcing and how can it be used in the context of natural language processing? So who who has heard of crowdsourcing? One. Not that many of you. And who wants to start? Tell us what crowdsourcing is. Do you want to?

SPEAKER 1
I know what it is.

SPEAKER 0
But it's okay doing it. That's fine. How can it be used in the context of an LP? Any ideas? Okay. It's a mystery. Well, then I will answer that in context. So what is crowdsourcing? So someone coined this phrase for it. Artificial, artificial intelligence. Very clever, isn't it? So it's essentially it's essentially a process through which you outsource processes and jobs to a distributed workforce called the crowd workers usually or. Well, yeah, yeah. Or labourers sometimes who can perform these tasks virtually usually online and that's usually facilitated via a marketplace or an online platform such as Amazon Mechanical Turk. Has anyone heard of. AM-T Amazon Mechanical Turk Some of you have, most of you have not. Crowd Flowers Another one and there are others. But this is yeah, as I mentioned, this is commonly employed to obtain label data to train A.I. systems. So, so okay, so that then explains how it could be used in NLP. Right. So how, how does this translate? Where do we need label data and LP? One. So who's got an idea of that? Yeah, of course. Sorry.

SPEAKER 1
You guys are.

UNKNOWN
Just talking about this all the time.

SPEAKER 0
Yeah, that's the way of labelling. Yeah, tagging is another word, I think, for labelling. So. So what? What, what is the process of labelling? Why do we need to label data? Is it.

SPEAKER 1
Just so is it that we need to check that you need to gather all that data and then just break it down to different parts like what.

SPEAKER 0
We are, but what this pulls? What do you mean by what this falls under?

SPEAKER 1
Like what category of categories?

SPEAKER 0
Right, exactly. So what is what is an example of a category in an LP that we why we need labelling. It's not a trick question. It's an easy one, really. And so part of speech, right? So nouns, verbs, etc., these are these are categories of words. So in this case and of course, you know, the data itself doesn't tell us what kind of what it is. It doesn't tell us it's a noun. It doesn't tell us it's a verb. But the grammatical constructs in which you know, the context in which language is used. And of course, it's not something you can get from the language, from the words themselves. Right. That's why we do part speech tagging somebody does it. So you've used in the last you've used part of speech sort of tax sets. Right. So somebody else has already done the work for you, just reusing the tax sets. But that is not something that is automatically created by an algorithm. That's something that is created by, for instance, labourers like crowd workers. That's the context in which crowd work. Crowdsourcing is is relevant and important to natural language processing. Okay. And this is just a diagram. So as you can as you could imagine how the process works of recruiting workers via a marketplace like took on the kind of Amazon Mechanical Turk you have on one side, you have the requesters that have tasks that need to be completed. So, for instance, a researcher wanting to create a dataset or one to get labels, part of speech tagging for a set of, of, of a dataset that they want to get tagged for their NLP system. Right? And then on the other hand, you have workers that actually want to earn money and work on interesting tasks. So this is that's why it's a marketplace, because, you know, money's exchanging hands. The workers, you know, they gain they gain some some money, some wage from from working on these so-called micro tasks that are being given on these platforms. Yes.

SPEAKER 1
But.

SPEAKER 0
Yeah. Like, for instance, as I mentioned, part of speech tagging of a dataset for natural language system.

SPEAKER 1
If you have comfortable asking. How.

SPEAKER 0
Okay. Yeah. So I'll give you another example. I think this is a this is the the kind of the example of that you're familiar with from the Web, which is image search. Right. So while it's by some sort of magic, the Internet understands which picture to show you when you search for cats, for dogs. Right. So how does that work? By tagging, right? There's been by by essentially having lots of examples of images of cats and dogs being labelled by people as cats and dogs. So a task is you get you get shown an image with a cat. On the on one of these micro tasks, and you have to label it as either cat or dog or whatever. Right. So very simple, like example of what a microsoft looks like. Yeah, I think probably one of the most classic examples of of the context in which crowdsourcing and labelling was first invented, let's say. Yeah. Crowdsourcing is a fascinating is a fascinating area of of computing actually when we're not talking about it too much in this in this context, but you get a little bit of an idea. So when it comes to crowdsourcing thing that we've got to be mindful of is you know, these are people doing tasks and they can make mistakes. They might be in a rush, you know, they might want to get through lots of tasks. So they'll be doing them very quickly, right. Because they get paid per task. So the context in which this labelling and crowdsourcing actually takes place is one in which mistakes can happen and a quality may be compromised of the data that you get as a result. And this paper here does a literature survey looking at crowdsourcing practices and they are asking questions of the datasets because there's a lot of research and also development of systems based on the work of crowdsourcing or crowd workers. And actually a lot of the time these questions are not clear. So where are the labels from? Are they from an external datasets dataset or were they relabelled? Who were the labels? Right. This question about where they were the experts were that Kraut workers or so. But so the assumption is that Kraut workers are not experts in a specific domain, but they just do lots of labelling tasks. How are they trained? You know, have they been given adequate training, of course, to distinguish a picture of cats and dogs who don't need training, but for more complex tasks, meaning training and the all the training examples given in the paper, how were they screened? So how, how was the quality of the labelled data set established and how were they compensated? So overall the authors find that there is some concern, given how crucial the quality of the training data is and and also the difficulty of standardising human judgement. Right. So the point of why I'm talking about this is, is because a lot of the ways in which the systems work is based on crowdsourced data and crowdsourced labels. And therefore you need to be aware that mistakes happen as part of that process. And therefore these the labels and the datasets are not unlikely to be 100% correct. Another aspect to bring up in this context is the crowd work. It's that craftwork is work as well, right? So and often it's a type of work that is you could characterise as a call centre type work and is often being shipped out, let's say, into low income countries where you can have, you know, offices full of people working for little money for on the on these, on these crowds tasks. And they often work actually in precarious working conditions. So what would happen is, is that in these call centre type offices, people are actually being hired by, by a company which is essentially just employing them and paying them less. And the company will take the income basically coming coming in for the tasks by the by by the people who are providing the tasks. They will take it and then give a very small cut of that to those people who are working in these in these type of work settings. Then in that in that sort of context, the workers, you know, rarely have a right to holiday. They don't have health insurance or minimum wage and do not have a union that represents them and they don't have protections from rogue employers and such. So we've got to be aware of these issues that exist when we use crowdsourcing platforms to to gather data for our research or for our development projects. There is there's some research that tries to deal with that. This work called Tech Optical recognises this. The issue that there are rogue employers, as I mentioned, and actually adds a functionality to Amazon Mechanical Turk to avoid dodgy employers. So the the other the other kind of issue that you get with Amazon Mechanical Turk is that you get some people who are basically providing the micro tasks are not actually willing to pay for them a lot of the time. And so they would then say that while the tasks haven't been done correctly, when actually to have and as a result of that not pay the crowd workers as well. So then you have this is essentially a labour review system that people created for for the requesters. Right. So, so the crowd workers can actually review the requesters and as a result of that, hopefully avoid the negative employers. So here's another paper that kind of thinks about this problem of how crowd workers are treated and how we might improve them, their treatment to be to be done in more fair ways. This system that they that they propose. So they come up with a technical solution. They call this Fair Work, which automatically pays the Amazon mechanical checkout workers a minimum wage. And this is essentially a technical solution to a social problem here. Another another sort of piece of research. Looking at fairness in crowdsourcing, these offers propose a framework that allocates market tasks based on this kind of based on human factors of workers such as demographics and compensation. Yeah. Yeah.

SPEAKER 1
When you say minimum wage, is it in the country of the company or country of the customers?

SPEAKER 0
Yes. So that's a good question. I think in this case, it's what they're they're they're not I think they're not really dealing with that problem in this work. I think they are assuming that both the the workers and the people paying for it. Well, all the payments are happening in dollars in US dollars on the platform, I believe. And and so it relates to the context of the US. And this in this piece of work, the authors are based in the US and so on. But, but yeah, so it is a, it is a US based system. And so the assumptions are kind of for the US market and economy. But the reality is that oftentimes that's not what happens. And the people who are actually doing the labelling are not based in the US and they are not they're not paid the same amount as the requesters are actually paying because there's a middleman who takes part of the money. That's part of the problem. So there is yeah, this piece of work here is around and mitigating biases in the contributor sample and increasing the hourly pay given to contributors. And again, essentially it's another technical solution for for the distribution of tasks based on based on human factors of the workers themselves. Okay. So the point here is to show that there are some ethical concerns around doing crowd work and crowd sourcing. And then you have got a bunch of researchers who are trying to address some of these problems, but they often are not going to solve the issue completely. But yeah, for for for us, I think it's just important to have some sort of level of awareness of some of the problems that can that that exist with crowdsourcing. And part three, which I'm going to speak through, it's not very long anyways, and I'm going to just talk about algorithmic solutions and transparency. So the paper that I referenced last week, minus two computer programmers, woman, Ester homemaker, which showed the problem with that with word embeddings that what embeddings and that bias right? Amanda That that's what we talked about. But the authors also propose an algorithm that deals with this bias. And what they are proposing, they call it de biasing algorithm, which is a technical solution to a historic data problem. Remember the the word and the bias in in the word embeddings stems from the fact that the word embeddings operate on the on on on a data corpus that is been compiled based on historic data. So decades of text from from newspapers and suchlike. So what they do is they come up with the D biasing algorithm two options. One is the hard one is a soft biasing algorithm, which essentially removes the gender associations for gender neutral words. And the paper goes into more technical detail about how that works. But essentially you've got here the, the, the, the graphs that show the differences between hearty bias and soft biased. And before so so without any biasing so for heart to biasing we have got a removal essentially of the gender Paris locations. So here this is for the heart by saying we have got far fewer numbers of stereotypic analogies by numbers of generated analogies than we do have for the the unbiased and the soft biased types of datasets. And then with regards to number of appropriate analogies, we then actually have with the heart the bias, we still have a higher number of appropriate analogies compared to the soft biased one as well. So for more technical detail, have a look at the paper again. Other researchers came in and wrote this paper. One of my favourite titles of all research papers, Lipstick on a Pig, which is essentially that they're suggesting that the kinds of methods to deal with bias by de biasing, such as the work I've just talked about, actually just hides than just masks. And it doesn't actually remove the bias and the gender bias is still there in the first instance, even between the gender neutral words and can be recovered from them. Right. So in other words, the yeah, the bias is still there. It's just it's just hidden. And finally, some work that we're going to be talking more about is another approach to dealing with bias is the sorts of technical approaches to transparency and or it's often also talked about as explainable AI. This this work here actually is critical of that work. And then questions the idea that black boxes. So the kind of you know when we talk about black boxes AI but black boxes can ever be opened. And so the idea of transparency is limited. Transparency in that way can also be harmful. I can it can, for instance, threaten privacy. If there is that that sort of part of the data and it can also privilege seeing over understanding that's as they call it. So just showing someone some relationship doesn't mean you understand it. And transparency can also be selective. So because it's it's an engineer type of approach, again, it has its own problems. They suggest that a holistic view is needed. So we need to understand how the socio technical systems work that comprise humans and systems can be held accountable. And I think that some of the ways in which thinking about the data statements, for instance, can be more effective because they are just they aren't trying to cover things up or solve them in a technical way, but they are showing and they are giving you the chance to make up your own mind about how biased the datasets might be that you might be working with. Finally, I'm going to make a note on the quiz, which is on Friday. So and in the lab. So the quiz is during the lab hours between four and 6 p.m. and you have 45 minutes to complete the quiz form. When you start the quiz and you can, you must submit the quiz by 6 p.m.. So you have to start the quiz by 5:04 to submit it by six. The quiz is on Moodle. So that also means that if you are not in the lab you can also do it from another place like home or wherever you want to be. Do do it on Moodle. It's ten multiple choice questions. These are one lecture, these cover the lecture content and the can also cover the readings, but only the PDFs that we put on Moodle is open book. That means you can use any resources to help you, but it's an individual quiz, so do not work with your classmates. You have to do it individually. Otherwise, while doing a normal lab there'll be a new lab sheet. The last week's solutions we will upload them to Moodle, but we want to a life coding session as is going to be too disruptive to those who are still doing the quiz at that moment, but we're there to ask and answer any questions you may have. So on Thursday for the next lecture.
