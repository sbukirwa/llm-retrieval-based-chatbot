SPEAKER 0
Today's topic Advanced V Design. And so we'll cover this. This allows you what we're talking about today will allow you to make your voice more engaging, more easy to use, and more successful. Please quiet down. Thank you. Based on Chapter five of Coca-Cola's book, which we've we've linked to the PDF, so you have access to the book as well. So going to talk about branching and disambiguation today. And then we're going to talk about dialogue management and other kinds of advanced topics. And first of all, branching. So branching based on voice input, what is it? Why? Why do we need it? So we'll do what we usually do. Talk to your talk to the person next to you or think for yourself for 3 minutes. What are some of the examples that you can think of where branching based on voice input would be beneficial and where should be avoided? And then we're going to talk about some of the approaches to branching afterwards that exist. Okay. So talk amongst yourselves for 3 minutes. Okay, So what we can do with branching is think about how we can constrain the responses. So that means that we want to design the prompt. So the prompt is whatever the voice in the process of the user in such a way that the user responds with a single keyword so that the keyword is included in the response that the user is likely to give. So this becomes an issue of prompt design. So how you tailor the, the prompt that the voice interface will produce to elicit information from the user? For instance, you might want to do things like please tell me the name of the restaurant you're looking for rather than, for instance, just say, where do you want to eat, right? Because that's why. Because that's ambiguous, right? So simply asking where you want to eat might just say in Nottingham, you know, that's not going to elicit the kind of response that the voice interface needs in order to process what it wants to ultimately produce, which is the name of the restaurant. Right. Same with for instance, what city are you travelling to? So you including in that you know you're making it specific that what the information is that you are looking for, not simply say, where are you travelling? Some might simply say I'm travelling to the United States as opposed to including city, which is ultimately at this point what the the user interface is trying to elicit from me so that we can progressive interaction. What's your symptom? What song would you like to hear? And so on. So this is about thinking about how you design your prompts. The responses that are likely to be produced will actually match the expectations that you have at that moment in time. And one of the strategies you might adopt here is using invest lists. Again, we've we've talked about endless lists on Monday, and then you can use those based on the uses following constraint responses. So you what you would do is the and likely best matches for the results that is being returned by the answer you. Then it's right over the list until the user confirms that you have indeed found the correct one, that you are proceeding with the correct response. A user has given an open speech. So when might we adopt open open speech policy? So this is actually to allow for way to allow for flow and the response doesn't need to be processed and you actually don't want to branch.

SPEAKER
So this is an example where branching might not be

SPEAKER 0
useful because you, you know, you want to be involved in an open conversation if you want. And for open speech, what you do is you provide a general reply. And this you could see, is similar to the generic confirmation strategy. So generic confirmation. What's a generic generic confirmation? See the vague response. And and so the point of a generic confirmation is that it always works regardless of what the user has stepped forward. That's why it's generic. And so this can be used to obtain information. So the open speech can be used to obtain information in a natural or in a conversational way. Right. So, for instance, here's a conversation between a virtual nurse which might say something like prompt the user to to tell them about the headache. So please tell me a little bit more about the headache you've been having. User response. Well, they usually start in the evening and last couple of hours. The virtual nurses response here is kind of a generic response. Right? So thank you. I will share this with your doctor. This is regardless of what the user might have said. This always makes sense. But of course, it's also important in a situation like this, there has been some kind of consent use that giving some kind of informed consent, that sensitive information can be shared with a doctor as well. Of course, because of the specific type of application where which is about medical information, this kind of sensitive and categorising input can be useful, for instance, emotions. So you may want to categorise the user's request in terms of what emotions it is expressing. So you have, for instance, if you wanted to adopt a kind of a binary approach to that. So you only have two categories. You have a happy and a sad category and a number of a number of different kinds of emotions that are being expressed and then be categorised with those within those categories. For instance, you might have a virtual companion interface asking how is feeling the user responding in some way kind of depressed, to be honest. And then this would then be we're still talking about branching, right? So the point of categorising the kind of press probably, I would say in this. Yes, in the sad because that has the capacity within it. So this would get categorised as sort of sad. And then the response would be it would be branched into giving the kind of responses for the user, having expressed a kind of a sad emotion. So this works for cases where there is no need to confirm exactly what the user has said and just ignore the category and move on. We also have wildcards and logical expressions as a way to as a way to categorise the user input and determine ways in which the branching should occur. Wildcards allow for more flexibility by allowing certain words to be repeated, for instance, without having to specify them explicitly. For instance, my computer is really a wildcard, so it means, because it's the wildcard that that the world word really can be repeated as many times as possible. And in fact, wildcard often means anything that is being said here can just be ignored as well. So that something like my computer is really slow and still needs the same output. And logical expressions refer to some boolean expressions like and or not that can be used to link was a phrase together. For example, if you are building a tech support type voice interface internet not working so you have an off operator averages importance of course. Forgot my passwords so you have forgotten password. So if you know that these to occur within the query, you know how to respond to that in a kind of automated fashion. Printer won't print. So again, this won't be a type of negation, right? A type of a negative expression. It will not print disambiguation. So we've talked about branching. What about disintegration? Different kinds of situation require that we disambiguate because the information the user has provided may not be enough, or it may be more than one piece of information that can that has been provided where really the voice is only looking for a single piece of information in order to move on. So here's an example of some let's talk about not enough information. So when there are more than one of a kind, this can occur. For instance, when you ask when the user's asking, what's the weather in Springfield now there are a bunch of Springfield's in the in the world. Most of these are in the United States, but there's one in Australia as well, which I don't know why I'm Central Coast. Nevermind. But so you might, you might need to get back to the user. Right. And report them and say which Springfield do you mean, Do you mean Springfield, Oregon or Illinois for instance? You also get this people quite when they talk normally omit information so they they do not include it. Right. So they might say when you ask them what kind of coffee would you like? All right. They might just say, I'd like a large piece. I didn't tell you what kind of coffee they want. They just consume coffee. All right, but you haven't specified it, Whereas your voice interface might require the user to exactly specify what kind of, you know, what coffee do they want. So then again, you may need to report them and ask for more information. And you also get this situation when the intent is unclear. So here, here's an example of that. And as a tech support assistant, how can I help you? Today? The user says, I need help with the Internet. Okay, so that's a vague type of request. The intent is unfair in that what kind of help to actually need. So here's a way to read from the internet. You I can help us out. Let me get some more information. I can help you set up your wi fi, find information online or fix your Internet connection. Which one you need help with. Again, you can also see that what I mentioned on Monday, It's a good idea to ask a question at the end rather than lead with a question, because then the user will start talking before you've presented options, which in this case you wanted to do. This ambiguous is also needed when the user provides more than one piece of information when only one is expected. So Case A here is where the user's response is ambiguous. So the in this case, in this example, the health assistant has asked what is your main symptom? Clearly asking for one single symptom singular, your main symptom. The user then responds, I have a fever and cough. So the user. Response. Right? The user's response is ambiguous, that question. So then in this case, one of the strategies as applied here is to ask the users to pick the more important one. For instance, by saying which one of those would you say is bothering you the most right now? So get the user to pick the most important one. Case B is when the user's request is ambiguous. Right? Notice the difference between request and response. And the strategy here is to ask the user to pick one of the options that are available. For instance, the user says Call Sally and the phone boy has more than one phone number for Sally, which is often the case, and thus will prompt the user to say, sure, mobile or work. So the user will have to pick the one one of the options that fit the need in that moment. So in part two, let's talk about dialogue management and also other advanced topics. So when you think about dialogue management, what are some examples of good dialogue management you can think of and discuss for 3 minutes and then share with the class? Okay. So many of the things that you've already said kind of fit into what I'm saying here on the slide. Dialogue management is all about making the dialogue flow as flexible and as effective as possible. So it's really about managing the step by step process, which where you might call conversational. So a lot of the principles you've already learned about doing conversational design, such as error handling, confirmation shortcuts, context, capture, these are all good principles of dialogue management as well. So good dialogue management is another word in a way to say you followed conversational design principles. So if you are following the conversation design principles, you will also be doing good dialogue management. And I think this was your point that you made. Good dialogue management adapts to the adapts to the user effectively. Here's an example the pizza app, and you'll all be making a pizza app tomorrow in the lab. So here's an example. So hi there. Welcome to post pizza. What kind of pizza can I get? This user says I'd like a large pepperoni, please. So this user has already specified this size and the toppings they would like for their pizza. Right? In their request. So all we you know, we have already filled the number of pizzas they want and the name and the topics. So what we know at this point is all that remains is the address and phone number. In this case, because because that's what this valley needs before it can proceed to completing to finalising the order. On the other hand, you have this kind of user who goes, Oh yeah, I want to order some pizzas. So they haven't having specified any of the information. The slots have not been filled yet, the slots that the app needs in order to proceed. So the app needs to prompt the user again. Great. That's what I'm here for. How many would you like to please? And then, you know, in a subsequent step, the app might say, What would you like as your toppings on these? You know, so you breakdown so you have good dialogue. Management again, is about providing flexibility both for this user that is kind of if you want an advanced user, they just, you know, in a single request, provide all the information that the app needs to complete the order and the kind of a novice user here who doesn't need to be prompted along the way so that they can provide the required information every every step at the time. It's quite a lot of people talking. It's very distracting when you're trying to give a lecture and you have people talking in the audience. So please be quiet. Thank you. Capturing attention objects. So this is an important part, again, of dialogue management and voice will actually makes this easy, as you will see when you start to work with one. So you need to recognise both the object such as a calendar. Right? And often this is this is essentially what we do when we do sloppily in voice interaction. You actually figure out which objects are they wanting to do an operation on and what kind of operation they want to do is the intent. What do they want to do? Is the intent and with what or on what objective do they want to do it? It's usually this long. Okay. So things like adding, removing the viewing, these are all types of intense. Of course, you know, you can have multiple objects that you can add or move or view and therefore you need to know both the intent and the object. So show me my calendar, add an event to my calendar, delete my meeting from my calendar. These are all things that you that in, you know, include both intents and objects in different combinations and invoice flow. What we what we have is the concept of utterances. Utterances, the ways in which use them might express themselves. They might do that in different ways. And so we need to tell voice about what the utterances are that the user is likely to use to complete the same intent, like I might say, I might say and add an event to my calendar or I might say, you know, take my calendar and put a new event on it. I have said the same thing, but a different words. So we so we need to express that in terms of utterances, what the different ways of being of expression mapped to the same intent and objects. So objects as I mentioned, are usually captured as lots and these are called entities invoice flow. So you will see that when you start workflows, flow entities are the ways in which you capture slots. Handling negation is an important part as well as of dialogue management and negative responses are of course important because if there's not, no don't. And neither in that, then it probably means the opposite if you just ignore C So it's fatal to basically ignore the negations because it means the opposite without the negation, if that makes sense. Here's an example. Mushrooms, but not pepperoni. Do you prefer pickup or delivery? Delivery. Okay. I have one large mushroom and pepperoni. What happened? The voice interface or the chatbot in this case ignored the no, but added mushrooms and pepperoni when the user said no pepperoni because the negation was was ignored by the chatbot. And again, another example. How are you feeling today? Not very good. Nobody's mentioning it to a good great to hear again. It's a bit embarrassing to say the least. So handling this, for instance, biological expressions would be a way of doing this on the voice flows. Not very good at that. So you might have to find a different way of handling negations if you need to. Should avoid display or say what is recognised now. This can be a very efficient or effective strategy to help the user recover from errors from it. So if the person makes a transcription error, this can be made visible or audible so that the user can hear exactly what went wrong. And it allows the user to recover and they're not left guessing what went wrong. I will come back to that strategy, don't worry. So it's a good strategy to use. This utterance must be recognised correctly in order to fulfil the intent. For instance, order details, delivery, address and so on. So you want to say it back to the right like a confirmation, but in the confirmation. See, you don't just say, okay, I got your order, it will send to tomorrow, you know, but instead you repeat back to the user. Exactly the address or post code, for instance, that you know, you have captured from the user to make sure you don't just ask them again, is this correct? Right. So you repeat back to them what you captured from the user because it's important information if you haven't got it right. That means they don't get that parcel. So that would be, again, a fatal mistake. So saying it back to them in terms of the types types of information you might capture from the user, that's how essential it must be recognised correctly to fulfil the intent. Is a good strategy. It can. However, it can also be distracting. So you shouldn't overdo that. If the users exact recognition of the utterance is not required to move on, you don't have to every time be sort of parroting it back to them like a parrot. Right. But instead only do that when it's necessary to make sure that something doesn't go badly wrong. And so when to adopt the strategy then depends on the context of your app and where in the process the user is as well. Sentiment Analysis. Using sentiment analysis as part of your dialogue management can be a useful thing for sentiment. Analysis is allows you to computationally identify a cut through categorisations of opinions expressed in a piece of text. So sentiment analysis has typically got three types of sentiments it can categorise, which is positive, negative or neutral, and that can be useful at times to do that. So this requires course natural language processing. You have some of the relevant skills for this already. Build your bag of work money from standard documents. Then you can compute similarity and distance and that allows you to to compute positive negative for neutral. In fact, I was one of the example datasets with a positive and negative film film reviews. So in movies, capturing the sentiment can be useful then to generate appropriate responses. The user on the fly as well. A little bit similar to to the example of classifying positive or negative emotions or happy inside emotions, right? It's a little bit similar to that, where you might it might be useful to do that at certain times, depending on the nature of your voice interface. Text to speech and recorded speech is a question that you you may have when it comes to thinking about what how do you want to generate voice output for your use of text to speech nowadays? So synthetic speech is much better than what it used to be. So there's a good range of choices available. I'm not going to jump into that demo right now, but you can check it out yourself and play around with the voice generated on the search website. You just type in some text and then you pick a voice and then you can hit it output and it'll play it back to you. And all sorts of weird voices was fun to play around with. There is a thing called the speech synthesis. Mark-up language was somewhat similar to HTML, right? In the sense that you can use tags to tune in this case, not what the website looks like, but what's what voice sounds like this synthetic voice. So you can tune the positive. So the pitch or the rate and speed and volume of the speech output and you can do things like that. However, if you want to, if you want your voice to sound really natural, then you may need to report voice talent. So that is the other strategy you have, which is to record human speech, which, you know, as a developer unlikely to do. You're more likely to just pick one of the synthetic voices that are available to you, but you could also create your own voice by getting a voice, talent, an actor, some kind to record them as they're reading out some things. And you can you could use that can be costly, of course, and not not required for what you're building. For example, if you took that approach of recording human voice or recording a human voice, then here's here's an example of San Francisco Bay traffic forecast, which you could ring a phone number and it would read out the traffic pool. Cost any any point of time in the day. And this worked based on recorded speech. And so this snippet here is essentially based on 19 pieces of recorded content. And so how this is recorded and if this is then stitched together together like this as of 1018, and there's a slowdown on Highway one and one and so on. Right. You can see. And so you can still hear that like public announcements. For instance, next time you're taking the National Rail in the UK. Listen for the announcement. The announcement is based on how this is based on stitched together recordings. Right. So you can see that that approach is still very much being taken by. By companies that produce announcements. And another aspect that may be useful for new dialogue management is speaker verification. This is also known as voice biometric authentication as well. We have touched a little bit on this concept of voice ID in the context of Speaker ID to avoid triggering somebody else's device. So when I say Siri, it should only be my phone or if your phone's going off as well. So another use case is, for instance, when you're transcribing multiparty meetings where so you have an automated transcription system to capture some sort of political debate and transcribe it, you need to know who's talking and you want to be able to say at least something like. Speaker one. Speaker two. And for that to always be the same person. So you know what they said and trace that automatically. So that's, that's the use case for this kind of voice ID, which isn't the same as using it for authentication, which is, you know, that's kind of in the context of security, right? Providing a secure access to a to a building or device or anything like that. So you could use that for authentication instead of typing passwords when various like you use might use fingerprint or face ID these days. The problem with doing that is it's actually not infallible and it can be faked fairly easily. And so it's not that secure. And and probably that's why you don't really see voice ID for ID for security purposes, because it's not that not that safe, really an easy to fake. Context. Context management is a really important part of dialogue management. So being aware of what's going on around the conversation as well as things that have happened in the past, we've had this example of, you know, who was the president of the United States, Abraham Lincoln. How old was he when he died? You know, ever Lincoln was. So being able to track what the user said previously and remembering what has been said is actually quite hard to do that. And it's kind of an unsolved problem and but can really make a difference in terms of the of how advanced your voice interface seems to to the user seems a lot smarter and can also save the user time you can but also you can do other things with context that are less softwares that you can take into account the user's timezone to say to them, Good morning when it's morning where they are, right, because you can get the time zone from their device. So that's something you can build into. And you know, it just kind of feels and then you say Good afternoon again or good evening when it's appropriate. And again it feels more smart and contextually appropriate. If your voice interface does things like that, you also might take into consideration when they last logged on to the to the app, right? Because you know that because it's your app you can keep a keep a log on When they last logged on, you can say things like welcome back or haven't seen you in a while. Could also use their location if you had access to that via their device. And they've, you know, granted access to their location. You can use that to give them you know, if they're asking for restaurants, you can assume that or the weather forecast. You can assume that they're asking for that based on their current location. You can also do things like track the stages required to. Complete the transaction. So they have, you know, this with some transactions, this flexibility in that you could you know, if you're a restaurant app, you know, the user might first order drinks and then the food, of course, the food and the drinks. Right. There's often times with transactions and they might add, you know, they can add multiple things on to that. They can add some sides, right. So there's no logical order of things that you have to maintain. There's some flexibility. But to know that they've already ordered drinks later on is useful, right? Because then you want from from again, do you want to order drinks because you know, the fourth order drinks and yeah, that's going to allow for more flexible dialogue management. The pizza example that we seen earlier as well. So, you know, they've already got you already know the size and the pizza they like. So don't you don't have to ask them again. And we can also do things with advanced multimodal input. What we mean by multimodal we mean in this case not just using voice, but also using other modalities to which we can interact like pointing or touch. Right? So we can use for instance, we have already seen hybrid strategies of outputs. So we have already seen we've talked about hybrids, I would say, between graphical and voice interfaces. So, you know, when you talk to Siri, it can produce and show you something on it's on the screen of the phone. So we know that we're quite used to having hybrid strategies for output price, the device outputting information. However, you could also think about hybrid strategies for input as well. So why not allow the user to point on a map and say, what's the capital of this state right on a screen? Right. I might be able to say points on the screen so I can take you, you know, the point of where the user is touching and combine that with what they said, and that's an example of having a hybrid input strategy. Or you might say when you're playing chess, move my knife night here and point on the chess board again, virtual chessboard on a screen or something. Or you may have we embedded in a website similar to now how we how we you accustomed to having chat chat bots built into websites as well and you might be able to task them by pointing in different pieces different parts of the website as well. We have to do something like advanced dialogue management. It may also be useful to build your own language models that you can use to bootstrap your application. So if you are building a huge system for a company that has a large web presence like British Airways or something, you know, or an airline, you may want to scrape their data, their website data, things like FAA cues that they already have in print. You may want to use that and the terminology and also things like customer service forms or, you know, recordings of customers calling in and asking questions. This can all be used this data to bootstrap this sort of call centre data. It's nice to bootstrap in your that your your voice interface that it can give the correct or give like better targeted responses on topics that you know, customers will ask about. So you don't have to start from scratch. But there's a lot of data usually within companies that you can make use of to build specialised language models that are more likely to be helpful for companies like that. This is beyond the scope. Most of what I've talked about today is beyond the scope of your coursework, because it'll take too long, but not all of it is okay. Finally, so we and voice user experience, which hopefully our lectures the last two weeks have shown is not a solved problem. And you can tell that because when you use zero voice interfaces and the experience of that is, you know, it doesn't work very well. There are big challenges around gaps, around the gaps between how people actually talk naturally when they use natural language and speech compared to the capabilities of things like, as you know, the transcription and the natural language understanding that is provided through and LP techniques. And however, we've also learned hopefully is that there's actually a lot you can do as a designer to make the theory as effective as possible given the limitations. You can draw on a lot of the dialogue management techniques you heard about today and in the last two weeks that help the user that helps with things like disambiguation, helps the user recover from errors, handling negations and things like that. Overall, what you want to achieve with dialogue management is guide the user to provide well-formed input in terms of your device, in terms of what the voice interface can handle. And you do that by by crafting your prompts in a way to design the response in a way that you can handle it well and enable the user to progress to successful completion with what they want to achieve with your voice interface. So thank you. That's it for today. Please remember to remember to register your voice for accounts. And we're starting with voice flow tomorrow in the labs. And we'll see you soon.
