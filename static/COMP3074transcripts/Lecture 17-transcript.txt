SPEAKER 0
Lecture. Just wanted to mention that I've released coursework too, so you can find that on Moodle and the course works. That's the description of the coursework specification. If I have time at the end of the lecture, I'll go over this a little bit more. But just to let you know that it's there and you can read it in your own time and you will be pleased to find that there is no report required for coursework too. So because we extended coursework one we made closer to quite a bit smaller. Okay. So we already made changes to it so that, you know, it's already accounting for the fact that you have less time for coursework too. Yeah. That's good reading. No waiting. We cannot change the waiting because the university won't let us do that while the course is running. So we can't change the weighting, but we change the size of it. So we made it smaller and much more, much more doable for you. What you will need to do is create a voice field prototype. And so you create a prototype that you export as a voice floor file. Now that's what you submit. And a video where you basically demo the the prototype that you made. That's what you need to do for coursework, too. That's it. So it's much smaller than coursework one. Okay. I won't go over the detail right now, though. I'll start with a lecture. So this is lecture 17 and we're going to talk about discoverability and response design. So what would I take to very important principles of voice user interface design? You will need to address those in coursework too. So pay attention to this and it links in with the lots that we're doing at the moment as well. Okay. So is this practical stuff, even though it might feel quite conceptual and theoretical in many ways there are practical implications, but I'd like you to take away from this for your coursework is what my findings were. Okay, so as I mentioned, these are a couple of advanced topics in voice interaction, some really thinking about the interaction with voice interfaces split into two parts, the first one being about discoverability and the second one being about response design. So when we're interacting with voice user interfaces, you can sort of conceptualise this as a, as a, as a loop, as an input output loop, right? So you have the human input humans to human to something, request something, addresses the, the voice interface and then the if everything goes well, the voice user interface will respond and produce some kind of output input output like you have with any kind of computational device really as well. So if you think about the input output, though, here you've got the problem that how do you know as a user what it can do? How do you know that. Right. With a voice interface in particular. So that's what we're going to talk about with discoverability is the problem of how do you know what it can do, How can you discover what it can do? So in other in other words, what do you need to do to invoke it? Then on the output side, you have the question of what if the output doesn't meet the intent of the user? So what if if it doesn't correspond to what the user has expected? Right. What's what do you do? And how can you, as a designer shape this the output so that the terms that are following are good, so that you enable the essentially that you enable the user to progress with the interaction. Okay. So to kick things off and to get your brains working on a monday morning, think about this for 3 minutes, discuss this with 3 minutes to share with the class. What would you do or could you do in your design to help the user know what they can say to the voice interface? Okay, discuss for 3 minutes and then share with the class. Okay, so what are we thinking? So there's obviously many ways in which you might be able to do that. So this might work. Can you hear me? I think you can choose choosing to ignore me. Okay. Thank you. So, yeah. What are some ideas that you have? How can you help the user? How can you help the user know what they can say nowadays? Yeah.

SPEAKER 1
Give an example to the user examples here. Like do you like to. I can, I can help you to create something.

SPEAKER 0
Yeah. So basically give, give instructions to the user. Yeah. That's good idea. Yeah.

SPEAKER 1
So basically like first conversation. You can make. Usually. I mean the keyboard. Yeah. Mm hmm. Mm hmm. Wants to do what we're doing.

SPEAKER 0
Yeah. So similar kind of response. Give instructions, but kind of do it on the fly as you are addressing the user. Yeah. Yeah.

SPEAKER 2
Including questions.

SPEAKER 0
Ask what? Leading questions? Yeah. Yeah, that's right. So you basically provide you ask the user a question, like, do you want to place an order? You know, so you actually including the keywords within that and then the user can say, Yes, I want to place an order. Okay. Any other thoughts? Ideas? Okay, well, that's a good starting point. So we are going to talk about exactly those kind of topics today. So the first one we talk about is discoverability in the context of initiating interaction. So when you first when you first initiate the interaction with the voice interface that you are not familiar with and you already of course, you already know the commands, you already know what it can do. You already know how to interact with it. It's easy. Well, you've learned it already. But the point is, how do you do that when you don't have have that knowledge, that prior knowledge of what the commands are that it can respond to. And then so that relates to the kind of the human input. And how do you support that as a designer and within the voice interface design? And then we're going to, in the second half of the lecture, cover the response side of things. Okay, So how do you know what to do? The assumption here is that you don't want to give a user a manual to read, right? That is another option. Give them the manual to read. That's a bad design choice, right? Because we want people to be able to walk up to your view and start interacting with it and kind of make sense of it as they go along. An intuitive kind of way, right? So you want you want the problem. You want to the design to be intuitive so that it doesn't need a manual manual and you want it to be that means that you must your the actions that the voice supports must be discoverable to the user. The question is how do you do that? There is a concept called affordances, and that is from psychology. This is specifically from from this book on by Don Norman called The Design of Everyday Things. I can recommend it as an example of affordances, right? The old the old T wants what you call a teakettle. Now, what's wrong with this? Design is quite obvious, isn't it? What's wrong with it? But the point of the affordances is that, you know, if you have a handle here in the right place, it affords to be picked up in a certain way and used in a certain way. And this relates to this is why it's about everyday things, all sorts of things you encounter in the world, such as door handles. So the examples in the book are, you know, door handles for, you know, for themselves to be used in a certain way. And door knobs are, you know, usually for a for themselves to be used in a different slightly different way, pushing it typically with a door knob or the door handle. That's the example in the book anyway. It's a difference that you pull the door towards you. So affordances refer to the potential actions that are possible, but these are easily discoverable only if they are perceivable. Right? And that is the problem, of course, that we have in voice user interfaces. Why is it the problem in voice user interfaces? Why is that a problem?

UNKNOWN
You can't see.

SPEAKER 0
Victor. Because you can't see what's behind them. Exactly. Because that's the key word here is perceivable. Right. So how do you perceive something that you can't see? Okay. So that's why affordances are difficult to put into the design of voice user interfaces. If you compare that to a graphical user interface that you all know and the voice user interface you have, and another kind of concept from cognitive psychology, which is called recognition and recall. It's easy for us to recognise as human beings amongst a range of actions and amongst the range of things is easy to recognise and pick something, even if at first we don't know exactly what it means. I mean these icons, you know, they some they are somewhat arbitrary, but you learn them very quickly and then you recognise what they are and you don't have to remember to open a command terminal, right. Like you did in the old days before there were graphical user interfaces. You had to remember the command that you type to open the browser. All of that was before the internet, any. And you can, with graphical interfaces, rely on recognition. And one of one of Nielsen's usability or heuristics. So Nielsen has you know is they are the ten famous he's written these usability heuristics. They're kind of famous amongst amongst user experience designers, let's say. And one of them is recognition rather than recall. So you should minimise the user's memory load by making elements, actions and options visible. But okay, that's great. We can. But the problem is within voice user interface. We cannot do that. So we have a problem there. That discoverability in graphical user interfaces isn't something that you can directly replicate in voice user interfaces. So what we did was in this research paper, we discovered we we research different kinds of strategies to provide discoverability in voices interfaces. Now, we've already I've already covered this. We already know that the, you know, the discoverability challenge comes from the user finding it difficult to find and execute available functions because of the, you know, because of the fact that there isn't a visible interface for the user to look at. So the challenge relates to the invisibility and also ephemerality of speech. So ephemerality is this concept that, you know, once you say it, it's gone. So it's also not a good idea to just list, you know, the the 25 options all at once that your voice user interface can support because it's too much information. So what we did was in this research, we ran a Wizard of Oz study. A It's a certain kind of study you can do where you basically don't have a working, fully functional voice interface, but you're mocking up some of the things behind the scenes. So we have a wizard behind the curtain, right? That's why it's called because like The Wizard of Oz, like the book, it's the child story. You have an experimenter behind the curtain who controls the interface. In this case, we have we have Philip here, who was a master's student in HCI here in Nottingham three years ago. And this was his master's project. And we just we ended up publishing it because it was good. And then he and then that's that's a participant that took part in the study, right? So they sit on the other side of the, of the curtain and the I'll talk through what was involved in the prototype. He made a prototype called Feed Me, which was, you know, a delivery service voice interface. And the task for the user was to order two different dishes from a local takeaway restaurant using the voice user interface. There were three conditions. And the first one was they would automatically receive help. The second one was they would have to request help and ask for help. And the third was there was no help. And then this is what they had to do. They had to choose a cuisine. And then the restaurant. Then which dietary options, if they needed any, then choose a dish, add it to the basket, do that once more because they we asked them to do two dishes and then check out. And what we did was we had the automatic help on the one side. Which was a little bit like what? A little bit like this. The voice interface would say, What would you like to eat? If these choose one cuisine, you can say Italian, American or Japanese on the user, you know, So the user would then be able to just obviously say something like, Can I get Italian, please? The in the other case, in the other condition, the user had to explicitly ask for help, so they would have to the voice interface would simply have the first part of the prompt as you can see. Just what would you like to eat? And then the user would have to say if they weren't sure, they would just be able to ask for help like so that could be able to say, What can I say? Or they could be able they would be able to say, you know, what do you have? What options are there different ways of phrasing a kind of request for help? And then the voice interface would reply with the options. So what we measured was different types of measures of performance. So we measured how long it took participants to complete the task, how many errors they made. How many turns it took them to complete the task. And how many stages they completed. We also asked them in terms of how well the system was perceived. We used the system usability scale to measure that. And then we also had a feedback session at the end where we asked them to provide some feedback. Here are some takeaways. I won't go into into much of this. And we found that the automatic strategy was significantly better than the baseline. In all measures. The performance in the requested strategy varied. And that's what we found. So when the requested was when the user specifically had to ask for help so that a performance there wasn't as good. And however, we found that tons per task was the only metric for which we found a significant difference between the two two strategies. And it makes sense, doesn't it? Because with the automatic one, well, the user doesn't have to ask for help. So the the voice interfaces turns a longer right there longer because the voice interface every time provides the whole information and options in one in one prompt. And so you need less turns overall to complete the task. So it makes sense for that to be shorter shortest with the automatic. This is time for terms per task. This is requested that where the user had to ask for help and the baseline was worst, where there was no help at all. In the paper we discussed ideas of varying discoverability based on the user's experience over time. Because, of course, you know, it won't be quite annoying probably though, to have this to be given the options every single time. When you're an experience user, you just don't need them anymore. So when you are a more experienced user, you may want to adopt this policy of explicit prompts so that this kind of policy where the user actually explicitly asks for help because they just don't need it anymore. Yeah. So that's what the paper was about. And so on. The motive for you to look at as well. So that's kind of covering discoverability. So you get, get an idea of, of how you can implement that in your own voice interfaces as well. So in the second part of the lecture, let's discuss the second sort of part of the circle or the second circle here, which is about designing the response. So now discuss for 3 minutes. What would you do in your design in this time response design and actually response design to help the user to move to successful completion? So what in your response can you do that you design so the prompts that you design, in other words, to help the user move to successful completion. Okay. So discuss for 2 minutes or so or 3 minutes and then we'll share with the class. Bit quiet, so better start again. So what do you think? Any ideas of how we can help the user in our response design to move on to successful completion? Yeah.

SPEAKER 1
Suggestion.

SPEAKER 0
Just say that again. Suggested. What do you mean by suggestive modelling? I want to use an important constraint.

SPEAKER 1
On the categories which is supposed to build what you should use. If you feel disappointed suggests audience people came to buy the ad, the last, etc.. So it's just trying to fill in the gaps instead of ABC. Oh, thanks for showing me to see.

SPEAKER 0
Okay. Yeah. So in a way you, you craft a response that you include suggestions for and actually you make what you say. Do you mean that's a confirmation isn't it. Yeah. So you are asking the user to confirm which of the options they want to choose. Yeah, of course.

SPEAKER 1
Let's say a situation is an input and a space. You need to put that almost everything.

SPEAKER 0
Yeah. Okay. Yeah. I still have to wait for it.

SPEAKER 1
You make a full case for the location, etc.. So everything is to follow in the final stage, which is a can you say? Yes.

SPEAKER 0
Right? Yeah. All right, good. Any other ideas? Okay. Yeah. I mean, some of this is really the stuff we've already covered around confirmation, around error handling and so on. But we're going to look at this in a little bit more detail and kind of, I think, in a more holistic way, thinking about response design, because that way you are thinking about designing the response to what the user said and you are already thinking about what the user can do next as well. There's a paper related to that. Whether I'm going to draw examples from. Which again, is a paper we had at KYW four years ago, and I'm going to use some examples from that. So what we did for this research, we built some text. Yeah. So Martin pitched this unit at the time, gave an echo to five households for a month each. And then he also built this device called a conditional voice recorder, which is this device pictured here, which consists of a microphone. And inside is a Raspberry Pi computer. And the microphone, the device listens to the wake word, the same wake words that the echo listens to. So something like Alexa. So as soon as it hears to wake words, it will then keep the recording. So it's continuously recording this device. However, unless it hears the recording, unless it has to wake what, it will delete the recording, because it doesn't it doesn't need to be kept. However, if it does here the wake word, it will record one minute before. So it will keep one minute of the recording up until that point and then it will keep at the end. It will keep another minute after it has to wake word that way. In your audio capture, you hear what happened leading up to the interaction with you, with the Alexa. And obviously, you you capture all of the interaction with the Alexa, including all the stuff that's going on that Alexa doesn't capture. Right. So it's just an audio recorder. And you hear some of these recordings now from this family. This is a family of four. So we have Emma, so we have Susan and call the parents. And then we have Emma and Liam, the children in this household. So this I transcribed this for you, but I'm going to play it to you. Might be a bit loud. Dan, I hope this will work so you can listen along. As you can hear, you can't hear anything. I really like to play this song and try this way.

SPEAKER 2
And you ask for a lot of money. And that's what sets us probably place. And sorry, I can't find the answer to the question. I can't for that. A family quiz. Sorry, I don't have the answer to that question. Alexa, he said crummy quiz. I wasn't able to understand the question I had.

SPEAKER 0
Alexa Comedy quiz. Okay. It never gets boring, does it? So let's talk it through. Through What's actually going on here? Turn by turn. So we have, you know, Emma says, Can you ask for a normal quiz? Susan then tries to address Alexa like so Alexa set a family quiz or set us family quiz. Alexa then returns this response, which is just this very generic response, right? It's a generic error message saying, you know, I can't find the answer to the question I heard. To which Amazon comes in. And she actually if you look at this response there, she actually changes the way in which the request is in in detail constructed. So she says Alexa set a family quiz. And so she tries to sort of emphasise the words. So she kind of changes the pronunciation and slightly changes the words, which again leads to the same non-response. Liam then comes in with this Funny, please said, you know, this obviously isn't going to make a difference, sadly. And so and then Alexa responds again with the same generic response, and then Karl tries without it. Just when you just said Alexa family quiz without a intent. Right. And your and you will know that. The problem here is, you know, technically speaking, well, they haven't used the right intent word. They have used they tried set, which never, never worked. And then Karl just leaves out. And that's also not working. So it's missing the right intent to kind of technically to invoke the action that they want, which is to play the family quiz. Now, the point, though, isn't that they got it wrong. The point is that the responses because we're thinking about response design, Right. The point is that the responses Alexa gives are not very useful for them to actually get to the right place where they want to be, which is to open this quiz. So you could think about the other you know, the other thing that's going on is when they ask to be, you know, set up a family quiz and the response is, sorry, I can't find the answer to the question I heard. That's not even a question. Right. So what this is what the person said, you know, is a is a command, an instruction, but certainly not a question. So even even the responses, generic response doesn't really make sense and certainly doesn't help the family to move on with what they're trying to achieve. These in other ways, you could think about responses of the responses Alexa gives as resources for the user to move on. Right. So. And what happens in the real world with Alexa is that we this was in our trial. We had 30 to 50% error rates or failure rates. So that, you know, what you've seen here is very common that when people are trying to interact with it, there are, you know, as you seen, that it was zero out of four attempts and that was successful. And overall, we see this kind of failure rate, which is very high. So actually, failure is normal, Right? And there's trouble in the interaction is normal. So therefore, you have to design for it. Design for the trouble. There's design for the failures. Here's another example.

SPEAKER 2
And I thought because it could be just me, the intro. You want to hear a station for B b intro.

UNKNOWN
Right? No, No. Oh, right.

SPEAKER 0
Okay. So here they're trying to play a game called Beat the Intro, where Alexa would play a few seconds from a song from the intro to a song. And, you know, it's a quiz game. And then you have to guess the name of the song. Obviously, AMA is trying to invoke the the device again here with this. With this. With this request. This is done. This is all the talking here. So while Emma starts saying Alexa, there's a there's a bright gap here in which Karl said is it can't be the intro White quietly sort of questions. Questions the phrasing of it. But Emma tries it and then Alexa gets it wrong again. But this time, you know, Alexa responds. You want to hear a station for B b intro, right? So what is Alexa doing here? Alexa's transcribing part of the request it heard. So this this section here being played by the intro, you can see how Alexa transcribed that as b b intro. So there's a, there's a typo in the transcript or a missed transcription, a transcription error. Right. So the difference here is, however, that Alexa makes that available to the user. So instantly there's no problem for Emma to say No, I don't, Alexa know, and Alexa then finishes. All right. Okay, so this although what, what you could also say this wasn't a successful interaction in the sense that Alexa was unable to start the game. Beat the intro. It still was successful in the sense that it progressed to completion, right? So you can see how this progresses all the way to this kind of. All right. And Emma has no problem responding and actually that No, that's repeated no, that's being said here is actually picked up by Alexa and Alexa can. All right. Right. So it comes to a successful completion. So in this sense, what this does, this card, if you want here a station for B intro, even though it's it's wrong, it's not it does not match the expectation of the user. It does help the user to instantly to kind of recover from that. Right. That error and move on. So in that sense, this station for the intro, this transcription is embedding a useful resource in the response. And this is this whole the concept here is one we call progressivity. So you think about progression through the sequence and what you do as a designer in your responses and your response design to enable that progression through the sequence. So when so good response design helps us do all those those things like recovery repair and progressivity. So in good response design, you help the user recover from errors. So that can be done by saying what went wrong, right. For example, you know, actually including the transcription. And if the confidence level is lower, as was the case in this case, where Alexa then presents the back to the user. So as an example of helping of saying what went wrong to help the user, which helps the user recover in case that is an error, helping the user repair a request. So saying this is what the system does not understand as well could be, could be a good way of of helping user repair and then helping the user move on, getting to get things done, which could be done by things like offering options, which is also some of the thoughts you had when when we started this section of the lecture as well. Okay, so now you know about response design. And of course this relates to the stuff we already learned about in previous lectures, like confirmations, right? Good confirmations, intel panels, book making available what the system understood like I think you said this, but also voiding over confirmation, which can be quite annoying. So and we already talked about this kind of more advanced ways in which you might do confirmations by taking into account system confidence. So here in this example, if buying paper tells the voice interface will, if it has a higher confidence above 80%, it will provide an implicit confirmation, something like, okay, ordering more paper towels. Implicit in that is that the paper towels are being ordered. Thus the the request to buy paper towels has been confirmed. So it's an implicit confirmation. If the confidence level is medium, something between 50 and 80%. For instance, here the voice ask for an explicit confirmation. So it asked the user to explicitly confirm. I thought you said you'd like more paper towels. Is that correct? So the user has to provide explicitly a confirmation of that if the. And if the confidence is low, then the the device will reprint and ask the user to repeat their their requests in the first place. I'm sorry, I didn't get that. What would you like to buy? Um. I mean, this obviously assumes that the the fact the buying has been has been captured here. But you get the idea that you reprint the user when their confidence level is low. And now. And yeah that's, that's, it's a we've, we've kind of covered this last section here about response design where you learn about recovery, repair, progressivity and confirmations. And I do have 5 minutes. Do you want me to go over the coursework to. For 5 minutes. That's probably all I need anyways. Okay. So Crosswalk two is due on the 10th of January 2023, not two. You submit a video which is five about 5 minutes long. Shouldn't be longer than that. And the export is a prototype as a voice file by Moodle. This is for overall for 30% of the overall module mark. The task for this is to build a conversational app so you know, a voice a voice prototype to support users in completing a task such as booking or ordering in a specific domain of your choice, such as travel or shopping. The prototype should also demonstrate the very design principles. Again, there are two assessed elements. There's the prototype and short video. Can you be quiet, please? At the top? Thank you. Hello. Can you be quiet, please? I can hear you talking. It's it's it's very distracting from trying to tell your classmates. So if you don't want to hear this, please leave the room. Thank you. Um, so overall, the prototype requirements are that your voice for a prototype should be built with voice. Floor must be built with voice. Will, in fact. And it must be in English. You should be supporting specific tasks such as ordering, blocking, conquering, and so on. I'll go over some examples in a second. And so it must not just be a Q&A bot or a chat bot, which is kind of for small talk. That's not going to be enough. However, you may want to spot some small talk, but that's optional. It should also demonstrate the voice user interface design principles. That's in fact a large, large chunk of of what gets assessed with this work is that it's about the design, the voice user interface design principles. The main functionality that you support here is and you know, these are kind of the requirements, so you should kind of read these quite carefully, these points. You should have several items. So what are items? Things like products, things like tickets, groceries, appointments, songs, pizzas, you know, these are all items and you should have options for each, you know, pizza, toppings, sizes. These are options, right? Or book in class, those types of groceries, quantities, dates, times, slots and so on. These should you should allow the user to query more information about the item. So all products allow the user to add, review and remove items from a collection such as what is a collection, an order, a booking, a calendar or an inventory. These are all collections and you should allow users to make transactions. So for example, ordering items, booking delivery, agreeing to purchase, you don't need to implement payment processing. Okay? So you don't need to go anywhere near payment processing at all. You can just kind of mock that up at the end. You know, you have to confirm your order type thing and they should be able to query their process and status as well where they are in the booking process. Here are some examples of acceptable tasks for the prototype. You don't have to pick one of these, but you can travel, booking, restaurant, booking, cinema, theatre, gig ticket booking, gross groceries and home delivery, ordering cooked food, home delivery, ordering cook along recipe provision, university timetable management, calendaring, kitchen inventory management playlists. And just you know, these are just some starting points. Be creative. Come up with your own ideas if you want. It's very important that you try to implement the very design principles. Obviously, the labs and the way the labs are designed is that you're already learning that now you can do that. Invoice flow is actually pretty simple, but you will need to implement them and think about them. Here are the seven voice user interface design principles. You should be implementing your prototype, including prompt design. A lot of what we talked about today in terms of the response design related that relates to that and things like conversation markers and key phrases, the shape and structure of the prompts themselves and how they support the users in the tasks. Discoverability. What we talked about today do provide contextual help to the user. Do they can they query what they can do next? And help to discover functionality and content and error handling? So what kind of error struts error handling structures are you using? Are using them effectively? Personalisation Is the experience somehow personalised to the user? The next lab released on Friday will show you how to capture things from the user like their name and confirmation. What confirmation strategies are used and are they effective? And then contexts. Do you use context tracking and use it effectively? We had some examples of of relatively simple context tracking can do and you can definitely do that and vice versa. And then integration. So this is about using data and content from external sources. And again, we'll show you how to use that in the next lab as well. And voice flow, again, is very simple. And then for your video, you should you know, you the video must show the prototype in use. So a screen capture, the voice flow interface. The video should be narrated by use or should have a voiceover must be in English, should not be longer than 5 minutes. Successful video should give us a good idea of the prototype features. So what you've actually implemented and shows off and explains. So you would explain that with you, probably with your voice and the narrative. Explain how the voice user interface design principles were implemented so the video could, for example, show the interaction between the user and the prototype, including successful examples, but also things like error handling where things go wrong, right? Submission via removal of the video and the export prototype. How do you export the prototype? There's a share button on the interface. Then you select export s and then you select project content and pick the option project file JSON. So there are different options there. So, you know, this is this is the option you should pick. And that way you get the via file and you just download it and then you uploaded to Moodle. And this is the grading scheme. So you can have a look at that. The way this is split up between prototype requirements was just the first section of this document. I've just got all those 30%. And the voice user interface design principles, 40% on the videos. 30%. That's it. And we'll see you all on Thursday for the next lecture and Friday for the labs.
