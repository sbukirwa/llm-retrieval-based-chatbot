SPEAKER 0
Okay. So welcome back. Hope you all had a good weekend. So today's lecture is about linguistics and natural language processing, a not always harmonious relationship as a subtitle in this one, as you will see. So what I want to do in this lecture is give you a very brief history of linguistics and natural language processing. And then I'm going to talk in a little bit more detail about some of the linguistic concepts that are still of relevance and then L.P. and where really also NLP has kind of changed into a what we call new era, as you'll see, and how it all relates together. So linguistics and natural language processing. Why do you think why do you think linguistics is relevant to this module? Yeah.

SPEAKER 1
As we speak in the languages. Language structure.

UNKNOWN
From. So I think being able to.

SPEAKER 1
Understand the car.

SPEAKER 0
Okay. So so that's a good idea. And it's not entirely wrong. But linguistics is not not necessarily about the different languages. What is linguistics? Sorry, The study of language. The study of language. Right. So and and typically. Yes. And it's it linguistic linguistics is if you if you're studying French linguistics, that's going to be very different if you're studying English, linguistics and so on. So. But it is about it's not so much about comparing languages, although it can also be a part of it. But it's much more fundamentally about the study of language, the study of how we communicate with language. So why is it relevant to this module? Why do you think it's relevant? Why don't we just teach you statistics? Isn't it all just statistics anyways? Any guesses? Yeah.

SPEAKER 1
Exactly. The first, you can just be quiet or like hearing the noise of the.

SPEAKER 0
Pronunciation of the words that you saw. Or in this. So. So yeah. So that's again, it's an interesting guess and it makes sense but any any more at the back of that. Yeah. Certain words. Yeah. Yeah. So I think we've, we've seen some of this already, haven't we, with inflection how, how language is being inflected as we use it and, and things like that and right at the back. So I think most of the languages that we can study, they have, they all have different specialities. So for example, if I write a book for a particular language, I thought, well, then I guess we got the same thing. Yeah, right, right, yeah, yeah, yeah. I mean, I think we can make. Yeah. Again. Yeah. And that's that's a really good point. And I think if you if you look around at in terms of the support of different voice interfaces like Alexa, we, we know it's pretty good in English and about eight other languages, but then it gets really quite bad very quickly. And that's actually because supporting different languages for language processing is actually quite hard. Like it's like you're suggesting. Okay, that's great. So what we are going to do is, is dive into a little bit of the history of linguistics. First of all, just to juxtapose these these definitions here, as we already said, linguistics is the study of language, the scientific study of language, and it includes things like analysis of language form meaning and language and context. And traditionally it analyses human language by observing the interplay between sound and meaning. So the sound and how we speak is also part of language. That's our linguistics. That's an area called fanatics, and it's not something that we are going to be so involved with in this in this module. But as you can imagine, once you go into voice based interfaces that transcribe things, we say to computers of course. So transcription. Phonetics is a really important part of that. And, and kind of phonetic models of language become really important to that. So with natural language processing and I underline this here, this is actually in this definition is actually a subfield of linguistics, computer science and artificial intelligence, and hence why we also need to understand and appreciate some of the linguistic foundations of of natural language processing. Who here has studied linguistics in the past? Like Moore had a module on it a little bit, maybe. Yeah. Not too many views. Some of you have heard about it. Some of you have had heard it in school. Some will. Well, some of this might look a little familiar to you as we're getting through it. And don't worry, this isn't becoming a module about linguistics all of a sudden. But yes, so I think that's enough on that. Moving on to giving you, again, a very brief history of linguistics. It is really an ancient study. It goes back to something like the sixth century before Christ. So really quite a while ago that people started to study language. There are things like formal descriptions of the Sanskrit language. It's a very early kind of texts. There are other kinds of studies and description of languages continuing in China in the fourth century B.C., and then studies of Arabic and Hebrew in the Middle Ages. And there's also some of you might have heard of the of the Greek philosopher Aristotle. And or Aristotle. And Aristotle is known amongst a lot of other things for the study of rhetoric. We use that word now nowadays, don't we, to mean to mean a certain thing. But really, it was kind of the study of persuasion, the art of persuasion, how people and at a time, you know, societies were very much oral cultures. So they would, you know, not, you know, write as many texts because because it was hard to this before the press and so on. So people couldn't very easy write down texts and distribute texts. It was very much an oral oral tradition and hence things like persuasion. The art of persuasion became, became a really important part of our society then. Yeah. So you can see how and this is really just a few a few, a few highlights. To give you an idea how this is indeed a very old endeavour, then what we understand as modern linguistics really began in the 19th century. And so a person that is credited with with founding modern linguistics is Fernanda Saucier is a Swiss linguist, and he's been credited with establishing linguistics as a discipline in its own right. And one thing that he's known for, together with Charles Pierce, is semiotics. And we'll talk a little bit more about semiotics now, because it's a really important part of sort of backdrop to understanding a lot of what we a lot of the challenges we encounter also in natural language processing. And nowadays, there's a large number of subfields today to linguistics, including psycho social and computational linguistics. So what is semiotics? Who has heard of semiotics? Again, a few of you have, but really just a few. And so this is the semiotics, the study of signs or sign processes. This is any form of activity involving science. So importantly, it must include the production of meaning. Okay, so what do we mean by science? What is an example of a sign? A sign is a word. A sign is a sign. So if you take, like, street signs, for instance, literal signs. But also anything we interpret as a sign. Right. Well, and and that can be nonverbal communication. And that can be just, you know, I don't know, a bottle a bottle left on the table can be a sign, whatever it might be. So the sign is anything that communicates meaning other than the sign itself to the interpreter. And that can also be intentional or unintentional meaning. So if you think about things like, yes, in language, typically we communicate intentional meaning, but if I display some symptoms, involuntary coughing, for instance, then that is not a an intentional communication. But clearly it is communicating something perhaps as an indication that somebody is sick. Really important is this is this sentence here? A sign is only a sign if if somebody interprets it as a sign. Okay. So this is really getting quite philosophical. So you might see the point of that is you might try to communicate with one another. And we humans frequently misunderstand each other, don't we? And we have to clarify and say things again and rephrase things. So if you think about the sentence, you know, the difficulty is that oftentimes the things we intended to be sign don't get interpreted as a sign. So we have to say them again in a different way and so on. And since Charles Pierce, we understand semiotics as a triadic relationship or a triadic system, a relational system between human sign and meaning and triadic relationship. Let's let's put it on the slides can look a little bit like this. So this is the trading relationship. And many of you well, all of you, I'm sure have heard of the terms, syntax, semantics and pragmatics. And and these are the three dimensions of semiotics. So we have a syntactic dimension which refers to the vehicle or the representation or the signifier. And we have the semantic dimension, which refers to the designates and if you speak Latin or meaning or the signified, so the signifier and the signified. And then we have the pragmatic dimension with the interpreter or interpreter, and we'll use that if you want. And this is information as a sign based on Pierce Morris semiotic triangle. Okay. So let's get into this a little bit and illustrate this with some examples. So, for instance, here the sign as sorry the signified is. What's this? Is it what we call this in English? It's a tree. Right, Exactly. But if you are if you are a German speaker, you might call this bound. Or if you are a Spanish speaking, you might call this Oh, boy. And of course, every language has a different name for us. So that is the signifier. So you can see how the syntactic dimension changes depending on the language in this case that it just spoken in. And of course, now if I don't have access to this, I cannot see the tree. So I have to rely on communication. So I have to rely on somebody telling me about it. Then this this relationship here becomes significant, Right? So what if I don't understand German, but somebody says, Oh, look at that bone. Well, I'm not going to do I don't understand it. I can't understand it. So if this this dimension is not this relationship is not available. And, you know, the the syntactic dimension is the way in which we understand this, right? So we have to understand what the syntactic dimension signifies to actually get from here, from the word to that to that what the signified is. Right. So, so what? The point of the point that I'm going to try to make here is that this relationship here is arbitrary. What do I mean by arbitrary? So there's no natural law that says the word tree signifies that. Right. There's no natural law. In fact, we know that because every language has a different sound, a different word for saying what that signifies. All right. So this is, in fact, based on a contract, what we say, a contract between speakers of the same language. Speakers of the same language. We understand each other. We understand that the word tree signifies this. If we are Spanish speakers, we know that the word able signifies that and so on. So think. And you've got to, I think, take away from from semiotics is that that you know, there's a there's a kind of a complex interplay really between something that seems really quite simple and something that will we will we will frequently make reference to regards to those three topics within natural language processing. So syntax, semantics and pragmatics are three of the core concepts that are still important in natural language processing. And we've already encountered this, haven't we, in the pre-processing pipeline that Jeremy talked about on Thursday. So where do we need to deal in the pre-processing? Where do we need to deal with the syntax of words? Anyone want to hazard a guess? Yeah. I'm sorry. Annotation. Yeah. So? So. So part of speech tagging. Right. So that's. That's actually why I wanted to get up here is. Yeah. Part of speech tagging and, and also the use where we also have the. So okay so what I didn't actually get to is this question. I forgot that. So where can you look up the relationship between sign and meaning. I've given it away. Now. I have. And I. So if you're unsure about this relationship, say if you are coming to a language you know that is not your mother tongue, you're learning a foreign language. Many of you are not native English speakers in the room. I'm not either. So where can you look up the relationship between sign and meaning? Yeah. Dictionary. Dictionary. Or another word for that is the lexicon right now. So where's Alexa? So clearly that is what gives you, you know, it, it gives you the semantic relationship and you know, it explains to you what that relationship is. So that's the lexicon. So where in the pre-processing do we use the lexicon? Lemon Translation. Right. Sorry, sweetie. So when we are wanting to understand the basic forms of words we can look at, we can look at the lexicon once to just look up those basic forms of words. So you can see how the point I'm trying to make here is how the relationships in and from semiotics, which are kind of, you know, century old, more than a century old, are still and, you know, fundamental to understanding natural language processing. There are other linguistic influences in computer science more broadly. And I just want to highlight out of interest, really. One of them is the contribution that the linguist Noam Chomsky made with his work on universal and formal grammar. And so he introduced this concept of context free grammar, which essentially is a way to describe the structure of sentences in words, in natural language. And this has become influential in programming languages. And here's some examples of the what's become known as the Backwards North form. It's been being used, for instance, in the document type definition for ten out. And so it is a way of expressing, you know, as as the definition says, the structure of in this case, elements of an original text. So for instance, and these again, these are quite arbitrary, but here's the definition of a postal address field, which must consist of a name, part of street address and is it part. So you can see this is based on you can see how this is based in linguistics, Right. Because it goes back to this idea of context free grammars. So, you know, and then in other example, there is how the HTML element itself consists of a head and body. And then, of course, we can arbitrarily add in document type definition. We can add elements to that, but basically define the structure of our HTML documents. Okay. And so what about grammars and NLP have graphs that sort of these grammars have? They've been influential in natural language processing? Yes and no. I think initially they have been. So in the early days in the 1970s, when kind of the early days of natural language processing and automatic speech recognition or ASR came about. So by we really mean this the transcription primarily initially of spoken language. Yeah, that's the difference. So it is of course additional to natural language processing. Whereas in NLP, we typically take that for granted and we start with the text, whereas with. So if we want to process spoken language, we need an assault component before we can do an LP. So initially these systems were dominated by rule based expert systems and this is rule based. Exposed systems are a kind of an old form of A.I.. And at the time, researchers were struggling because, for instance, they apparently did not make the distinction between training and test data. And so, unsurprisingly, what happened is that their results look good because they were all based on the same training data and they didn't test them with a different unseen set of data, which is of course, one of the, I think, you know, one of the core parts of principles of modern machine learning techniques is that you you must test the performance of your algorithms and of your machine learning models on unseen data. And, and grammars were written that only apply to a few verbs and so on. So I think what transpired is that these grammar based approaches were really quite difficult. And these, these sort of attempts of writing these grammars for for computers to understand language weren't really very successful at all. And this whole period has become known as the AI winter. And so that period of time where there was a lot of disillusionment around the actual performance of AI at the time. And these kinds of approaches are also known as symbolic approaches, whereas heard of symbolic AI. Plus head of statistical A.I. for some symbolic A.I.. Okay. So nowadays you take that for granted. Really? And we take that for granted. Because when we talk about A.I. nowadays, typically most approaches tend to be statistically driven, at least to a certain extent. But really, actually, that's not quite true, because in an area like NLP, there's still a lot of symbolic work in that. And so when we talk about something like the linguistic aspects of NLP, you know, this is really still based in what has been termed a symbolic approach to A.I.. And nevertheless, I'm going to talk a bit more about the statistical approaches to A.I. which gained ground over the following decades, and I'm going to come to talk about that in the following part. Are there any questions? Okay, let's dive right in then. So let's talk a bit about the success of statistical approaches to AI and in particular to NLP. So this does a very good job, this article of talking of of explaining this in this context. The authors are from Google and they've written this article in 2009 on the unreasonable effectiveness of data. And in it they say that one of the biggest successes in natural language related machine learning has been statistical speech recognition and statistical machine translation. Okay, so since you go speech, what's statistical speech recognition again? What is that in other words? Essentially the transcription of spoken language into text. Yeah. And statistical machine translation is of course. Exactly is exactly that is translation. So, you know, when you were talking earlier about about Google translate, you know that is very much using. Statistical approaches to that translation. So the machine has possibly no knowledge of the meaning. It just knows to map the words. It just knows to do the translation based on huge numbers of statistics, using a huge amount of data on which it on which statistics or statistical machine learning is applied. And the authors here call the web the largest training set of the input output behaviour that we seek to automate. And the web as of today contains at least four and a half billion pages of text, and that is largely text. So there's a lot of text there, of course, and that is perfect if you want to merely translate language because it is an input output problem where your input is a string in a certain language and your output is that know matching string in a different language. So it's a it's kind of a very simple input output mapping problem, and it's perfect for statistical approaches to machine learning or statistical statistical approaches to A.I., in other words. And so the semantic relationship is essentially just learned from the statistics of the search queries and the corresponding results. So there's no real knowledge in the system about semantics at all necessary to do this. So in other words, memorisation is a good policy if you have a lot of training data, right, which is the case when you have got when you can use the web which is exists in so many different languages, it basically is a huge database of probabilities, of short sequences of consecutive words or engrams. And Jeremy has already talked about anagrams and he will talk more about about in graphs and how we do processing with engrams as well. So is it all statistics and natural language processing? Why are we even still talking about linguistics coming around back to that question? And although many applications are predominantly statistically driven as it as the authors have highlighted there, so automatic speech recognition and transcription and translation are predominantly statistically driven. However, the problem is that there are there aren't enough datasets that are available for traditional NLP problems like document classification and part of speech tagging. So the understanding, because you cannot use large statistical, large datasets and machine learning very easily to to understand parts of parts of speech and things like that. So the fundamental structures of language and grammar and these kinds of concepts are difficult to solve with statistical approaches. And that's why linguistic concepts, as I said, which belong to the symbolic approaches, do remain important for many aspects of the And NLP pipeline is again to remind you have shamelessly stolen out of German slides a picture of the of the pre-processing pipeline here where as we've already seen linguistics play into annotation and limitation and stemming at the very least. So let's talk a little bit more about syntactic and syntax. So what is that? Syntax largely refers to the combination of words such as letters into words and words into grammatical sentences and so on. And so in programming, we all we all have all seen and encountered some syntactic errors when the code is not well-formed. So the code might not compile, for instance, because of syntactic error. So grammatical doesn't mean that the sentence cannot cannot be nonsensical. So something can be syntactically correct, but semantically incorrect. All right. So here's an example. Colourless green idea. Slip furiously. You know, this this is a grammatical sentence, but it doesn't really make any sense. So it can be it is a syntactically correct sentence. Um, but. But again, it doesn't mean that it's not nonsensical. What this example shows is this concept of constituency, because the sentence consists of a noun phrase. So we've asked for the sentence consisting of a noun phrase and a verb phrase. And in and in these two are consist of two adjectives, a noun verb and an adverb. Right. You can see how this is sort of broken down into different different parts, which is known as constituency. So this, this, this concept of noun phrase and verb phrase is quite important when we think of things like engrams. So things that consist of multiple words. So we've heard of, of unique rhymes and diagrams and engrams. And of course, this one, this is an anagram of three colourless green ideas. Right. So but there's a it is in language important to treat these things as things that belong together and not as always as individual words. And that's why this this concept of noun phrases and verb phrases and so on is quite important. And talk a bit more about syntax and morphology. So morphology or is is what guides the syntax. So for instance, send is a morpheme or an atomic unit. And here are some examples of combinations of different morphemes like sense sending and resend. So you can see that if affixes are being used here, suffixes of s eng and and re and these affixes usually make verbs. And the affix able usually makes adjectives and l y makes adverbs in English. Of course. And so in natural language processing, we've already encountered stemming and limit ization as key processes that reduce words to their stems or limits are. So what's the difference between a morpheme and a lemma? So morphemes are the smallest linguistic linguistic units in a word, that carry meaning. Take this example of unbreakable word unbreakable that consists of three morphemes. So break and able. But unbreakable dilemma of unbreakable is just unbreakable. So it's the same. So the lemon is the same as the word here. So but yeah. Just so you understand the difference. So parts of speech we've encountered spot parts of speech, tagging or annotation as one of the important things we need to do in natural language processing. And so we need to basically categorise the words into the lexical syntactic categories like noun, verb, adjective, preposition. Here are some examples of those. Then there are also minor minor categories such as complement sizer that if determiner like the a those that some and degree intensify is so varied for instance. And for an LP pos tagging or POS tagging is, as we have seen, a key pre-processing step. There are part of speech text sets that are useful tools that we make use of, for instance, in analytic. And you've seen this already in Lab zero. So and if you haven't if you didn't get to this, this part of the luxury, please make sure you just kind of work through the lap sheet. And so you have at least kind of covered covered the the main parts of this because that's going to be an important part of the coursework as well. A bit more about semantics. I already mentioned that lexicons are helpful resource for an LP. So as a resource that tells you something about the semantics of words, they are not just important to look up perhaps the lemma of the words, but also for this example of disambiguation. So Jeremy also talked about this in his previous lecture. We have we have certain words in the English language that are homonyms. So that means they are spelled exactly the same, but they can mean different things. So for instance, the verb sor is the past tense of C the past tense of two C however, the the the noun. So is a cutting instrument, right? So if your natural language processing algorithm encounters these words, how does it know which verb? How does it know what that word is referring to? Well, it has to do some disambiguation looking at the surrounding context of the sentence to to disambiguate which which word is actually being met here. So that's where lexicons are coming into, coming into play, for instance, as well as pragmatics. So this is the kind of you remember syntax, semantics, pragmatics, pragmatics being the third sort of the third dimension of semiotics. And where is this important in natural language processing? Well. Language is full of situational and contextual references. Right. A lot of the time what we are meaning in context when we say something can actually only be determined in reference to the situation or the context in which they occur. When I say, Oh, let's meet here tomorrow, tomorrow, how do you know where here is? Right? You know that because you are part and parcel, part and parcel to that situation that we're in. However, if I'm talking to you on the phone, you know what? You would have already know how to know where. What do I mean when I say here you would have already you know, you need to need to have this reference to the situation or the context. And these there's a shorthand for these kinds of terms that that give us this sense of situation, which is index ACLs. These index articles are indicative of situations where we must know because they why? Because they index they index a certain situation in a certain way. There are some examples of this here, this, there that. So oftentimes when we talk to one another, we say, oh, that that. Can you grab me that there? Right. So I point at something. So again, you can understand that as a human being because we are sharing the same context and you can resolve that and understand that. But if you say that to a natural language processing system, that is a completely different challenge. Right. Prepositions as well, like he and she and so on are a kind of really difficult to resolve. So in natural language processing and in conversational use interfaces, in particular in dialogue systems, we call this context tracking. So here's an example of that, where we have a fictional dialogue between user and Google, where the user says, okay, Google, who was the 16th president, president of the United States, Abraham Lincoln, comes The response was the 16th president. And the user then asks, How old was he when he died? Perfectly normal type of type of response from the human to something right in a conversation. But for a machine, the machine then has to resolve that what was meant by he or that the he this being used here is actually index equal to the response that was given in the turn four. Okay. So that's clearly quite, quite a tricky thing to resolve for a conversational system. So we'll encounter this again when we talk more about conversational systems or quiz conversation user interfaces later on in the semester. So these types of problems occur when we have systems where we have multiple turns, we call these terms. So here this is an exchange consisting of four turns and what what will become in what will become useful to understand this kind of these kinds of conversations is conversation analysis, which is a type, A type of sociological analysis which looks at in close detail at how social interaction is accomplished. Turn by turn. We'll see. We'll come back to this later. We talk about quizzes like a set. Okay. Just to summarise, we learned about symbolic and statistical approaches today, or some symbolic as they're sometimes called to an LP. And how importantly, you know, you can't just throw machine learning at it and it's going to solve all your problems. It's just not going to work for language. So there needs to be a level of symbolic approaches based in linguistics to do an LP successfully, at least at least as far as what we know today. And I tried to highlight some of the whole linguistic concepts that are important in NLP syntax, morphology, morphology and how it relates to part of speech tagging and pre-processing, NLP and semantics and lexical which are important to to dramatisation stemming, but also disambiguation and then pragmatics and its importance for turn based dialogue systems. Okay, so next lecture remembers in level three in this building. Your timetables will eventually reflect that, but might have not done so yet.
