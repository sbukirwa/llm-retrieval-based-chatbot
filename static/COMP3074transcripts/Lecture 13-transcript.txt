SPEAKER 0
We are basically starting a new chapter on conversational and voice user interfaces in lecture 13. And what I want to do is review a little bit the progress through the module that we've had, because we've kind of come to a point where it makes sense to do that. And before I get into an introduction about the conversational user interfaces and user experience, okay, So we've completed the first part of the modules. This is largely been focussed on the foundations of an LP, and you've learned various ways of practically applying techniques, processes and applications in the area of natural language processing. Of course you also get to demonstrate your learnings in coursework one, and that is due next week. Yeah, next week, Friday. So who's that? Who's actually already working on coursework one hence hands up. Keep your hand up. I was trying to do a bit of a count, so. I'm going to say about 50% of you, maybe a little bit more. Okay. Whose mouth, like halfway through feels like confident. Who feels confident That kind of halfway through. I'm kind of happy. That deadline is next week. Happy to submit next week. Want to get it over and done with about five of you? It's five or six. Good. And who was finished? Was finished? Who's completely done? And was like, Oh, I'm. I'm ready to submit. I'm bored. No. Okay, That's good. Okay. Um, yeah. So another thing is we have a quiz this week as well on Friday. Just another reminder. So, yeah, it's a couple, a couple of busy weeks for you coming up. We yeah, we are aware of the challenges and we're just let's just say we're thinking about it at the moment. And so we're where we are right now. You should be able to basically implement a basic but functional NLP pipeline. And of course, that's what you're doing in the coursework. And and that pipeline is there to drive an interactive and IP based system, also known as a chatbot. You also have got an appreciation of the theoretical and critical topics around A.I. and in particular the real world problems that there are on the complexities involved in designing and using these applications in real life. Our lectures, we try and sort of split them up a little bit like this. We have theory lectures or conceptual, more kind of more conceptual lectures. Of course, you know, this is computer science was all applied. So when I say theory is not the same as, you know, as if we're talking about some kind of theory of the cosmos or quantum quantum physics. And we are hoping that those those types of concepts that we teach you about are kind of around the critical issues and relating to the research, design, development and deployment. So all sorts of related to AI driven systems and the different stages in the ecosystem, if you want, of these types of systems from early conception through to deployment and use in the real world. A number of things that we of course touched on here are ethical issues around these topics, various biases that can exist in these systems, filter bubbles, interpretable AI to name a few topics that we've discussed and with. On the practical side, Jeremy spends quite a lot of time in his lectures giving you more of the detail that you that is required to do the coursework, but also beyond that, the kind of broader conceptual mathematical foundations and so on of an LP in the second half. These practical lectures will change to being about voice user interface design or conversational design. And that's where we are now that we are going to start that second half of the term. So and then also in terms of the labs, the first half, roughly half of the term focussed on the practical development of your chatbot in, you know, the practical work in national labs processing using Python Analytic, as you all know, and this concludes with course one which is due next week, the second half of the labs will be dedicated. It's not really it's not really half of the labs. It's really only a couple of labs. But they will be dedicated to voice user interface prototyping and this will be done in voice flow and we'll be giving you more detail about that basically as soon as you finished with coursework one. And that will lead into coursework too. And we be given more details closer to will be released shortly after coursework one is finished so you already know the schedule. Just kind of recap where we are now, where we are this week. This is this week. And so the lab will be a quiz, right? Friday will be a quiz in the lab and basically kind of the last chance to talk to us about your coursework related problems because that coursework is due next week and the were then due to release coursework to maybe next week. Okay. So that's kind of where we are in the schedule. So part two of the module, how does it relate to part one? Well, part two is all about if you want to if you want to think about what you've learned so far, the engine often NLP system, you will be learning how to create the cockpit. Right? So just the metaphor, of course. So something that you use to drive that the but the user interacts with so built the build the actual voice user interface to. The LP engine. And this includes practically that you will learn the design principles and techniques that help with understanding, designing and building and evaluating voice use interfaces with real people. It will be about the application of these principles and these techniques in the work that you are doing in the labs and in course for the course work. Number two as well. At the same time, we'll continue our work thinking about theoretical and critical societal and societal issues around systems, this time thinking a bit more about specifically voice interfaces and in the real world. Okay, so we are going to talk about conversational UI. So conversational user interfaces or UX stands for User Experience. So what are some of the examples of these types of conversational systems that you have used? And what worked well and what didn't work well about these systems. So discuss for 2 minutes with each other or think about it and then share with the class 2 minutes. I mean, like. Yeah. I'm not going to bother stopping the recording.

SPEAKER 1
We're just talking.

UNKNOWN
I guess I'm looking.

SPEAKER 0
Yeah. Should be read from this record.

SPEAKER 1
Let me just check the record. Yeah.

SPEAKER 0
I was doing. Okay. Okay. What are some of the examples of systems, conversation systems that you have used and what worked well and what didn't? Yeah.

SPEAKER 1
So I'm actually doing one of my short reports on Conversational Asia. Mm hmm. The biggest thing I realise is that they're not actually conversational agents because they don't really converse with you. It's like I can see, like, Hey, Google, like, how are you doing today? And it was. But then it won't last. It won't be like I like even if I respond after, it's not going to follow up on that conversation. So it's not really a conversational agent, so it's more like a voice assistant.

SPEAKER 0
Yeah, I think that's a fair point. The point is that it's not conversational. Arguably, it's the voice assistant and it doesn't follow up and have a conversation with you, I think. You know what me what conversational means in this context is very different to what an interpersonal conversation means. And so I think it's it can be very misleading to to talk about these systems as conversational. Yeah. So, yeah, yeah.

SPEAKER 1
So basically, all the systems you have for the Google are so obvious. Mm hmm. Obvious examples. Yeah. So they really will quite. Well, they still have problems with understanding certain accents. Because in the room they don't touch. Yeah. Oh. Because I believe they, like, processed the voice, the frequency of the likes. Mm hmm. Um, of the sound. So it. It's like, very noisy. It's hard to actually find the right ones. Mm hmm. And I definitely would like to go walk as far as the fact that they are quite massive. The, um, the models themselves. Right. To be connected to the. There's no way you can still. Mm hmm. But I would probably still be lacking if it were appropriate to ask for something that I'm not.

SPEAKER 0
Going to do. Yeah. Thank you for that. Any other. Any other thoughts? Comments? Okay, let's let's crack on. And we're going to touch on some of those things that you mentioned for sure today. So some introductory terminology to start off with. When we talk about quiz, I like to just call it Cookie. I'm talking about conversational user interface and that can refer to different types of configurations of that technology. It can be it can refer to text based chat bots or it can refer to voice based voice, write voice or voice user interfaces, or there are hybrid versions of GWI. So a graphical user interface, that's the standard g y, graphical user interface or Google Plus voice like Siri. So here's is a screenshot of Siri. So of course, when you talk to Siri, it will frequently bring up something visual, some visual information as well. And quite often it will simply say, Oh, I found this for you on the web or something like that. And then just present a website basically for you to click on. So kind of a hybrid between graphic kind of voice activated, but also graphically supported interfaces. And we have the UX term, which is user experience. And this is a broader term that encompasses the interaction on the experience of the interaction, not just the design of interfaces and information like you might do when you talk about interface design or user interface design, although these things are obviously related. The idea was that the of this term is that it's much broader than that. If you think about the way we interact with a lot of different types of services, a lot of the interaction also takes place over longer periods of time. You think about how you interact with your social media and so on. And it's not just about, you know, one interface that you interact with at one time, but it's also about how things are spread out over time and, you know, things like notifications, you know, that keep you up to date about what's going on within the social network and so on. This is all part of the user experience and UX design or user experience. Design is is also an industry profession, of course. So some of you might be thinking about a career in UX and there are different flavours of UX design in that profession. Obviously web development or app development interface or game development or game UX graphic and other kinds of user experience designers are out there for HCI research, which is obviously related to human computer interaction things a lot about user experience as part of what it what what we do in HCI. But HCI is largely an academic discipline. There are important touch points with industry, and particularly with the UX design industry and other kinds other kinds of research and development places in out there in industry. But HCI largely remains kind of an academic discipline. I would say there are many phases of conversational interfaces. You know, I would say that we have we now have more and more kind.

SPEAKER 1
Of.

SPEAKER 0
Of of the the actual kind of anthropomorphised robots or sometimes called also called cobots collaborative robots. So often times these are kind of thought of as social or companion robots, but they're also domestic robots, you know, that are there to do do a specific chore. And while you might not think of these as as exactly conversational, I mean, we've done some research where we made a voice interface for the for the vacuum cleaner robot so that you can talk to it and tell it where to clean and things like that. You know, it's not a million miles away to think of these things that perhaps you might not think of as conversational would also have a voice interface that you can interact with them by voice as well. And of course, we have the whole smart speaker ecosystem where, you know, you have different different kind of configurations of this. Some of these now also, of course, have displays as well. So they're kind of more edging into the hybrids. And so this sort of hybrid space where there's a graphical input into the graphical user interface present as well as a standard, you know, standard that was just which is just a voice, really just a voice interface as well. And these. I used, as you know, many How many of you are using something like a smart speaker? Just a quick show of hands. Not. Maybe not that many of you. How many of you are using Siri or a voice activated thing on your phones? Maybe slightly more. Yeah. Um, and. Okay. And then we also I think we're also bridging into new and different types of domains and application R's areas such as mobility and transportation. And where, in my opinion, you know, it makes a lot of, lot of sense to think about voice activated controls. And you might, you know, you talk to your satnav, but also maybe in future for sure, and some kind of advanced and expensive vehicles already you see that you can basically talk to your entertainment system in the car and, you know, tell it to play radio station and so on. Clearly, this is this makes a lot of sense because why? Because you're in a in a safety critical situation where as as a driver, you want to remain what's called a hands free is kind of a hands free environment. So your hands must be free. All that at all times, really, for the driving task and not be encumbered with fiddling with other controls. So it makes a lot of sense in this kind of safety critical and hands free ice free situations to have voice control as well. So a quick kind of summary of the smart speaker market and this is how it's been developing. And so to say it's a growing market. Just two data points here about how it's developed in in in globally and then what the market shares are. You can see that Alexa has got the biggest market share rising or increasing for, you know, 20, 2021 Google assistant, relatively close behind, perhaps growing proportionately. Siri actually being quite a small slice of the market only and then everything else smaller than that. And then there's a separate graph for China where there are and where there is a different ecosystem, there are different kinds of voice activated devices available. The adoption of these devices varies. So in September 2020, data was saying that I looked at that around a third of Brits own a smart speaker, which was a fourfold growth since 2017, which was essentially 2016, was really when Alexa was introduced to the UK market on in the European market, I think it might've been slightly earlier in the US. This is the smart speakers are the second most popular smart home device after a smart TV. And in the US is similar, a slightly higher figure there. And unsurprisingly though it's quite low in countries where the first language is not English. And you can see a comparison here for Europe, where you've got Germany, Ireland, France and the development of the smart speaker market. And while Ireland is English first country, but still quite low in comparison to the UK. So then just an update for this year. I checked what the numbers were for this year and it's actually grown, but only from like something like 29% to something like 39%. And this is this data was only for England as well. So 39% of adults in England owned and used a voice activated personal assistant or smart speaker device. Note, though, that this also includes voice activated personal assistant also includes Siri. Whereas if this data is only smart speaker so probably does not include Siri, so can't really compare the two very well in the US. The data that I found here. So it's about 35% of the population that own smart a smart speaker. This was in early 2022, so in March, in March this year. And also you can see that and some are saying that the market may be becoming more stagnant now. And so you can see that the rise is sort of, you know, not as steep now from from year on year. And so it might becoming might be becoming more and more kind of a more mature market, but also one where there is a year on year, lots and lots more adoption. Okay. Let's let's actually look at an example of what user interface interaction.

SPEAKER 1
Alexa, say hello. Hello? Alexa, What time is it? The time is 6:50 p.m.. 615. Hmm. Alexa, remind me to fix my clock.

SPEAKER 2
I could fix my clock on your to do list.

SPEAKER 1
Thanks.

SPEAKER 0
Okay. So, for example, then let's think about technically what's going on. When the user says Alexa remind was fix my clock. Hmm. And Alexa responds, I put fixed my clock on your to do list. So clearly, we have a kind of a request response model, right? So that we can we can already observe, you know, sort of basic structure of how these things work. User requests says a request and the device responds. So what is happening between the requests and the response? Now, you already know a lot about natural language processing. Clearly, there's some of that going on here. So what is happening, do you think, between between those two that you think is noteworthy? So I discussed for 3 minutes and then shared with the class.

SPEAKER 1
Hmm. This is a reference.

SPEAKER 0
The request and the response. Does it every time. Right. What are we thinking? What's going on between the request and the response? Yeah.

SPEAKER 1
They were discussing about the mind of a sister. Mm hmm. Like the other thing that reminds me of Adolf Hitler, like a brother. And then something like four months down the line, there are things that are important to do list. And there are things that do I do I do my does all the time. And so we strike different things. So if you're just thinking this is involved, think might provide a framework in this case that it's smart enough to understand the context. I'm not going to debate that. Mm hmm. It's really interesting to see how that goes for different things, like things that can be normal and different with virtual reality. Mm hmm.

SPEAKER 0
Yeah. So I think So what? So what you're highlighting is the relationship between something like Remind me and to do list. Yeah. So there's clearly something going on there.

SPEAKER
And what else?

SPEAKER 0
What else is happening between the request and response? So I think.

SPEAKER 1
Is just across the same device.

SPEAKER 0
What do you mean by processing the voice?

SPEAKER 1
She analyses the frequency of. Grapes. That's what gets down to maybe not Walnuts spread like that. We learn some more. So she might have just had this object or something like that. It looks pretty similar, I believe, to natural language processing. We did already, with the exception. I think, Alexis, feel free to this moment going on behind the scenes. This particular times and it reaches the part that knows what it has to do. What was the question? What was should be the response? It does something. So in this case he puts it on the to do list and then it generates a response in some natural language and rubs it into the cell generator. I think that's it.

SPEAKER 0
Yeah, I guess. Any other thoughts yet?

SPEAKER 1
Conversational assistants now. Like, are they going to be currently doing that work or is it still.

SPEAKER 0
That's a good question. The question is most of them going through a recurrent neural network? I think a lot of them use transformers as well. So, yeah, I mean, I think it's I think it's it's a shifting landscape in terms of in terms of how it's designed. I mean, in many ways what you're encountering and what you're doing in the coursework is it's very much still how it's done, but scaled up as well. Are there any other thoughts and comments about what's happening? Yeah, Don, this I'm not I don't know. I thought I saw a hand.

SPEAKER 1
Like, once a month. Yeah. The rest of a sentence. And then it is. It still is. And I'm sorry if I say I first thought I of the sentence here.

SPEAKER 0
Yeah. So there's something going on called slot filling. Yeah. That's the term for that. So the slot being something that comes after in this case, this phrase here remind me everything after that just gets treated as a slot and that slot gets put in here, right? Yeah, exactly. So that's probably enough for now. Clearly, if you had read my slides, then you would have had the my kind of breakdown of this. And I think so. I think one thing that's noteworthy as well is the wake word, Alexa being the wake word. So the processing of the wake, what actually happens locally on the device without it connecting to the Internet. So every time. Because otherwise it would have to send everything to the Internet all the time, wouldn't it, for it to get transcribed all the time. So the Alexa the wake word processing actually happens on the device. And it's the same for Google and for Siri as well. And then everything once it hears the wake word, everything that comes after that and it hears and listens to it gets sent off to the cloud to be what? Transcribe. That's the word, right? And that is done by by something called ASR. What's an automatic speech recognition? Well, the answer I was doing is essentially transcribing the acoustic. The acoustic, the acoustic wave that is the capturing, the voice input, the sound. And there's something called an acoustic model, right? Acoustic model. Then maps the the sound that it's hearing to to worsening the English language, which then allows it to be transcribed. So what we get is the acoustic model based transcription, in other words. And then we also get something usually called a confidence level that is also returned from the ACR. So the confidence level is how confident the ACR is that What is presenting back to you as a transcription from what it heard is actually matches what the person might have actually said. So if there is some noise in the background or if there's speakers overlapping each other so people talking at the same time, obviously that confidence will decrease. And then we have in green here this, this isn't an action or an intent. And, and well, I'm kind of right. This is let's just take it step by step. So that's what the ISI returns. Right? And then the now you basically then we have a string. And then from then on, everything is the same as well, what you what is familiar to you from when you build a chatbot. Now you have a string that matches what the you basically the user input. But the difference that there is some confidence level because of the first step that obviously introduces some uncertainty regarding as to whether what the is outputting actually much as what the user input was because of the spoken language component of it. And we have you I know you are in our key components and I'll you stands for Natural language understanding. It's kind of a slightly broader term, I would say, than an LP which reads about the processing. So we now have something called keyword recognition where the where essentially the the keywords, you know, the string is being kind of processed for keywords, keyword spotting. And that will then get. Matched to an intent. So as intent matching, that's also something you're doing in your coursework. And then with that, so that gets matched correctly. In this case, remind me that token or those tokens gets matched to the intent to do list in this case. But it could also be, like you were saying, there could actually be some uncertainty in that as well. For instance, it could also fit with the reminder app or something as well. So in that case, there might be a problem and the user might need to be re prompted. And then we have the variable or slot and we have so this variable slots kind of the same thing. But in, in this kind of in these kinds of applications, these are often referred to as slots and this is called slot filling. Then we have as, as you said at the back there dialogue management. And so so that's all about figuring out what is the best thing to say now, right. And constructing a response. So it's vocalisation text to speech. Vocalisation is then you know the reverse where the the word I know you output essentially gets vocalised and generated as an audible response by the device. So with these types of technologies there's a whole whole host of challenges. Of course, as with other other types of AI driven technologies, we've seen voice is, is a profoundly personal trait, right? Voice voice itself reflects where you're from. For instance, it reflects usually what your mother tongue is, what dialect you're speaking, or whether you have an accent. It also typically reflects your sex or your gender. It it can also reflect sexual orientation, socio socioeconomic backgrounds and all sorts of other aspects. And there are issues with voice around privacy and surveillance as well. And I'm sure many of you know what some of those might be. And this is, you know, particularly given that oftentimes these devices are situated in home environments, essentially there's there's an issue there because the way it works at the moment very much is still that the audio gets sent off to the cloud and transcribed. So, you know, in other words, information from your home which might personal or intimate, get sent off somewhere and therefore, you know, there's a risk of a breach of your privacy or there's a risk of surveillance. These devices kind of being mis used as surveillance and surveillance devices. So let's talk a bit about the socio economic challenges here that that exists with regards to these devices. First of all, language support. So there are 7000 or so actively spoken languages on Earth and 91 of them have at least 10 million speakers. So they're kind of the, you know, if you want, they are the more prominent of those languages in terms of numbers of speakers. And still now and in late 2022, about eight years after Alexa first came to life, it only supports eight languages. So you see quite a large mismatch there between between the numbers of languages that spoken in the world and the language that supports and only dialects only in three languages in English, French and Spanish. There are there is research then on that that kind of looks at dialects, accents and pitch of voice, of spoken voice. This this paper here looks at gender and dialect bias and YouTube's automatic captions, voice user interfaces, you know, struggle with certain dialects. This is this graph here shows so this is word error rate so high as worse. Obviously it's worst for people from Scotland does not come as a surprise for those of you who know the Scottish accent.

SPEAKER 1
Most buttons on all the installed voice recognition technology. And I love to hear the voice recognition technology and a last and Scotland. It'll trade voice recognition technology. No, they don't. Scottish Accents. 11. Could you please repeat that? 11. 11. 11. 11. Could you please repeat that? 11% gives us immediately an American accent. 11. 11 sons are in a shell, man, isn't it? 11. I'm sorry. Could you please repeat that? Same thing with Jackson accent? 11.

SPEAKER 0
Okay. You can see where this is going. Absolutely nowhere. So we also have what the problem we have with the detection of voices is that it's also worse for higher pitched voices. So that means that it tends to be worse for women and probably also for children. So if you want to, you know, in terms of thinking about things as biased, there is a gender bias there and then also a kind of an age of bias, right? It works better for adults than for children. There's also issues around racial bias. This often has to do with the training data used to in those acoustic models. So the matching between the sound that you hear and the voice that is being transcribed, being biased and not being representative of a diverse population. This work looked at racial disparities in automatic automated speech recognition. So exactly what we've been talking about, and they examined five ACR systems by the big brands and what they did in their studies is they used those to transcribe interviews conducted with 42 white and 73 black speakers and found all five systems substantial, had substantial disparities between the two groups. And here you can see the average would error rate. So what error rate is a standard measure of discrepancy between machine and human transcription based on things like substitutions, insertions and deletions. So where you basically compare something that's transcribed by a human with something transcribed by a by a machine. So you basically use the human transcription as a baseline to see how well the machine transcription works. And the average white error rate was a lot higher for those speakers from the black crew compared to those from the white group. And the the problem, you know, the authors trace this back to the underlying acoustic models, as I mentioned, that were used by the ASR systems. And so when you think about the word error rate here, the authors are suggesting that a word error rate bigger than 0.5 basically implies that the transcript is unusable. And then you have 23% of the transcript. So this line here, 23% of the transcripts of the black speakers being unusable, whereas it's only 1.6% of audio snippets of white speakers resulting in unusable transcripts, thereby proving that there is a significant bias difference between these groups of speakers. There is an area of phonetics called socio phonetics. So what is phonetics? Phonetics is the study of the production and perception of spoken language. And there's a paper here that that looks at that from an HCI perspective, the study of social phonetics that is about the social factors that influence the production and perception of speech and how these are shaping social sociocultural identities. And we already know this intuitively about the work proves that accent and voice quality is influenced by things like geography, where you're from, where you live, sex and gender, age, sexuality, and social class. And if you think about how voice interfaces sound, however, the synthesised speech generally, generally they sound like it's a very modernist average. If you want voice, which has a kind of a mainstream accent. And there's definitely a lack of diversity in what the voice represents and the kinds of groups in the population that that the voice, the voice sounds like you might. There's also issues around gendering, synthetic speech. Why of all voice assistant generally given a female voice by default? Yes, you can change them, but typically the default setting is it sounds female. Why? Why is that? Why is that? So there's been apparently Amazon says, you know, they did some user trials and you know, their users prefer a female voice. There's also some earlier research that found people find female voices more agreeable and more pleasant. However, there's also it can also be quite inconclusive in terms of evidence. And one thing that's for sure about gendering is that also if you are gendering voices, you're also embedding gender stereotypes into devices. Yeah.

SPEAKER 1
Like, did it not look into the fact that, like, they might see the boys as a more agreeable person because you're not used to giving people's questions.

SPEAKER 0
Yeah, exactly. I think, you know, just because it tells it tells you it's only scratched the surface to think of what might be going on. Right. And, you know, so this report by a ESCO, which I recommend actually talks about, it's called I Blushed if I Could, which is one of the responses Siri Siri gave. She said something, something to it, something unpleasant. And the report criticises the kind of female servant or personal assistant stereotyped stereotype that's often embedded in the the ways in which these devices are framed and the way in which they respond. So there's some social science research that suggests that this also invites sexualised or gendered language. So calling, calling it certain sexualised words that are, you know, considered rude and so on, but in a in a gendered way, because of the way the voice sounds, you know, the way that the devices have those female voices by default. There are also there are efforts to create gender neutral voices. There is, for instance, one one called Q by a company called Equal I you. But these two, these kind of gender neutral voices haven't really found their way into mainstream, although I think Apple likes to say that Siri now sounds gender neutral. And I'm not so sure. But and they are I think the big companies are aware of some of these issues. But I think the default remains more or less the same. So then all all the big companies have had kind of surveillance and privacy lapses or issues with their devices because, of course, all of the devices send the data off to the cloud. And these that data then also gets listened to by other humans. It doesn't just get transcribed by it. And so all the time and that's it. But also, there is actually a lot of human labour going on behind the scenes to correct AI and to train the acoustic models. Right. So humans listen to the audio recordings to correct and manually transcribe what was said. Right. So, so every time you use a device, although probably it will just get, you know, transcribed by A.R. and no human will ever listen to it. There's still a chance that your recordings will get listened to by a human. And this has been and there's been some issues where these things have leaked out into the press and the stuff. Listen to the recordings. And, you know, contractors regularly hear confidential details as a result of that. And yeah, so that this this is this is going on for sure. Another thing to be aware of is the aforementioned labour, the human labour that is going on to make these devices work. Right. So there's been some alleged mistreatment or exploitation of workers. The workers are producing these linguistic data sets, right? They are training the acoustic models and so on. These were, for instance, in this case, not Google employees, which we know they're well-paid and these are the jobs you want, but they're actually underpaid subcontractors that Google then hires in. And, you know, these are these are jobs that you wouldn't want to do with banks, like a place like a call centre job where all you do is transcribe data. And one company is called Pygmalion, and there's been this scandal about it where this this this company didn't treat its employees very well, even though it's creating training data for Google's neural networks. And there's a lot. And don't be fooled, because there's a lot of human labour involved in these types of systems. So it's not the way the way the devices are designed. It's sort of designed to hide and mask all of that from us when we interact with these devices. But there is in fact, a lot of manual work going on to constantly train these systems and make them better all the time. Okay. That's all I have for today. So we have a quiz on Friday again. And we have coursework one due next week. Thank you very much.
