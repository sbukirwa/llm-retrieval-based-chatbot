SPEAKER 0
Okay. I hate violence. Screen. Okay. So today we're going to continue on our streak of advanced topics. Hence the very imaginative name of Advanced, a big part two. So as a quick warmup, just to see basically what you can pull out of your minds. I like to know without searching online, if possible. What do you think you're doing in the loop? A I. S and I've got a couple of minutes to just think about it. Maybe chat with your neighbour if you think they have a clue. And. And I'll see what what comes out of this.

SPEAKER 1
You have to be more. Yeah.

UNKNOWN
I've got. Yeah. I.

SPEAKER 0
20 today is. We have. Okay, Who is the brave soul who wants to volunteer and serve what they think? Yemen in the loop, I guess. I don't see any hands raised. Yes. So it can be thought of as a long life, more confrontational or conversational. So yeah.

SPEAKER 1
All right.

SPEAKER 0
So you're saying it more like a conversational AI then than just like a single one shot? There's something to it. Yes. I like the idea of a back and forth. We're going to get back to this. Any other answer? Yes. And I want everyone to.

UNKNOWN
You always want to know when your.

SPEAKER 0
Okay. So the idea of like an asking for information that, you know, we know in the 1890. All right. So you're going to be like, what, last week about like the full back of like asking. Okay, there's a little bit of that. So I think we got some good clues here over, I think like a back and forth and then like, I understand what you said, but I asking a human and and there is something to it, Right? So we know that we have humans, we know we have eyes. And so far we've talked about how do we train classifiers like parameters and then how do we evaluate them, what do we look in classifiers to see whether they're working or not. And I think here what I would like to emphasise during this lecture is the other thing that I put in bold here labelling, correcting, interpreting, which are other activities we can do with eyes that we haven't really talked about in details until now. So this is the loop in that human in the loop. And we're going to talk about this during the whole lecture. Oops, too fast, right? So the idea behind you might look is that instead of trying to replace the human, we want to augment them. So the idea of a conversation, the idea of a back and forth is there. Right. We have some issue in like modern machine learning research, which is focuses on automating things, which is fine if all you want to do is kind of move the needle a bit higher on some arbitrary performance metrics. However, it does create some issues. Data sets that we create tend to be saturated. It's very specialised models which don't really generalise outside of it, and keeping humans in that kind of loop allowed us to kind of avoid those issues of models over specialising over specialising and being useless outside of context or if that context happened to change. So I think I've referred before to the idea of concept drift, which is one of the main issues that we're trying to address with you mean the loop, which means that the statistical properties of a target variable. So think of it like the label you're trying to estimate change with respect to like some unforeseen, like the unseen criteria over time. And this causes issues because the predictions become less accurate. Right? So we talk about language and language changes over time. So if you trained a model on Shakespeare in English and then try to use it here, running into a lot of issues just because language eyes change, but it's the same for any possible. So there's different approaches to keeping humans in that kind of loop. Um, so I'm not going to go over all of them because that's to be, you know, an entire course in itself. But I'm going to talk about them a bit. Interactive sense making is more of a collaborative approach where the human is embedded in a visualisation kind of loop. We also have explainable AI or interpretable AI. So explanation, and that is in training. We also have adversarial training where, for instance, an A.I. tries to create a, I don't know, like an image trying to fool someone. And then then that person managed to create some labels. Or vice versa. If a human creates content meant to fool an eye. You know there's a train. It's to become better. Then we have active learning. I will talk about in the second part of the lecture, which is more focussed on sampling and then meta learning that we're not going to talk about. But it's very interesting as a research topic, which is more human guided training. Okay, let's go to start. So individual machine learning, you might, you will find a lot of links between this lecture and the first lecture from Joel, and that's normal. Just trying to reinforce some of the ideas. So a quick hendry's graceful. Let's just say that we have a general practitioner which has been replaced by an eye doctor. And you send that doctor your file with the results of your yearly routine Check-up And the doctor spits out one of the following predictions. So you can see here the first one big read prediction saying you are in immediate danger. The second one you do with any you have a high chance of being in danger in the next few days. The third one you do with any right now you have a high chance of being in danger the next few days because I detected signs of embolism. And the last one I knew with you right now, because the brightness of the skin at the back of your knee is highly consistent, is warning sign of embolism with a high level of confidence. You can do a CDP test to confirm the diagnosis. So which one here say one of these is like, which one would you trust more? Would trust a more be? See. Really? Oh, wow. So smart. Right. So let's go over all this, right? The first one, I mean, it's not wrong, per se, Right? But it's very general, very high stake prediction. You know, you're in danger right now. Don't know why, but you are no justification whatsoever. So you're in danger of something good, but not good enough. Second one, a bit more precision about like the scale, the scope of the prediction. Third one, you actually start to get some actionable information. Right. If you already knew that you were at risk of an embolism, you will take this very seriously because it means you could die like within the next couple of hours or days. I don't know. I'm not a doctor, so it needs to be acted upon urgently. The fourth one, finally, it's more of a complete prediction. It gives you an explanation of why. Which means that if you know, for instance, that your camera is very bad and therefore the diagnosis is based on a bad camera, we've take the picture, which is not good, then you know that you don't have to take it seriously. However, it tells you what what it thinks it tells you. Wade thinks it gives you some level of confidence and then he tells you how you can check. Basically. Always good things to have. So here, right. We can see there's multiple criteria that we need to take into account when we output a prediction. Right. First one, what if the prediction is wrong? And usually this is the most you know, the highest concern. If there's no actual problem, then the hospital gets overloaded with people who think they're going to die. Not great if there's a problem, but it's downplayed. But you're actually in danger. That's also a problem. If the problem is different from what is stated, then you might be wasting time chasing the wrong diagnosis. So all of these. Right could have huge impact, like in any kind of context. The second thing is what if there is an alternate explanation of here? For instance, if it's Bayesian is entire diagnosis, they based on some reading and then you know that they're reading the sensor. That reading is not accurate, then maybe there's an alternative explanation which might mean that you don't actually actually at risk of embolism. But there's something else. Basically, you don't need to know what is happening. Like, why did that prediction happen? And all of those questions can be answered by interpretability in your predictions as something that Joel mentioned on Monday. And we learned a bit more details today. If if you can get one thing from this lecture is that you should know that trust in a machine learning algorithm, it's built mostly in how you handle the mistakes and know how you handle the successes, like if your algorithm is right most of the time. But then when it's wrong, it kills people. Nobody is going to trust. It doesn't matter how good it is. 99% of the time. So, yeah, to get that that little thing in my. Hmm. I don't think I need to go into a lot of details on this. Interpretability brings a lot of advantages, right? You can have a human experts double check your results. Always good when there's a high stake kind of prediction. Like a doctor, you can have a human user take a liability for what's happening. Right? Because if you have a doctor and the doctor says the algorithm is right, then the liability is transferred to the doctor, because now they validated a diagnosis. And more importantly, in your case, because I don't think any of you are doctors, an AI or machine learning developer can actually debug the inference process if you know what your prediction was made, you know why it may be wrong and then you can take action to fix it or put some warning signs that some things should not be taken, you know, too seriously. Interpretability also brings some drawbacks. It is additional work on you as a as in the AI developer usually comes at a performance cost. So for all of you maybe be doing the machine learning module, you will see that the very thing that makes a neural network work better than any other algorithm is what makes it less easy to interpret. So you have a trade off to find the basically between I want to understand my prediction or I want to, you know, have the best prediction possible. And finally, what does interpretability even mean, Right? Interpretable to whom? To me, an expert to you was to do it to someone down the street. In how much time? If you have a self-driving car, you don't have 5 hours to understand that it's telling. Why sitting you there as a pedestrian in front of you. It is a split second decision. An interpretable at what cost? If you double the competition time, that might have a big impact on the actual application that's using the A.I. and therefore makes it not worth the cost. So all things to keep in mind when when we talk about interpretability, there's many taxonomies of interpretability. And Joel mentioned one on Monday. I'm going to go musky on that one. So intrinsic versus post hoc, intrinsic means it's a property of the model. Post hoc means it's something we add on top of it. Model specific business model agnostic. So model specific means that is for a specific class of models that for neural networks or for decision trees or whatever model agnostic means, it's a feature of all models. And finally, local versus global. And that's mostly focuses on what we are trained to explain. Local means that we are interpreting a single decision, a single prediction. And global means that we are interpreting the overall model. Why did we behave that way? So I'm going to go over a few of these that I've picked arbitrarily because I think the most interesting and convey the message better. However, you will find that there is a wealth of literature on this just because interpretability is quite often what makes a model useful versus useless in a professional context. This is the tree. I think the adult talked about them on Monday. So this inventory tend to be inherently interpretable just because of their prediction model. So their model classification as this finite sequence of discrete decision. So basically you just go down the tree until you reach a leaf. And that's your prediction. And algorithms like three, four, five and 83, which learn that tree by minimising some error metric you don't have to worry about. And interpretability in this case is trivial unless the tree gets very big. You could have a tree with a thousand different levels. And then in that case, you know, even though technically it is interpretable. Good luck trying to get anything useful out of it. Another one that we've seen. K nearest neighbour classifies based on, you know, distances. So here we have a new document and that was always closer to blue documents. So it's blue. Fine into validity is quite trivial, right? We know how to explain this unless, unless the space is a very high dimensional bonus. Two dimensions here. If you have a solid dimension, 2000 dimensions, then trying to explain why it's closer to this thing, then this thing becomes a bit trickier. We also have model agnostic methods, so the stick methods separate the explanation from the model. So we want to separate things. And then when we use that, we use plots, text, feature ways and things like that to basically build an explanation that tells you why a prediction was made. We have some desirable properties for a model agnostic explanation system. We wanted to be flexible, right? Otherwise, it's not really agnostic. Want it to be flexible with explanations. So be able to kind of output different type explanations. And then we want it to be flexible with respect to representation. But this is not very important. Forget about this. Let's go over a couple of example of modal agnostic methods. A global surrogate model is, I think, one of the most. The easiest one to understand. So when we try to have a global surrogate model, what we do is that we train a second simpler model to approximate a model and then use that to explain it. A second example here, let's say we have a super vector machine that we have trained on and that I said that I'm going to call X and Y. So X for the data, Y for the labels. Then we classify that in a set giving us X and y hat y hat being not the actual labels, but the labels after classifying with the support of the machine. Then we want to be able to explain this as sort of the machine is very hard to explain. So what we do is that we train a decision tree not to predict the actual labels, but to predict the labels which have been output by the super vector machine. And then if the accuracy of the decision tree is good enough, we can use it as a tree to explain the decision of the the machine. So we train a model which is simpler to approximate a model which is complex, and then use that simpler model in order to explain our predictions. And you can do that with any sort of models. This is a very general algorithm. It works quite well. And usually, you know, we want to have like a model which is quite easy to read. Let's look at local surrogate model. So look also at model are used to explain individual predictions.

SPEAKER
I mentioned Joel mentioned Lyme on Monday, which is, I

SPEAKER 0
think the best known one. I want to know basically, why did the model make this specific prediction? What effect did this specific feature value have on the prediction? And the way line works basically is that we generate a dataset which is set of all of documents which are very close to a specific point. So if you have, let's say, lack of image, we generate images which are very similar but slightly different from that image, and we train the model on that specific image. So for instance, here we have a dataset and then we just make up a fake ID, a set around a specific point, and then we train the model on that one. And then we see basically by varying some features, whether it changes the prediction. And that allows us to understand the effect of the effect of of the features on those predictions. Very useful, but is very sensitive to quality features, which means it's not always applicable to any sort of datasets. And it means I'm the type of of interpretable machine learning is the explanation by examples. So typically they are quite model agnostic. And what they do is that instead of trying to do some fancy things with the model to try to produce an explanation, what we do is that we select specific instances, specific documents, specific images, depending on what you're trying to classify. And we use that to show some properties of the model. So it could be something like this decision boundary with some kind of counterfactual examples. Or it could be some representative examples in the data. Or it could be which examples are the highest influence on the model? We're going to go into a bit more details here. So a counterfactual explanation. That's the first one in the list right here. Oops, sorry. Counterfactual issued is an explanation which is related to something which has not technically happened. That's what counterfactual means. And basically what it looks like is this If feature X was higher, the prediction could have been positive. Is feature has was lower, it could have been negative. Basically giving you a specific point and telling you if this goes higher, the prediction changes or if this goes lower, the prediction changes. It's used to identify at some level potential causality in the data. And one of the examples I think visually has is that it's very easy to interpret, very flexible, very easy to implement. And main advantage is that sometimes there can be many, many explanations for a single prediction. So yeah, just a spoiler alert. So I'm not going to give you one minute to think. Can you think in your daily life of some example of counterfactual explanations that you might have encountered and feel free to talk amongst yourself? So just to repeat, counterfactual means that, you know, by varying a little thing, by tuning a little feature, then you change an outcome. Please communicate amongst yourselves. You know anybody who wants to present a guess. Yes. Michael. Yes. Oh, that is that is a good example of that kind of factual reasoning, is that like if you, you know, put your alarm just a little bit lower than you made the cut off of when the class starts. So the outcome is I make late on time. I'm on my own time to class. Yes or no? And then your feature is the alarm time. Anything else? Yes. So the guy first. Yes, this is I love this because usually it's the typical armchair sportsperson. Like if only he had done this, they would have won. But technically, there are like an infinity of other potential explanation that could have, you know, move the needle one way or another. Yes. Yes. Yeah. That's more like a binary thing. But yeah, that's. That works too. Yeah. Issues and have switched on. Yeah. Yeah. I mean, that comes back a bit to what he was saying about sport, right? It's like you have like one explanation that you're linking to thing this all good. So those are very good examples of kind of factual. I have one that you might find very upsetting. An exam, right? This is in boundary 40%, grade 50% different masters over that you pass under that you fail feature your answer to a very challenging question in a quiz. If only your answer will change to a different value. You've that been moved slightly higher and past is really the wrong value. You move slightly lower and fail. Very, very dramatic, but I think it's a very nice example as well of counterfactual reasoning, right, Because there's many potential things. You have many questions, many answers, but there is a set of answers that move you towards success. And the set that move you to was not success. So it is good. This is going to so counterfactual I think is is is one of the easiest one to interpret but also one of the trickiest one because you can trick you into thinking you understand a lot more than you actually do. And finally, another one of prototypes, which is more used to actually understand the data and not really the model, but in a lot of cases and the setting of data is basically equivalent to anything understanding a model like K nearest neighbour, for instance. The data and the model are basically the same thing. So what we do with our prototypes is that we have two types of data points. We have what we call the types which are representative like documents or examples in the data like this ones here, right? You have a nice cluster here centred on this. And then we have criticisms which are data points which are actually outside of the sphere influence of this. So all the outliers and then we can use that to basically generate a, a, an explanation for a key nearest neighbour prediction for instance. That's all good. So now let's move on to the second thing, which is active learning that I put in the first one of the first slides. So the motivation of of active learning versus standard supervised learning is that in a standard supervised classification, for instance, you have your training data with labels. You've seen that in the lab, right? And then you just randomly sampling them, training your algorithm and then testing it on a bunch of other data that was held out in the beginning. And that's great, right? But that's not all of it. In active learning, what we do is that the machine learning starts from scratch. No data, nothing. And then the machine learning algorithm actually assembles the data itself and then tells you what you need to label. So that's kind of a reversal of roles here where the machine and the algorithm asks for your collaboration in order to improve. Now, another quick activity, just the goal of minutes. So the general idea is this, right? We select a bunch of data points based on some criteria. We query the human user, usually called either teacher or oracle, depending on the papers or books that you read to label that batch. And then we retrain the model and then start over again. So now, if you were a classifier, I know it's hard role playing here. If you were a classifier, how would you choose which document or batch of documents to send to the user for labelling? And maybe 2 minutes. Feel free to check it out with your neighbours.

UNKNOWN
I mean that I.

SPEAKER 0
I saying? You know the answer. I think. Oh, no.

UNKNOWN
Very. I have.

SPEAKER 0
It's definitely getting closer and closer to the. I'm reporting this right now.

UNKNOWN
When we were here.

SPEAKER 0
What is very sleepy to do. What is happening. Anybody who wants to be brave and hazard a guess. Yes. But if if. Okay. So basically, based on distance, what you're saying is that, like, if there's a bunch of stuff appearing like a law in that area of the space, then label those. But don't you think that if you label all of those, you're going to be spending a lot of time labelling pretty much the same thing over and over again? It's an interesting it's an interesting approach and I'll leave it behind and then see whether there's any other. Is that a Henri's? Yes. I mean, I think I think.

SPEAKER 1
That what you see. Like a lot of the same sort of ways. Over again. We want to be like, okay, so like, Oh, that's like mom, dad, And then like. We want to work. That happens over and over again. You know that word, right? Uh huh. Okay. Parents. And then maybe, like, Catdog is like coming up. Also, like.

SPEAKER 0
We're talking about documents here, not specific words. Right. Is that is that relates to what you're saying.

SPEAKER 1
Like, aren't you just basically like, if. How about those words at the same like you? Just like about parents, right? Yeah, I see. So they would come up quite a lot in that know.

SPEAKER 0
Okay.

SPEAKER 1
So then you want to be like, okay, then it will be this stuff. And then it's like, catch all first alligator, and then you might. Okay. Mm hmm. It's not just in the way that you.

SPEAKER 0
What is the actual criteria that you're using to find those?

SPEAKER 1
Just the frequency of words like words that are not.

SPEAKER 0
Yes, there's some there's some truth to it. I'll come back to that a bit later. Yeah. Randomised completely you. But which one do you prefer? So in the beginning you're randomised, so you use them for like random documents and label them. Okay. Then what you say once you, once you have some data and you. Okay, so start with Randomising and then move on to something else. Okay. Anybody else? Yes. So you were first? Yeah.

UNKNOWN
Why don't we just take that?

SPEAKER 0
Kayla was probability of. Yeah. Yeah. Came and. Yeah. So what you say close probability you means. Do you mean the one which are the least certain labelling?

UNKNOWN
Yes. So that.

SPEAKER 0
You're getting warmer. There was a yes, my. Okay. That's good. That's good. This is interesting. I think there's a bunch of this is a trace of good answer in all of these. Not a complete one, but. But let's. Let's move on and then see basically what I was trying to get at. So the thing is, they're actually multiple ways of doing this. The four main ones, uncertainty sampling. This is the biggest one you'll see everywhere. Query by committee. Very cool, but not as popular. And then expected error reduction or modal change. Let's look at them. The most popular approach is uncertainty sampling. And then what I was trying to get you to get at and I think get there towards the end is that you want to label first the things that your classifier is at least sure about. So, for example, here, right, let's say we have this kind of decision boundary here. K Nearest neighbour algorithm. Then you really want to label. You don't want to label documents which are here. Who cares? They're well into the red area. Not not interesting. If you have limited amount of time, what you want to label is everything that's very close to those kind of that kind of decision boundary here. So that being you here, you want to 1 to 1 to label this one with the label, everything that's here, because those are going to have the highest impact on the actual model because it's closer to that kind of threshold where you move from one class to the next. And this is the idea behind this sampling. So if you do, if you use anything which can output some probabilistic estimates, then for voices for binary classifier, either probability or close to zero five. So like 045049051. Then it's a very good one to dissemble because this is going to give you a lot of information about the actual classification process. So that's the main one. And there's a lot of kind of subcategories of this because there's many ways of actually measuring that consistency. But they all fall in the same kind of area. The other approaches which are so unpopular, I put them all in one slide. Query by committee is also an estimate of uncertainty with a bit more complicated to train. So what you do is that you train multiple models. Let's say you SVM random forest decision tree neural network. You have a kind of committee of models and then to estimate the uncertainty of a specific label. You have them being classified by all the pacifiers. And then you have them get a vote. And the ones where the votes are the most tied up have the highest uncertainty and therefore the other ones to be sampled first. We also have something called expected model change. And in the model change, kind of active learning, what we do is that we select documents where knowing the label will produce the highest change in the model. So in those kind of approaches, usually you have a way of quantifying the change. So for instance, in a new network, you know, you have the weights of all those neurones. And what you do, we just measure the difference of the weights, basically see if there's a large shift. And that gives you an expected model change. But every model has their own kind of way of measuring that. And then expected error reduction. We selected documents where we know that knowing the label would reduce an estimate of the generalisation error the most. So once again here, I'm not going to go into details, but there are papers on that if you want to learn about it. We really want to reduce the kind of error in addition error, and then we find the documents that do that. Okay. So once again, did it keep you awake here? 2 minutes. Can you think of any reason why we would want to do that instead of just feeding the algorithm like a big batch of label data that we have? And feel free to talk amongst yourselves for a couple of minutes. If you're still awake.

SPEAKER 1
Long. Through. I.

SPEAKER 0
Okay. Anyone wants to. Is that a guess? Yes. Wasn't it quite recent? If it comes.

SPEAKER 1
Down to me something very nice, I'm thinking I may have.

SPEAKER 0
The accident. So robustness because you're sampling more important things. Yes, I think that's part of it. That's good. That's a good answer. Yes. I think. Yes. Saving time, saving resources. I mean, that's the main one for me, right? I think this actually covers all of the. All of the fame, right? Yeah. So, as you said, robustness. That's the first one. So data might change over time. Right. So by having a process like this, you can do continuous kind of labelling. And then if you think about the actual lifecycle of your machine learning application over time, you don't want to spend all your time retraining a bunch of things and then changing the model in the middle of production and then doing that over and over again for the rest of your life. So having sort of continuous labelling I think allows you to avoid kind of succumbing to the concept drift and the other issues which arise when data changes all the time. They don't may be expensive to label. And I think it's very hard if you don't know, really know it, to know how expensive some datapoints can be. Before I moved to Nottingham, I was working in a company doing some drilling for oil and gas. And we had data points which cost us $2 million apiece. So you don't just throw away millions just to acquire data. And that's an extreme, of course. But even if you deal with something like illegal data and then you want to hire a lawyer to label some stuff. Well, I don't need to tell you that the bill is going to be very upsetting. And same with medical data. If you need someone to label, I don't know, like images of of cancer experts and those experts are expensive. And so you might not have access to that much data. And in this case, being able to do more with less is an actual advantage. So I think the overall thing is that you care more about quality over quantity. So a few good labels that a point is worth more than many, many redundant and not as useful ones. And I'm coming to the close of the lecture here, so don't worry. The key point of this lecture is that we need to think of A.I. not as an isolated thing, but as a social, an ecosystem. Right. It's it's embedded in a system that interacts with human. And you need to give the A.I. ways to actually interact with humans. Other than, you know, making predictions and then waiting for the engineer to go in and then like, fine tune a few things through interpretation. You can inspect your model, and by inspecting it, you be able to debug it, which is always very useful. And then you can handle mistakes of the classifier or whatever predictable you're using more gracefully, right? You can have. Oh, I predict this is cancer. Because of those reasons, you should double check with an actual doctor. And that's always better than you know. Have you ever been able to use anybody? Have you use a web MD before? It is a common job and that whenever you have, it's always cancer. It doesn't tell you why. It tells you it's cancer. And that's not great if if you're someone who is very stressed by their health. And that's the case for many people. That's not something you want. So having kind of being able to double check and then produce an explanation is, you know, is always good to have an intuitive learning. You can actually keep a model learning through his entire lifecycle. So you can have it label more examples as time goes on and you can have it correct Previously labelled examples. Right. There's nothing stopping you from reassembling existing examples and then showing them to a human again, just to make sure that, you know, things have not changed. So just putting this decision back here. So now we have a more complete picture of the ways things can interact. This is not exhaustive, right? So don't take this as like gospel. There's many other ways humans and they can interact. But I think this is the more complete picture than the traditional. I train the model, the model output, something I'm happy and I've done. If you want to do some more reading, I strongly advise this one interpret all machine learning because it was posted for, you know, politics and politics are not specialists, which means it's very accessible and gives a good explanation of the way things work. And then active learning, there's a very good tutorial online on active learning for deep learning, but it's very applicable in most cases and can give you a bit more details on some of the math behind some of those things and how it works. And that's it. I'll see you all tomorrow for the lab.
