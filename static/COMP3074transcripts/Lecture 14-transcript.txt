SPEAKER 0
Right. We have a quite a lot to cover today. So this is not typically a lecture which I give. Usually Joel gives this one, but I think I know why because he's busy today. And so if if you see me and I look very confused by something I just said, that's that's the reason why. But I think I think I pretty much got it. So don't not worry too much. We have a lot to cover. If we don't manage to finish in time, Don't worry. None of this particular lecture is going to be on the quiz tomorrow so we can always finishing up on Monday at a calm pace. Today we're going to talk about basic design principles. So the lecture is in four parts. That's why I'm saying if we don't finish it, we may leave some of the ending for for Monday. We're going to talk about conversational design. We're going to talk about redesign, process, confirmation and error handling. Let's get starts. So this is the textbook from which a lot of the lecture content is actually extracted. It is in the library. If you want to have a read. You will find out that it is extremely informative by Cathy Farrell, which was the designer in Google. I think of the of their voice assistant. Let's talk let's start by doing a bit of definition work. The concepts of command and control versus conversational. So Joel touched on that on Monday with the length of interactions and what defines really the conversation. So on one hand of the spectrum, we have one shot interaction. So that would be like typing something in Google and getting an answer. And on the other hand, you have multiple turns, though. If, for instance, if you and I were chatting and, you know, we don't just blurt things at each other, we actually, you know, there's an actual conversation. Current smart speakers, if you have one, you'll notice that there are mostly one shot. I say mostly because they have some basic understanding of context. But you know, we're not going to get, you know, a very interesting conversation with most of them. And they all use a wake word like, for instance, Alexa, or I'm not going to say mine because my phone is listening or Siri or whatever. And then once that wake word has been detected, it is basically listening to whatever comes after, after that. So what we mean by conversational here, right, is not a proper human to human conversation. It's basically the most basic form really, of conversation, like acknowledging thank you with your welcome. I think it's one of the clear example of this, and we call these conversational markers. So the as if there's a bunch of conditional markers that one needs to understand when it comes to designing of we. So things like timeline. So if you're ordering things in time like first halfway and so on, I don't recommend positive feedback greetings. But there are many others, obviously. I'm sure you should just dig in your mind. You can think of many of kind of markers that you can let's do something good that you can, that you can use. We are progressively moving our assistants towards more conversational thing. So for instance, here, like a very basic if you ever tried the Google assistant in its early days, that was basically what was happening. You can have a query and then you try to follow up with another query. Then, you know, it would just not understand and redirect you to a Google search. And it was extremely frustrating. But now we are moving towards more context driven conversations. So, for instance, if you ask a question to Google about, let's say, who is a 16% of the United States, and then you can refer to that and say, how old was he when he died and then where was he born and so on. So you can see here, I'm not actually repeating Abraham Lincoln or the 16th president of United States. It is implied in the conversation that there is an 0tt that he would the hell there's an entity which is which is. What's happening here. Okay. There's an entity which is kind of implied in the conversation. There's some principle. I'm sorry. I need to.

SPEAKER 1
This.

SPEAKER 0
No. Okay. Oh, here. Right. So I have to look. There's some principles that we have to follow here. I already referred to it. Keeping track of context means, you know, we need to basically know what is the topic of conversation. So, for instance, here, Abraham Lincoln, we need to be able to do something called core reference resolution. So that's what it means, a call reference resolution. This is resulting resolving he to referring to Abraham Lincoln. But it can apply to me things that places products or whatever you need. And this is a very basic kind of fundamental task in natural language processing. It's a very hard task. Don't don't think it is already solved. This is something that people are still working on. Some of the principles is to let the user be in control, basically, of how long the conversation should be. So, for instance, I don't know if may be an old news, actually, I don't remember when that was published, but there were some upgrades to Alexa where it was always listening without the Alexa keyword. And that created a lot of upset in the world because nobody wants to be interrupted by a robot when they're trying to do something else. Another principle is to set some clear user expectations. So paraphrase here has Don't let don't ask a question if you won't be able to understand the answer. So, you know, if you're programming we or bot in general, don't ask the user a question. If you know you have no way of actually processing what they're going to answer back. That's just creates frustration and make people extremely unhappy. So the principles to provide clear and actionable prompt, I refer to that in some of their earlier lectures on chatbots. So for instance, if you have, you know, two possible actions, then explicitly saying, you know, do we want to change it or send it after, if anything, about writing an email or if someone has just finished an order, Do you want to confirm the order? You want to change the order or join the cancel the order? If you ever interacted with like phone interfaces, you know those like chat bot audio chat bot being this something that they always do, right? Press one to do this, press two to do that. And this is just the NLP equivalent of that to be concise, right? You know, you don't want a chatbot to be chatting at you for like 10 minutes before it lets you answer something. So that's something that to keep in mind you have to be clear and you have to minimise the amount of information that you're actually pushing on the user to help the user understand what they can do. Just that. That's a concept called discoverability. So if you know, if you're interacting with the voice user interface but you don't actually know how you can interact with it, you need to be able to push the user towards discovering some of the actions that they can do and also to help their user recover from errors. So if there is a specific error, like for instance, you lost connection to the Internet, so you have no way to pass the information from the user, well, you need to let the user know, right? Don't just wait there until they figure it out. And also by giving some contextual help, like, for instance, if I say, what is the phone number for this store and they don't know what the store is, you don't actually stop insulting to the user. You just actually give some information and say this store is not is not in my database or anything of that type. So that's one of the key principles. Now let's move on to some redesign process and process a very short part. But I think it's very important because it gives you some ideas of the sorts of things you can do. You will be able to do for the second post work and actually might even be usable for the first people work. But so the goal of the design process when it comes to is right, is to bring from an early concept to a fully functioning prototype. So you'll recognise a lot of design tools that you can use in other style of applications. So if you've done some design of like know standard applications or any sort of really any applications, human centred design is usually the main kind of school of thought when it comes to redesign processes. And so this approach to make the system usable and useful by focusing on the user is blah, blah, blah, their needs requirement. And by applying human factors, ergonomics and usability knowledge and techniques. The goal here is to really make something which is as usable as possible. And how do we do that with voice user interfaces? The first one. Well, simple dialogues. It's usually the main, the main one, visual mock-ups flow prototyping tools and actual platforms and provide tools to do that. Let's go over a few of these. Simple dialogues, I think, is the most straightforward way of doing it. So you just basically write a simple dialogue between a V and a user, and then that avoids basically writing prompts in isolation, right? You want to be able to visualise the overall flow of the conversation and not just, you know, if you listen to this, if you hear this end to that one common way of doing that, actually this is kind of what I explained in my letter. You pick five common use cases for u V, and then you basically, you know, start with best path kind of dialogue. So it means the user manages to achieve the task they set out of sitting on to do, whether it's asking about the weather, ordering something on Amazon or whatever that is. And then you progressively move to writing dialogues where things go wrong, like the bot cannot understand the user or there's a loss of connectivity or the item doesn't exist. Basically, try to visualise what is what should be happening if something goes wrong. And this is very useful to illustrate early concept. You can think of this as like, like a user story in traditional traditional software engineering. And you can also have like a colleague or a classmate and then just basically read it like a theatre play to each other to see if it makes sense in their interaction part of it. The second one, Visual Mock-ups. So that's kind of relying on either on tools or like actual, you know, pen on paper and basically is, is, is storyboarding. So that's another basic kind of functional software engineering technique, right? And you can use kind of the graphical ways of representing the way things are moving. And you can just yeah, I mean, that's basically it, right? Is, is and yeah, it is especially useful if you have who uses a graphical component because storyboarding pure audio is not that interesting. Flow diagrams are by far my favourites. So I've referred to this a little bit in an earlier lecture and basically it is visualising the overall flow of your entire application. So I don't know if you can read this because I think it's a bit blurry, but basically it's it's, you know, it's a flow diagram for full chart. You just have a starting point and then some dialogue, many multiple choices. So if someone answers flight, then they go to this thing and that's another dialogue with some user interface. And then if they give the date, then you just move to the next part of the conversation and so on and so forth. And that allows you to basically visualise whether you have some dead path in your, in your, in your V, right? If there is something that leads to like a very complicated path where the user is going to have a hard time achieving the task, then you might want to revise your interaction so that you avoid this kind of, you know, this kind of interactions. Amazing. Okay. So the part for me is confirmation. And I think we're getting into something a bit more practical here. A key part of redesign is that you want to make sure that the user does what they actually want to do and not actually do something else. And the way you do that is by using confirmations, writing to get the user to confirm what they want to do. But you don't want to push it too far because, you know, I don't know if you ever used the Google assistant, but if it asks you three times, are you sure you want to create this event in your calendar? I'm guessing you could get quite annoying, right? And there are a few considerations that you need to take when choosing a way of a confirmation strategy. And the first one being, you know, what happens if I get it wrong? So if I accidently put a wrong date in my calendar, it's not a big deal. If I accidentally purchase something on Amazon that might be more of a problem or book the wrong flight, that could be also a problem. The other thing I need to consider is the kind of modalities you have access to. So if I'm using my phone, for instance, well, you know, you can still display stuff and say, which of those flights do you want? If I'm only using my, you know, Google whatever assistant thing at home that I don't have any physical visual feedback, which means that you need to be very careful in how you ask for confirmation. Yeah. And then, of course, there is, you know, some constraints based on the weather, etc.. So if you have a small screen, you don't want to have a list of ten possibilities. But if you're on the computer, it's a bit more feasible. And if you don't have any screen at all, then you want to minimise the number of items that you're confirming. And finally, what type of confirmation is most appropriate? Obviously, if you know, if just because you have access to a screen doesn't mean you should be using it for some, for some things. And then also what kind of explicit, implicit confirmation do you want to do? So when I talk about explicit. What I mean is, you know, I think you want to set the reminder. Is that right? That's an explicit confirmation. You should state what you think the user wants and then they take another turn to explicitly confirm. And an implicit confirmation is that you don't actually ask the user, but you do repeat the action back to them so that they can actually correct you if you're getting it wrong. But either don't if you're getting it right, then then they don't have anything to do. Okay. In terms of consumption methods, there are a few ways of actually getting that free tier. The confidence, implicit nonspeech, generic and visual. We're going to go over all of these. Now we talk about free tiered confidence. Basically, it means that you need to have you take into account some sort of confidence of the system in what you understood the user to want. And then basically you kind of calibrate your confirmation based on that. So if you have a 99% confidence that they want to, I don't know, play a song on Spotify, then, you know, just play the song. But then if it's more like 40%, then maybe you want to ask them to confirm the example here. Please buy more paper towels. If it's more than 80%, you go for implicit confirmation and then they still have a chance to correct it. If it's a bit in the middle than explicit, explicit confirmation. And if it's less than 45%, then you just re prompting. Basically ask them to say it again. Implicit confirmation, you just basically give the answer as we just saw it just now. That could be something. For instance, be someone ask what's the weather tomorrow? Then you can say the weather in Nottingham tomorrow is just to make sure that you understand what the answer is, so that if they actually wonder whether somewhere else they know that they can fix that query by asking a second time and being more precise nonspeech confirmation. Like for a query, like turn on the lights. Well, you know, the response could just be to turn them on. And that's usually more than enough. It's a very low stake interaction. You don't need to go that that deep and then a confirmation. So that could be something like, are you fitting today? Pretty good. And then thank you for sharing that with me. So here. Right. You know, actually showing any understanding of what was actually said by the user, You just need the generic confirmation. Like the name says, you just go very generic and then move on. Basically, if if you have read the paper, which I released, I think in the second week on Eliza, this is pretty much what he was doing all the time to fake interaction. This is just to stay very generic so that there's no way you can get it wrong. And finally, visual confirmation. So that's the one is a bit tricky, right? Because it only works if you have a visual medium. So it works fine on a phone, for instance, because you have the screen assuming the user can actually use it. So if you design on the smartwatch, for instance, you know the screen real estate on your smartwatch, it's not that it's not enough to actually display much. And it's very good if you're going for more of a hybrid experience. Okay. Any questions? I realise I'm bracing for this because I'm a bit worried I won't be able to finish in time. So if you do have questions, just raise your hands and I'll try to answer as good as I can. Otherwise. Just close your eyes. Yes.

SPEAKER 1
I don't think there's anything else. Like an.

SPEAKER 0
Explicit greeting. Yeah. It's basically varying degrees of explicit. You don't have to find a kind of trade off of, like, bothering the user in different ways, visually or in audio. And then you have to choose what fits best with your experience you're kind of designing for. But yeah, yeah, you're right. Like implicit kind of wraps what is like a thing in itself. And then you have different flavours of explicit. Any other questions? Okay. Right. Air handling by far my favourite part because I always like to think that, you know, the value of a system is in how graceful you handle when it's going wrong. And so her handling is a very important part of anything that interacts with humans, because that's that's what makes an experience either frustrating or not frustrating. I've talked about prompting before, which I think is the simplest way of handling errors. So something like, Oh, I'm sorry, I didn't understand. Can you repeat? But the thing is, they're not always necessary and they can be very annoying. I say this as someone whose accent is not very well understood by my readers at home. Repeating yourself five times to get the weather is not nice at all. You can use other techniques like silence. So for instance, you know, if you just don't reply, then at some point it's a bit uncomfortable. But at some point the user understands that they need to repeat something. But yeah, it's it's the point of silence is that it doesn't actually tell you what you need to fix. Right. A quick thinking exercise. 2 minutes maybe you can share with your neighbour. Can you tell me how can a voice user interface go wrong and you can talk? They don't have any neighbours. No, you don't have neighbours. Yeah.

SPEAKER 1
And the reason?

UNKNOWN
They know. I think it.

SPEAKER 1
Will help in this situation. I don't feel like I of. You know, just as a woman, she's. This. You wouldn't be able to go back to work.

SPEAKER
I have to say that I.

SPEAKER 1
I. I have. So that's that's a. Oh.

UNKNOWN
I think that's the biggest.

SPEAKER 1
I think this has.

UNKNOWN
To do with.

SPEAKER 1
This is one of the. I feel. But that's doable.

SPEAKER 0
Okay. Anybody who wants to propose a way where they think voice user interfaces could Google wrong? Yes.

SPEAKER 1
How long is it when it's like, super noisy? Like there's a lot of difference? Um, I think so. It might, but I think it's more like you might like 24 hours. And also, someone else said this before you even talk to me. I'm going to I'm going to respond about this.

SPEAKER 0
Right. So basically wrong input. Is that what you're trying to say?

SPEAKER 1
Like, I don't find the.

SPEAKER 0
Like, noise in the input. Oh, okay. Okay. That's an interesting one. Yes. Behind.

UNKNOWN
Now my second confirmation.

SPEAKER 1
Because. It closes down. Yeah. Thank.

SPEAKER 0
Yeah.

SPEAKER 1
Something else you also have in mind?

SPEAKER 0
Yeah. Yeah. So it's a very confident about something he got wrong. Okay. This is a very interesting one. Any any other. No. Yes. Yes. Sorry.

SPEAKER 1
You have to see this. You know anything else? You. So the city was not, in my view, analysing it, which was. The. And also, I think in some ways it probably buys confidence in the Congress to look at this. Based on the. Device will be working on detecting voice.

SPEAKER 0
Okay, So. So the first one, the thing is kind of the same that was just set now was indic that there's something like accent, but also you could just be sick, right? If you're sick, you know, the stock is stuck and then it doesn't understand the thing. He understands it differently. And the second one is a hardware issue because I was always trying to say, Yeah, I like the microphone. Okay. And yes.

SPEAKER 1
So the answer is I might go up. But like they're serious problems. Just like it's so when it's like thing, it's a bit like saying an alarm. Yeah, I hear you.

SPEAKER 0
Oh, it's on sounds answer annoying actually. Yeah, that's right. Yeah, I think it is. Does that count as a harder I think I guess that could kind of is like almost like a hardware issue. They're supposed to remove their own kind of sounds but it doesn't always work. So I think a lot of the ones I actually yeah, I think all but one were set right. So the first one, no speech detected. So I think that's your hardware. Our microphone is off basically. So that's pre ACR, so pre speech recognition. So no speech equal, no response, not good speech detected, but nothing is recognised. So that's a transcription failure. So that could be because you have an unfashionable accent that was not used in the training data and therefore it is known to stand the response at all. Or it could be, you know, something else or something was nice, but it was interpreted wrong. So that's a transcription error. So that's the 2 minutes, 10 minutes thing. So here you have an unintended response and it may be avoidable by confirmation. But of course, as we all know, that only works if the confidence is not too high and it's a mistake. And finally, something was recognised correctly, but the system does the wrong thing with it. So this is more of a semantic issue as in that you basically I went wrong. So you you're right. And as to what you mean, it just does not do what you might have thought it should be doing. That's all very good. So let's see, basically how do we actually fix those things? So the first one, no speech detected. So pre ASR here, no response. Anybody has had a guess how you would kind of deal with that. Sorry. Yeah. Turn on the microphone. Yeah, I guess it depends. Why? Why is not addicting, Right. So I think here what you can do, anything you can really do is show something when everything is working correctly. So that's like, that's the that's the goal of a status light on all sort of machines. You have a light which is green when it's working. All of this thing is red and he suddenly turns green. I know something's wrong and then it's going to be some trouble. But yeah, so then the only thing you can tell is everything is doing correctly. And then this way, if something goes wrong, then that signal is not there. So if you don't have the light, that means Alexa is not listening, which means that it won't be able to detect anything speech detected, but nothing got recognised. How would you deal with that? Yes, the user asked the user to repeat. Yeah. And he so you know, there is some sound, but nothing was actually recognised. Okay. Well this there's many ways actually of of doing that. The first one, the simplest one I think is you don't do nothing very poor, but sometimes it works. You can call it out explicitly as you said, like you just ask for is like a big prompting. Just say, Oh, I didn't understand that. Can you repeat the name of the restaurant or what? What did you mean by this? Or something like that? You need to be explicit in what you're asking of the user. Yeah. Need to be precise because it helps the user understand where he went wrong. Um, or you can also indicate that you system as process the query but not actually return a response. So that would be when Alexa's lights are spinning, but then nothing comes out of it. So as a user, you know that your input go, then you just know that nothing got out of it. That's also a very common technique in traditional software engineering. When you when you build graphical user interfaces. Right. So something was recognised incorrectly. Transcription Error. Unexpected or unintended response. How would you handle this? Yes. Okay. I'll start with clarification. Question. Any other.

SPEAKER 1
He said.

SPEAKER 0
Uh huh. And if she. All right? Yeah.

UNKNOWN
Like my mom, she goes like.

SPEAKER 0
Yeah. Okay. Yeah, I guess that's. I think that's a more. So this is I, this know what I was thinking about, but I think this is more like a general design thing. You always have the user be in control of when the conversation stops. Also very useful when you asked, you know, Alexa to read the news and you just never stopped. You just starts reading you the entire first page of the BBC. So here I think there's there's multiple ways of actually doing that. I think the simplest one usually is to present a partial response based on your interpretation and then ask for confirmation of what's just said now. So, for example, you want me to police station for eventual rights as a confirmation of what Alexa actually understood. Um, it's good to be able to build the potential transcription error in a response so that the user can move on if it's if it is correct or can actually fix it. And then finally, something was recognised correctly, but the system does the wrong thing with it. So semantic error, how any idea of how you would handle semantic errors? So what do you say? I know, but you say, right. Good code. That will fix everything.

SPEAKER 1
Like.

SPEAKER 0
Yeah. Just be better.

UNKNOWN
Really, I. Site.

SPEAKER 1
Yeah.

SPEAKER 0
So I have a way to correct basically like a course correction thing. Yeah, I think this is like, yeah, this is so yeah, you're right. Like having confirmations, having way to course correct are like traditional ways of actually dealing with this. So for instance, I mean confirmation or if there is some ambiguity, basically make sure that you you know you clarify that. So if I say like what's the weather in Springfield and obviously the US, you know, said his name is used like in ten times so in the clarified AG 1 to 1 or Maryland and then give you the actual answer. Well yeah of course correcting once again that's more like a general design thing If you do it, if you do a job well, you never go into the kind of ambiguities. Um, I've talked about the general idea I've been prompting, which is basically like the main way of actually handling errors, right? So you want someone to repeat themselves in a way that kind of avoids the mistake altogether. One thing to do when we're printing is to avoid generic error messages like, you know, please repeat yourself because that's annoying on the side of the user and then it may entice them to just not trust your application. What you need to do, however, is to instead of green generic is to tell exactly to the user what is expected of them. So for instance, here a good example is that if you are asking the user for a flight number and they give you a wrong number, right, you can tell them what is that? What is what it actually looks like. Like the phone number is three digit long and follow that if there is you eight and then in that case, makes it a lot easier for the user to identify where they went wrong. They give you a wrong number and then how to correct it If you just told the user, Sorry, I don't recognise that. Therefore we just repeat that like until they get tired of it and then use a computer. General points about error handling. It's very tempting, but she didn't blame the user. It's not their fault. People don't like to be there. Sometimes it is their fault. But still, they still don't like to be blamed. You need to be able to kind of accommodate different kinds of user. Novices, experts. If you know, you ask yourself whether someone from your parents generation will be able to use it, and then also whether someone from the next generation people to use it, you need to be able to kind of, you know, have a have an experience, which is good enough that multiple types of user can can use it. And but what I mean by that, of course, is that for a new user or someone was not, you know, experienced with this kind of applications, then they might need more instructions, more helps. The discoverability is very important because if someone who has never interacted with a chat bot or if all his interface, if you just put them in front of Alexa, they'll just be very confused because how do you interact with the microphone? On the other hand, experienced users like, you know, all of us of course may wish to use shortcuts and that you need to be able to build that in your application so that they can do what they need to do faster. What we call universals are is an important concept is basically global commands, which. Sorry.

SPEAKER 1
So would it be better to just have one story at the very beginning?

SPEAKER 0
Well, have you have you had to go through a tutorial for.

SPEAKER 1
Yeah. Like if for example, if you have like an elderly couple and you're trying to figure out how to use it, but you don't know how to use it, so they go through somewhere else and be like, oh.

SPEAKER 0
It's I mean, this I think this is part of the discoverability thing, right? As in, like me say, you know, Alexa, what can I do then? It's kind of like a tutorial in the way that you guys do towards a type of interaction you can have. I having a tutorial at the beginning, I think it's fine. It's just that you might, you know, people don't just come up and then, you know, know everything and memorise everything as they go right. If you put me back in front of my Google assistant that I haven't used in five years, I probably forgot a lot of stuff about it. So yes, a tutorial can help, but it's a very strict and naive version of having good discoverability in your voice user interface, which is more important basically because if you have good describing it the image, you're going to ask for help at any step instead of going for a guide, basically.

SPEAKER 1
Yeah.

SPEAKER 0
So it's on the right track, but you make it more interactive. Visually is better. Don't take it. Okay. There is a concept called universals, which I think was Yes, sorry.

UNKNOWN
And so I.

SPEAKER 1
This post.

UNKNOWN
Was. It's pretty obvious. You know.

SPEAKER 1
On. That you want to see us once again.

SPEAKER 0
You mean rude words Like.

SPEAKER 1
What you.

SPEAKER 0
Discriminative? Yes. In the social or the statistical sense. Sorry. Oh, okay.

SPEAKER 1
I'll come with you. If we remove this system in sentences. Yeah. As. The system to ethics violations.

SPEAKER 0
Yeah, this is a very interesting thing, and I wish Joel was here because there is a lot more work on this than I have. But basically there is kind of a role, and it's even more important that a lot of the comments that you give actually don't stop there. Right. There are some of them get redirected to some humans which are actually helping improve the A.I.. So they might actually be the victims of of of this kind of things. Yeah, you're right. I mean, some there is a lot of work on like detecting offensive language in this and then kind of prompting the user as in like, can you say in a more polite way or something like that. But it's a problem we haven't solved yet. And you do try insulting your system. Don't do that. This we called it. Yeah, I think. Can you can you ask that again to Joel next on Monday? Because I think he's going to have a lot more to say about this than than I have. But it's a very interesting question. Yeah. Okay. I think. Yeah. So I have two slides left and we're running towards the end, which is good. And I can take question after that. The concept of universals is important because basically, as we said, that you need to be able to interrupt processes with your voice, which is not always, always easy. So you need to have something like help stop or continue or pause, which helps you manipulate the interface without having to touch any buttons. Right. And it's yeah, it is very difficult to invoices interfaces because there's no physical affordances in the way you kind of use your stats. You need to be able to actually tell that. So once again, going back to discoverability, you need to be able to tell the user what they can do. And this is also part of like universals as in like at any point in the interface, you need to be able to ask, you know, what can I do? And then the interface need to be able to tell you you have a choice of doing this, this or that, or like something more general. Yeah. Okay. And some final things to think about prompts. It may be useful to write a list of recurring things a system might say because that helps you kind of, you know, understand how how your system interacts with the users, you know, key phrases, accessibility to that and if you actually. Yeah. So this is just a repeat of all the stuff I've said so far. So not that important. And. Yeah. Oh, that's more interesting. Talking faster. When you design a voice user interface. We talked about accents. We talked about different level of expertise, different speeds of speech is also something that is very important to consider. You know, you need to both be able to accommodate people who talk differently. Being able to interrupt the smart speaker or any voice interface at any time is also very important, which is why they're always listening for the wait word, even if they're still in the middle of speaking. Either be able to provide actionable prompts. You know, users need to be able to know what they can do, of course, and you need to be able to tell the user basically, you know, what process there are in the middle of doing. So if you're using a graphical interface, right, it's very easy to know what you're currently doing because it's displayed. But the voice interface is a bit different. There's no feedback except when you actually enter something.

SPEAKER 1
Yeah.

SPEAKER 0
And yeah, so text to speech, personalisation. Being able to configure aspects of the voices interface like the voice or dialect is using is also quite important if you want it to be usable by people. And yeah, I think that's going to get a bit tricky in multicultural countries. And that's it for now. Once again, and just stop the recording. So.
