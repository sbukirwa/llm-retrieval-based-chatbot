SPEAKER 0
Oh, yes. So you'll notice that this week's lecture is called Advanced Topics. That's a neat trick you learn when you start teaching, is that if you name something advanced topic, you can change what's inside. You don't have to tell anybody. So that's the reason why. And we're going to touch on two topics today. We're going to touch on the anatomy of a chat bot, which I think you'll find very relevant right now if you're working on your coursework. And then we're going to touch on something called evaluation. How do you know when things aren't working? How do you know when things are not working? So let's start with the first part and let's chat a bit about chatbots. So first, as a as a warm up, I like to just like a quick hand raised who here has never seen or interacted with a chatbot, whether on the phone, browsing the web or anything, who has never interacted with a child born and raised. So you've you've all done that good means you fall into my trap, because I'm going to ask all those who have interacted with a chat bot, what did you use it for? What kind of task was it? What could he do? Who wants to volunteer was brave. Yes.

SPEAKER 1
The last time I tried to back the participation of like, Oh, like, what do you what are you calling for community here? That's right. And then eventually they realised. I realised that they called me and asked me to teach.

SPEAKER 0
Yeah. So just to repeat that, like you wanted to book an appointment at a bank and then the bank rather than deal with you as a human being, we directed you towards a intelligent machine with took your information and then in the end still pointed it towards a a human agent, which is most of the use cases of chatbots in the industry. Right. Because you've seen the right path. But it could have been that if your thing could have been answered by the bot, then they could have avoided having to interact with the client. And this way they can save, you know, time, saved money and everything and so on. So this is like most of the uses of chatbots. Anybody else as he's what, you. So can you can you feel like there is a lot of noise? Okay.

SPEAKER 2
Yeah.

SPEAKER 0
And there was no robot in the branch, Right. There was an actual, like an assistant. Yeah. So once again, banking. So it's banking isn't, it's always trying to save a buck. So yeah, banking is a lot If you ever booked a restaurant table, sometimes every direct route towards a chat bot airlines, you love it as well. But basically like they want to manage customers. So they had a little bot that can avoid or at least reduce the workload a little bit. So that's good. You've all at least been acquainted with with what it does. So let's look at the different parts of a very simple, a very basic chat bot. So functionally a chatbot is just a sort of agent which waits for a user input, right? Whether it's text, whether it's voice, whatever, then does something with it and then responds in a way that is hopefully useful to the user. And so the key thing here is what do you put in the middle, right? What is the actual processing? And this is the stuff we've been doing for the past couple of weeks. That's the natural language processing is how to actually take that input and do something interesting with it so that you can then output something useful. So some of the tasks that usually have to do. So I talk about this a bit in the course work description, but intent routeing which is or intent detection, which is basically guessing what the user wants to do. So if you both can do multiple things, then intent is very important, right? If your like bank chat bot can manage changing of address, booking of appointment and so on, we need to know what it is that people want to do in order to route them to the correct subsystem. But then it's not the only thing, right? Something like sentiment, analysis and emotion classification, which I mentioned in the this classification lecture can also be very useful because if you want to know that the user is in distress or you want to know are they happy or sad in a customer service chat board, that could be very important. You want to know, well, that user is sad, so maybe we can point them towards an agent. That one is happy. Maybe we can ask them to leave a review. This kind of very like basic but still very useful Routeing then there are things that question answering. So if you've interacted with Siri, if you interacted with the Google assistant or whatever, you've had at least some extent, some experience, description, instrument. If you ask Siri what is five plus five or where is that? What is the capital of English of the UK or whatever, Like in what year is the Second World War started? That can give you a very factual answer and more. There's a lot more things you can do with a chat bot. They make ones for roleplaying games that make one for like simulations. It's a very versatile sort of kind of structure. However, most of the tasks that a child can do can be modelled as these two things specification or retrieval. Now, if you look at this, you might recognise, you know, not free and up to and this is the reason why we have you made those things is because now that you understand those tools you can put them together and make the bot that you want. So let's go over a few of those things basically and explain in what context you will use them. So intent detection is maybe the most, you know, the most important one, because if you don't know what the user wants to do, then your book is not going to be very useful. So as I mentioned before, simply asking question can be an intent making. So small talk can be an intent calculation, can be any intent. And all those things like you're talking about, you can add as many as you want. You're going to have to free ten, 100. It really much depends on how much time you want to spend on it. Intended action is very easy to model as a similarity matching kind of kind of thing. So what you would do right is that you could keep a list of example inputs like this ones and then the course when the intent make small talk, ask asking a question and so on, then do a basic similarity search, see what the user input looks like the most, and then just use the intent of that as your prediction. So that's one way of dealing with this. You could also deal with it as a classification problem. So you would train a classifier like a decision tree or nearest neighbour or whatever with that kind of data where the input would be phrases, sentences, utterances, I think we call it in the literature, and then the output would be the actual intent that was meant you train your classifier on that and then you use that as a way of routeing the user towards the correct subsystem. Question in Syria quite similarly, and I think this has been mentioned to more details in one of the labs can be modelled as a similarity matching problem. So for instance, if you keep a list of question and their answers, so like, like tuples of like question and so question and certain answer, then if you have a new input and you know it is a question that the user is asking, then it's fairly straightforward, at least conceptually, to look for the question which is the most similar to that new question, and then use that answer as the answer to output for the user. So this is like the very simple kind of basic algorithm for this, and I even put it in that very simple pseudo code for you here you can see it is very straightforward to kind of model that as a similarity matching and then just basically try to answer it as best as you can. It is important to know that no system is going to be 100% accurate and yours isn't expected to be either. For starters, you don't have nearly enough time or data or computational power or knowledge to make it otherwise inaccurate. So don't freak out if it turns out that your question answering system only gets like very basic questions, right? Another one that I mentioned cinema, and that is this or emotional classification or all those kind of methods which allow you to understand the user. So basically trying to make predictions about the user itself can allow you to adapt your system in a way that is intelligent. So, for example, here. So this is another very simple pseudocode thing. You could ask the user how was your day? And then classify the sentiment of what they are saying and if it is positive, you could have a specific kind of output like, Oh, would you like to record this in your journal? Or if it is negative, you can have another, another kind of output, which is, you know, would you like me to tell you a joke and then basically kind of grandchild from there? Now another quick activity, but we use it very important for the sake of the coursework. If I was asking you to classify the intent of a user and it turns out that all intents are equally unlikely like that, the predictions are basically, you know, 51% chance this, 49% chance that or lack of community uniform distribution. What would you do and sit here and discuss it amongst yourself And this we'll see after that. What do you do?

SPEAKER 2
How. Yeah, but you kids are going to think it makes me sound more smart. He was only briefly speaking in English. I think so, too. I've got some. Video from. As a final solution. I want to take them seriously. Do you mind if I sit here?

SPEAKER 0
Okay. Okay. Okay. Anybody wants to.

SPEAKER 2
Meet.

SPEAKER 0
Anybody brave enough to volunteer some answers. Our. You know, you have. Yes. Hendra is over there. Can you.

SPEAKER 2
We can do the most. I would think you right. Right.

SPEAKER 0
Okay. That's very good.

SPEAKER 2
You sent me this to be brief.

SPEAKER 0
Just ask. Well, at least some answers for the rest of the people. Yeah, you're right. You're right. So asking the user to reformulate. Proposing them something like alternative interpretations. Okay. Anything else? Yes. More details. Yes. But what if you're not sure what they mean? Like what kind of details you ask me talking about? Right. Okay. Yeah. I know you already, my son. You're blaming the user?

SPEAKER 1
I think it's more like. Oh, you're something crazy. Correct? Do you mind?

SPEAKER 0
Oh, okay. Okay, That's good. Ask more information. Yes.

SPEAKER 1
There's. There's a couple of channels I would like have on to respond to. Be like. Oh, I didn't understand what you meant. Can you elaborate? Just as a reminder? I, I was like, if you talk about these things, I can respond.

SPEAKER 0
Yes. So propose a list of potential things, kind of redirecting the user towards a menu, right? Yeah, basically, That's very good. And what does any other. Yes. Sorry. Then you. So using the context of the conversation to to improve, the guess is that what you're saying? Okay, that's very good. I didn't even think of that one. But you're right. Yep. Sorry. One last one to give.

SPEAKER 2
A general statement. Just give a general statement and just try to move on.

SPEAKER 0
Okay. This is great. We actually covered all the potential answers. Mostly. So there are others. So you're right. What you're saying, like using context to improve, the guess is a way of doing it, but it's more advanced than what I had in mind. These are general conversation designed strategies. I'm not going to go into much details on that because Joe is going to talk about it in a specific lecture dedicated to that. But I'll give you some clue about the main ways of dealing with something like this. So the first one, and that was the first one, so I got very good is refunding admit that you don't understand, have to reformulate. And I think that contains also asking for more details or like any sort of way, you can kind of nudge the user towards repeating but differently, like say it in another way. The second one disambiguation, I do understand, propose a set of alternatives. So that covers many things, right? They can cover either the menu approach as in like, I don't understand. These five things are the things I can do, but you can also cover. I don't understand that. Do you mean A or B? So there are two things. There's two ways of dealing with that. I don't understand. Do you want to book a flight or do you want to cancel the flight? Or basically, depending on what your book is doing, you can kind of move that. And then the last one is escalation. I mean, defeat. So in a business kind of book, that would be where you redirect someone to an agent because turns out the borders are not to book an appointment or or any of the things. Right. So customer service boards are like very notorious for doing that because the goal of a customer service board is not to automate everything is to reduce the workload on the human agent. So if there's something where the borders are no way to answer, and one of these things I've worked, then just, you know, bring it to a close, I'm afraid I can't do that. And then that's it. Move on, basically. So that's very good, right? So managing uncertainty and having some fallbacks in the conversation is how you have a board move from being annoying because it's wrong all the time to being wrong most of the time, but at least it knows it's wrong. So how do you do that? Well, actually, it's simpler than my sound, right? Because if you've done some of the labs, especially that pre you've played with classification, you are aware that those are those classifiers and even like similarity measures, they always output numerical scores that tell you basically how confident they are in something. So for instance, a roller coaster fire and there should is most classifiers. There's only a few that can do that can output a probability distribution over the classes. So instead of telling you it's intent A, it tells you, well, this is the probability distribution of order intense. And then if that distribution is a bit too close to being uniform for your taste, which means that there's no difference, basically, then maybe your confidence is very low and maybe you should ask the user to be prompt or basically going to do something else. Retrieval methods. So if something based on similarity, use a similarity score and similarities course as we know are bounded between one which is perfect match and zero, which is, you know, that match. So the closer you are to zero, the less confident you are in in what you're, you know, what you're understanding. So then once you have those two things, you can basically, you know, fix a threshold and see if it's more if it's less than zero one similarity, maybe, you know, maybe I don't know what I mean. Maybe I shouldn't be outputting random answers. Um, yeah, and there's something also called the end strike rule, which is very common in books design, which is that if you get the same thing wrong twice in a row or end times in a row, then you just escalate the conversation, start over from scratch. So there's another very smart processing you can do, which is kind of having like a confidence level and then knowing how many times you want to get it wrong before you move on. You learn more about all of this in Joel's lecture. So I'm not going to go into details, but this is the basics and this is more than enough to make a chat bot, a textual chat bot. Quite, quite smart. Oh, beautiful timing. Okay. Final bit of the part about butt anatomy is the actual text tagging functions. I didn't go into lot of details into this in the first labs, but I did mention two things. The first one being part of speech tagging. And if you remember the bugs in the lab in this bit. So you might have remembered it by the speech tagging basically to tell you what is the grammatical role of each word in a sentence. So if I say my name is Gerry, it will give me this tags which corresponds to pronoun, noun, verb, and now. So if you combine this with some clever processing of the sentence and say, for instance, if I detect my name is followed by a noun, then that's the name of the user, then you can use that to do some clever processing and then memorise what is the name of my user and then use that in your dialogue. Similarly named entity recognition is a bit more in-depth and what it does basically is as the name indicates, recognise named entities. So what name entities, those are name into this. I don't think you can read it from there, but basically types can be organisations, person, location, date time, money, person, facility and so on. And what you can do is you can use that to basically have clever questions in your bot and then use that on the out the input from the user to understand what they mean. So if I wrote both for instance, and the bot says, Where are you from? And then I detect a location in the answer, then very likely that's where the user is from. Once again, this is not perfect. I'm referring to to the chapters in the textbook to know how to use them. It's very straightforward, as you'll see, but you can do a lot of very powerful things with it, right? Hmm. So that's all for me. I'm going to move on to the next part about experiment, design and evaluation. First off, let's start. And once again, criminals talk to each other and so on. Why do you think we need to evaluate our systems? This is not a trick question. I will select a couple of minutes and talk. Communicate amongst yourselves. I.

SPEAKER 2
It was really. Yes. Do you think? Also. This. Right now. We'll do for. Although most okay. I know you've. I think this is going to work. I didn't shoot this, although I do have to. Get it done is done. Yeah, right. Yeah. I think I kind of agree with him. No. I mean, I'm talking to you and I'm using, like, what? I'm going to say this for the evidence. Well, I don't think it's going to be over, you know, afterwards. So, you know, I wasn't comfortable with this in terms of research, but it's not. Yeah. It's probably the of this game. The whole design is now in your hands. Oh.

SPEAKER 0
All right. And anybody wants to hazard some answers in this territory? Yes, please. Sorry, I didn't hear you. To improve the system. Yes, that is correct. Anything else? Anybody else? Is that a trick question? Yes.

SPEAKER 1
But there was a case in the United States where they sold like there was this one protection item.

SPEAKER 0
Yeah.

SPEAKER 1
They were actually physically using it to like ten blocks in different areas in New York City and Chicago. But one of them broke it open at one point. This was offered him and then you realised that it was just like, is this really a piece of plastic with pants on? And I think like they did in advance electronically just on their end before it came up, like millions of dollars for this this system that is basically just a scam.

SPEAKER 0
Right? So quality assurance. Yeah, that's I mean, that's very fair. Yes. Quality. And I think that actually covers all of the things I want to talk about. Right. If you want to improve something, you need to know or evaluate something you need to actually like do a proper evaluation. Just because it looks how it works does not mean it's working correctly. And I think at this stage in the third year or masters of Computer Science, that's something that should be relatively obvious, right? So we need two things we want to measure. One, to evaluate something. First off, we need to focus on performance metrics, like what is it that we are measuring? Second thing is how do we measure this? We need to have some sort of experiment design. So this lecture is not exhaustive, so don't think that this is all the ways of doing things and just giving some examples, which I think make sense in this context. But the fact is the actual evaluation of systems is a research topic in itself. It's very hard. You'll find two types of evaluation. So when we talk about system evaluation, I'm sorry is really means that we are testing the components in isolation. And that's very important, right? Me to know that all of those things work separately. Then we also have user centred evaluation, in which case we are putting people in front of the ball, in front of the system and see how they interact. And this is a very nice thing to keep in mind. The difference between screwing around and science is writing it down, and we try to go more towards the science parts of it, which is why we need to fix a very strict experiment protocol and follow it in order to evaluate our system. First, let's start with the simplest one classifiers. I've mentioned a bit of this in the labs, right? We want to evaluate the classifier. Usually what we care about is how good it is at classifying. And yes, the standard be obvious because it is right. We call that accuracy. And basically accuracy is just the number of correct predictions divided by the number of all the predictions that it has made. So if you use it, we tested on 100 inputs and then it gets half of it wrong. Then you get a 50% accuracy. Of course, this is not a perfect score because what happens if, for instance, you have a sets with two classes and the first class has 99 documents and a second class as one document, If your classifier just output, you know, class one all the time, you will get a 99% accuracy and yet be the stupidest classifier to ever be designed. So we invented order for two types of score and one of them being the F one score. You can see the formula here 24 divided by two positive plus half of false positive plus false negatives, which comes from this thing here, the confusion matrix. And then basically what it does is that if you prefer it, if you have like one class, which is overly overwhelming the others, then they tends to be penalised rather than then rewarded as it would be in with accuracy. So those are the main ones that are low. Over there you you'll see stuff like rockers, Precision Recall. There's many, many scores. And as I said, it is an actual research topic. But those like three things get you 90% of the way most of the time. So the way we actually do the evaluation, the experiment design this something you would see called the train test adding and that's the main kind of experiment design in classification. The idea being that we can estimate generalisation error of the classifier by simply holding out a part of the data. Then training our classifier on the thing that we have, the training set and then testing it on that bit. We held out in the beginning. So for instance, if we have a thousand documents, then we can split it in 6020 20. We should call train the validation test. So the first 1620 we use to kind of fine tune our algorithm, change some like parameters and so on until we are happy with it. And then we tested on the final 20, which is our test set for final evaluation. So this is like the general form of that. Of course, this is not always the best, which is why we invented stuff we call cross-validation, which is basically a train test with with rotation. So I'll put in an example you'll see is actually quite straightforward as well. If, for instance, we have a thousand documents and then we use 8020 train to split, what we do is that we, we remove those 20 and put them aside for testing later on. Then we take the hatred that we have. Our training said we divide it into four different partitions, which means that there is no overlap in the in those sets. And we use first we use partition one for validation and then partition to three and four for training, which gives us one performance metric here. Then we do the same thing with partition 21347 performance, then so on and so forth. Until we have tested on each of those partitions which we call folds. And then once we have this, what it means that from one training set, one dataset, we have extracted four numbers instead of one, and we can do that to just average the performance, which gives us a way more stable estimate of that performance metric than if we just had it on a single dataset. And also, I'm not going to go into the details, but the fact that we have multiple measurements means that we can do some pretty fancy hypothesis testing in scientific papers. Okay, so then we have something also system evaluation for question answering systems. So question in setting systems, I mentioned that in the labs basically retrieve a list of potential answers, rank them by decreasing likelihood of being the correct answer. And then returns the top one. So that's like reachable based question in certain systems. Now, there's two things we can measure here. Right. First one is, is the top answer, the one that was returned? Is that the correct answer? And the second thing we can measure is how far is the right answer from the top and. Huh. Now, I don't think we need to spend a lot of time on that particular bit. So I just wanna ask, what do you think is the best measurement about between those two possible possibilities? So take a minute to think. You can chat with your friends if you want to, to confirm your thing. And I'm going to ask just a hand-raised poll after that. Talk about yourself.

SPEAKER 2
Yeah, we used to way. Yeah, well. Was that because. You. Well, when she's finally going to be here. Well, if you listen to that. Listen, I mean, what do that's difficult to do? There's a real. Fixing something. Thinking about was. Oh. Mr.. Think Nigeria and not. You know, reading it. I mean.

SPEAKER 0
That's all right. Kind of race poor Who thinks A is correct? Raise your hands. Come on, Be brave. There's no penalty. Okay. Who thinks B is the right answer? Ooh, Overwhelming ish, Right? The truth is, there's no right answer. So actually, overall.

SPEAKER 2
Yes.

SPEAKER 0
So A's, fine. A's, a completely fine thing to measure if what you're trying to measure basically is how good your system is at providing the right answer. B, it's going to be a bit more fine grained until allows you to see incremental improvements a lot better. Right? Because if you make a little bit of change and then your answer goes from being the 10th in the list to being the first in the list, you'll still wrong is still wrong, but it's not as wrong. And and I think that's good, right? So I would say B's probably a bit more useful than eight, but ethnically, both are going to be fine things and you'll find both in the literature if you happen to get lost there. Right. So I'll go over this a bit. So we to know basically what we're trying to measure. So if you try to measure whether it's actually outputting the right answer, then basically we just end up in the same case as classification, right? This is just accuracy all over again is instead of coefficient is correct answers divided by all the answers that were provided. So the formula doesn't change. If we want to know how far the right answer is from the top, it gets a bit trickier. Now don't get scared. This is actually very simple to understand. The normalised discounted community gains basically measures to measure how well-ordered your your ranking is. So this is how it works. First off, the discounted community of gain by itself at the is just the relevance of the first item of your ranking. So that can be zero or one. If it one then is at the top and you can stop there. If it's not one, then it's a zero. And then you have to go into the next bit. And then what it does is that it sums in your ranking from two to P the relevance at that particular point in a ranking and then divided by a logarithm of the position plus one. So basically the closer you are from the top, the higher your score, the lower you are in the top, then it doesn't really matter anyway, right? There's not there's not much difference between being in the 14th place and the 15th place. But there's a big difference between being in the first, second place and the third place. And then what you do. So that's the the discounted cumulative gain. And what we do is we divide the discounted cumulative gain by the ideal this the ideal discounted cumulative gain, which is the discounted cumulative gain where the top position is the correct one. So in the case of question answering, if there's only one single right answer, then that bit we can just forget about it because it's always going to be one, right? Otherwise we we can have panic. If you have multiple correct answers or some answers are more correct than others and you can have more gradation in your relevance, You can have a relevance of one for the best answer is 0.5 for an answer, which is, you know, technically on topic, but not the one that's asked, and so on and so forth. So yeah, normalises Continuum into Jane's very used in fields like search engines and things like that but also use in question answer at one time. Okay, good. I'll speed up to the end once. So that whole system kind of focussed evaluation, but we also have user centred evaluation. I've talked about analogy systems before. We know that this system relies on two things, which is, is the output understandable? Is that correct language? And then does it actually achieve the goal that we want it to? Is it communicating the correct thing? So communicative goal and is it correct, readable and fluent? When we want to evaluate this, we tend to go for user studies, right? We want to we want to look at the outputs and grade based on on those criteria. All we can do is that we can just have multiple analogy systems and compare them and ask the users which one is the best that are the two main kind of structure in user experiments, what analogy systems. There technically are automated methods to evaluate energy systems, but they're not usually ideal. They tend to oversimplify the problem. There's an entire river for you if you want to read more about it. However, is it's a hot topic because that's something we would like to do, right? Nobody wants to have to recruit a thousand people to evaluate the system. That's just very annoying. And so there's a lot of diversity in how we evaluate things. It's once again, evaluating the LG system is a field in itself. Okay, so now how do we actually evaluate chatbots? So we've seen how we can evaluate diverse order and IP systems. And the reason why I'm talking about those first is because the evaluation of chatbots is actually reliant on a lot of those techniques. There's two things you can do when you are invited, know actually just free things you can do when you are evaluating chatbots. You can either evaluate the components separately. So what that means is that you just kind of break down something to this component, right? If you have an internal routeing system, then that's a classification problem and you can evaluate it as a classifier. If you have a question answering system, then that's just a retrieval problem. And then you just evaluated with some people in this league and some other kind of metrics so you can break it down like this and then figure out a way to to kind of evaluate it in a clever way. Something else you can do is you need testing kind of the dialogue itself. So basically what you do is you just model your chatbot as like flowcharts and then you just like write a test scenario as in like, Oh, I want to book a flight and then run through each of the steps to book a flight and then see basically how your chat bot is. Resist to small perturbations in the input. So you just add a few mistakes in the grammar, add a few random words, and then see whether it breaks down everything. Or the chat bot can recover basically and guide the user towards the right thing. So that's more testing the overall design of the conversation. Finally, user experience testing is how do users actually interact with your bot? And that unfortunately requires having some people just, you know, round up a few friends. Your flatmates. Close the door, lock it and then give them a set of tasks to achieve. Like, I want you to sit down on this computer and book a hotel, stay, and then you can either either let them formulate their own queries if you want to test the bot behaviour, or you can give them specific instructions on what to type. If you want to test a specific path in your flowchart and then you observe them and it's very like, you know, how, how do, how many interactions do they need to actually achieve that goal? Where do they get stuck? Do they get annoyed? They try to escape. Do they understand how to interact with the system or are they asking you questions every 5 seconds? How annoyed they look, basically. And then you just barely take note of what happens and then use that as a way to improve or either evaluate or improve your chat bot in your next iteration. Conclusion just in time, Chad, what can be made? Very simple, very complex. Depending on the amount of things you want them to do and how deep you want to go in the design. I highly, highly recommend using something like a flow chart to think about what kind of chap what you want to do. Because if you just go straight into the code, you're going to have something very simple where can ask a question and get an answer and then either ask another question or leave. And thus that's a terrible kind of checkbox. You want to have something a bit more complex, right? You have 12 different states. Bots need to be able to manage uncertainty in their understanding of the input. That's very important because that's what makes a chat bot less annoying and easier to work with. There are many ways of testing a chatbot bot. You can test individual components, you can test the language generated, you can just test the dialogue itself, you can test the user experience and it's up to you. I'm not expecting you to test everything all the time because that's that's that would be ridiculous. But it's up to you to basically pick and choose the kind of test you think is more relevant of your bot. And that's part of the challenge, isn't it? And just in time we are done. So if you have any questions, you can come see me as I walk out. Otherwise, you're free to go.
