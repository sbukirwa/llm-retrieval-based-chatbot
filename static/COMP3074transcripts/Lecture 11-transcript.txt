SPEAKER 0
Welcome to lecture 11, which means we're officially beyond halfway. This is true. Time flies. Today, we're going to be talking about interpretable AI and concepts and techniques to realise it. So first part of the lecture, we'll cover some definitions and explanations of of what we understand by interpretable AI or interpretability. And also it is quite an intuitive concept. So I think many of you have an idea and can make a guess, and then we'll get more into the detail of what is involved, what kind of techniques are involved in realising interpretable A.I. and different kinds of model properties. It's still a bit loud up there. Please be quiet. Thank you. And so part one what is interpretability? So given that it's Monday morning and you're all still waking up and, you know, let's have a chat together. So, first of all, can you think of any examples of interpretable A.I. that you have encountered when you were using your devices, your social media, or any kind of digital services? So yeah, discuss with each of the 3 minutes and then share and you can include good and bad examples of, well, not, not good example, bad examples of interpretable but bad examples of something that is not very interpretable. So yeah, have a chat to each other and share with the class. 3 minutes. Go. Oh. What? I'm trying to think of something yourself.

UNKNOWN
I'm angry. I hate to like.

SPEAKER 0
I don't know many examples of that, but I'm hoping that they will cause. Yeah. And to give you will give you an explanation for his recommendation on me because he of course I the of part. Yeah yeah exactly. On the classic example for me it's usually Amazon book you know oh music recommendations just because other people for this you know being this you know. Have also because other people have also bought us thing. So I recommend the systems to try it. But it's not that I mean, it's not fully interpretable in the sense that you understand the models. Yeah, I'm not sure that it's.

UNKNOWN
Something that you understand. Yeah, know. So five or six women?

SPEAKER 0
Oh, yeah. Diagnosing Section six. Okay. But then just you give it the symptoms. I don't know yet. And. Yeah.

UNKNOWN
Yeah, but he's.

SPEAKER 0
Okay, I'm muted. All right, then. So, um. Yeah. What? What have you come what have you encountered that you think might be interpretable or not very interpretable that you might like to share? Yeah. Correct.

SPEAKER 1
But my client, what I thought it was. Was that the fuck you ask?

SPEAKER 0
And then you will understand.

SPEAKER 1
That if you want to pretend that you are the response to this, like should be something along the lines of like, what the weather is in this area today.

SPEAKER 0
Okay. And how how do you know that it's interpretable? So I think this is so. So I think what's meant most of the time by interpreting is that it should be interpretable, understandable to you why you're saying something. Why? Why is I suggesting something or not? Do you? Yeah. Yes. Yeah. Yes. Okay. That's that's that's interesting, isn't it? Because it's kind of directly observable what the move is. And therefore and also chess is it's very, very strictly rule based. So there's only within you know, there's only so many moves that are possible given the state of the chess board. So, yeah, it's kind of it's kind of a straightforward example where it's interpretable by a lot of time is of course the examples of AI is not that is it's so constrained like chess in terms of what I can can do or say. So um, maybe some other things are needed. So yeah. Any other ideas, any other thoughts, any other experiences of interpretability? Are you saying AI's not interpretable and that's why you're not saying anything? Yeah. I know. Yeah. Yeah. Yeah. So, you know. Yeah. That's just too much similarity. Yeah. I suppose you. How do you. Later. Let's go. Yes, sir. Yeah, as you can see. Yeah. So the case you didn't hear that the examples were decision tree classification. And I think, you know, that is considered an a technique that is inherently interpretable because of the nature of it. You can inspect every element of it and you can follow the reasoning through the tree and the other examples and similarity as you've all now done in the labs. And I think actually that quite a few of the examples in natural language processing are interpretable because you can look at it and you can inspect the elements, you can see what the similarity scores are for different things. And I think that's also part of the reason why we teach an LP the way we do it, so that you understand you build up for understanding, so that you're able to interpret, right as it were, what I what the chatbot is doing next and how you actually coat that into a system. Okay. But it was kind of mean, wasn't it, Because I asked you what is what, what are examples of it before I've told you what interpretability is. So, so it's just kind of also to get an idea of what intuitive understanding you might have of the concept of interpretability is often used interchangeably with a lot of these other terms we have, such as accountability. We talk about different kinds of accountability, but you probably have heard of ego accountability. Most often when people talk about, Oh, we need we need the government to be accountable by that, you know, some implication that it needs to be legally accountable, but also other terms for this are explainability. And you might have heard of explainable A.I.. I mean, I have certainly mentioned before it's kind of a hot topic and and sort of a cutting edge of something between human computer interaction and A.I.. So Explainability then we have transparency, which again sometimes is the term that's kind of applied employed for tech, particularly when it comes to talking about A.I. models. We're going to talk about that more today and then, yeah, interpret interpretability or intelligibility or trustworthiness. I mean, in some ways and it's not always the same and it sort of depends on the context in which these terms are used. What is that actually being meant by this? But I think, you know, for the for the for the concept of AI context of this of what we're talking about in these lectures, we can kind of use them interchangeably, you know, on kind of still understanding each other. So what is it all about? It starts out with this notion, I suppose, that A.I. has agency. So in other words, I can be seen as agents that are capable of acting by themselves. You know, we see this we see this, for instance, in systems that make decisions, make decisions on our behalf or or not. But when you have a system that can make decisions in some way, you could also you could also talk about the system having agency. And that's important because, of course, humans have agency and and human interaction and an understanding of what the other person is doing of the actions of another person. The reasons underlying those actions is really crucial for us to be able to understand each other as human beings. And this is important because if we don't understand the actions of another person and they are struggle and we say, Oh, what are you doing? You know, can you explain yourself? There's a breakdown of interaction, there's a mismatch that can be this is daily mundane stuff, misunderstandings. We often say, oh, you know, what did you say? Sorry, what do you mean by that? This is all part and parcel of of human interaction, is that we often misunderstand, there's often trouble in interaction and we clarify. And as part and parcel of that interaction, we take that for granted. We don't really think about that as kind of that comes natural to human interaction. And as a result, we can usually understand each other and that is called being naturally accountable to one another. We don't often need to really explain ourselves to each other. We are just naturally, the conduct we do together tends to be naturally accountable. Give you an example. This situation we're in right now is naturally accountable because, you know, I'm. I'm. I'm. Here. I'm talking to you, giving a lecture, and you're sitting there listening to me and not as totally not fully accountable because of the nature of the relationship we're in. You know, you hear students to learn to listen to lectures, and I'm here and in my role as a professor giving lectures. So this is completely naturally accountable. However, if one of you were to come to the front right now and start talking to the class, that would not be naturally accountable anymore, would it? I'd have to say, Do you want to take over my class? That's fine. But they would have to explain themselves. So this this is the this is the whole point about with with where things with I break down because we do not have that kind of of course that kind of natural accountability for actions that an algorithm or a system might take. So there needs to be there needs to be explanations on top of what the actions are, for instance. So, for example, when the algorithm denies your loan, God is not naturally accountable straight away to you is a why? Why is that happening? Why is the system, you know, saying why is the computer saying no in this in this instance? So why why something happens is and is not naturally accountable. Therefore, there's a need for explanations to be given all for other kinds of transparency so that we can understand the reasoning behind why certain decisions are being made by AI systems. You'll see this. So I think one of the classic examples, I guess in some ways I recommender systems everybody's seen on Amazon recommendations that are being shown to you, which usually says something like you were being shown these because somebody else has bought this item, has also bought these items, right? So that it's a very mundane, I suppose, example of a post hoc explanation. Why am I seeing why? Why are you Amazon, why are you showing me these things as recommendations? Sometimes they work a lot of the time. A lot of times they don't. Same for any other social media systems that present you recommendations, suggestions. Netflix for instance. When it shows you other things you should watch. I'll give you an explanation why because you watch this other thing. But these same actors also in something like that. So these are all attempts of making, in some ways algorithmic systems or air based systems accountable. And why are they doing the things they are doing? So let's have a let's look a little bit more at decision making. As an example, this paper is is on Moodle. It's called them mythos of model interpretability and Lipton here discusses supervised learning taking a classification problem right where we have a trained classification that is trained on an input set to predict an output. The example Lipton is using is that of a CT scan images and using those to predict whether whether or not this kind of stuff in the CT scan image. So typically how these problems be solved is there is a ground truth. And so so there's a set of that dataset which has been labelled by experts usually, as you know, I have here examples of cancerous images or images that show cancer, and these are labelled as the ground truth. So there's then a binary presence of presence versus absence of cancer, and then you can that system will then be able to predict whether there's cancer. And I'm seeing images based on that training data. Right. So that's a very it's a very, very classic, but also very in a way simple example for a classification problem. It's a binary classification problem as well. Now, the important thing is, and this is where computer systems are completely different to human beings. There is no reasoning in this process, right? There is. The algorithm doesn't know why a tumour might be cancerous or not. It only knows correlation, right? It doesn't have anything else. That kind of a world of knowledge behind it of why something looks abnormal. It just knows of this that these training examples are being flagged as cancerous. And these these images I'm looking at, they look similar to those training examples. That's it's only correlation. There's no reasoning. So then you can get problems with humans understanding these models in in kind of this is a simple example. But of course, models get more and more complex. And you know, clearly when these systems are used in decision making, in medicine, in criminal justice, in financial markets and so on, there is an increasing problem that if we have black box type systems that are that people really struggle to understand. And so what is being put forward is this idea of moral interpretability. So just make the most one term principle. What can we do so that this does not happen, so that it's not a black box, but it's transparent? Why is this important to do as lots of different reasons? Here, here are the ones that Lipton mentions Trust. So it's important because we need to have confidence that the model will perform well. We also want to make sure that the model can be trusted to make accurate predictions. But we also may need to know, for instance, that the model does not account for racial biases in the training data. Right. So having in if you have interpretability and you know what we discussed in a previous lecture around this idea of data, that data statements and data sets helps with interpretability of cost and causality, That's the point I was making about correlation. Correlation is not the same as causation. These things often get mixed up, and as humans we we want to see causation and things. We are sort of drawn to that, to these conclusions naturally. But it's important to note that that is not the case, especially with A.I. driven systems. And there are unobserved causes and these may not be encoded in the dataset. We have this example in the previous lecture that relates to transferability as well, where if there's a change in the environment or the context in which in this case it was machine learning was deployed, that can invalidate the model. And that was the example of the that the data is the work where they try to predict whether having a respiratory illness with pneumonia would lead to death. And it predicted that for people who had asthma, they would be less likely to die, which is nonsense, of course. And the reason for that is because the extra context, which was that doctors realised they had asthma, so they hospitalised them and care for them quicker. So they got they need it more quickly, which is then something that that accident context is then lost when it's encoded into a dataset, simply looks at the properties of a patient, right. Like for instance what their, you know, the history of, of, of diseases, other reasons why it's important in form of deafness. So instead of making decisions you you might instead of these systems being designed to make the decisions by themselves, you actually use it to provide the information for humans to make better decisions. So for instance, you could use a diagnosis model like take the the cancerous CT CT scans example. You don't then as a system, you're not developing a system that says this is cancer. You're saying this might be cancer, right? The output of the system suggests to an expert this might be cancer because it looks like these other cancerous images. I suppose a different approach to thinking about how you design a system. That's why I put HCI there, because in human computer interaction, we are thinking about making tools to help people do their jobs better instead of thinking about making a AI that will take over the jobs and do the jobs themselves. Okay, so there are different ways we can still use AI, but you are just you're using AI to design tools for people to use. You are not designing A.I. to replace critical expert decision making that humans can do. Fairness and ethics clearly is also an important part of interpretability. So it relates to earlier lectures we had on biases. So how can you be sure that predictions do not discriminate funds? For example, on the basis of right of race? We we have to ensure that certain kinds. Invention of the valuation metrics such as accuracy. We've had this you know, these models can be totally accurate, but they do not take into consideration broader, broader biases and external problems with external context, for instance, and AUC. So area under the curve, that's another popular evaluation metric for for certain age driven systems. And again, that is also similar to accuracy, does not account for issues around fairness and ethics. So demands for fairness often lead to demands for interpret into principle models interpretable models. Okay, so, um, let's talk a bit more about techniques and model properties. So how do you actually we talk for the reason that we've talked about the reasoning why interpretability is a good idea, why we need it or might need it in certain contexts, particularly where decision making is critical, like in medicine. And so let's talk a bit more about what techniques and more models exist now to to cut two types of categories of techniques and properties I want to talk to you about. Again, this is based on Lipton's paper. So Lipton talks about transparency and post hoc explanations. I'm going to talk about what that means in more detail. So transparency is directly about how does the model work, How would you make it? It's the opposite of black box, as I wrote here. So you're trying to make the box transparent so that you can see into it and you can see the the pieces of of the of the model at work and you can understand, you know, take the example of the decision tree that was mentioned earlier. And in Decision Tree you can see the parts of the tree and you can understand how each part acts to try to compute the classification. So that's the kind of goal for transparency is providing a sense of an understanding of how the mechanisms work. Then we have post-hoc explanations. That's a kind of explanation which is provided post-hoc means after the fact. So after the results has been computed, what else can the model tell me? So these are like natural language explanations, examples already given like Amazon recommendations or Netflix. You know, why? Why are you showing me this type explanations for, for example, we had a tumour classification. This would be an example for a, for a type of explanation you could give. This tumour is classified as malignant because of the model, because to the model it looks a lot like these other tumours showing, you know, examples of these other tumours. As a result, the, the advantage of post-hoc interpretability is that what, you know, typically opaque models can, can be interpreted after the fact without sacrificing, predict predictive performance. So instead of saying, okay, everything should be a decision tree because they are beautifully interpretable, decision trees don't lend themselves very well for very advanced types of problems. So sometimes the issue with is that with deep learning models, for instance, it might be that, you know, take lots and lots of factors into consideration. It is very difficult to achieve transparency. So the kind of work around is or the alternative is to provide post-hoc explanations, right? So talking about transparency, a little bit more detail. Yes. In that time. Yeah. So like I said, the beginning question was, is isn't that explainable? I'm not interpreting where it's kind of we can treat it as meaning the same kind of thing. So yes, and it is often used in the context of often people talk about this kind of layer. So different kinds of transparency to talk about here, three different kinds of transparency simulate ability, which refers to the level of the entire model, decomposed ability, which refers to the level of individual components in the model and algorithmic transparency, which refers to the level of the algorithm. So stepping through these in turn simulates ability. For example, if you really want to understand a model in full, you should be able to, as a human being, take the input that data from the model together with the parameters of the model, and then step through every calculation that the model would make to produce a prediction. Again, if you think about a decision tree, you could do that usually because decision trees tend to be not huge. But clearly when you think about. When you think about much more complex models than that, you won't be able to achieve that in a reasonable time because that's where you use computers. And so this is limited in in terms of what it can allow, in terms of interpretability to this limits to simple models like linear models, decomposed ability. This is when you would take each part of the model, you would take a you would take the input parameter and calculation. And each part of that would admit some kind of intuitive explanation. And I would say that a lot of what a lot of what we're looking at in natural language and how we build not know every aspect of how we build the chatbot. But many parts of how we built the chatbot are in fact, at that level and so interpretable. We might, you know, here is actually the decision tree example again. Each node might correspond to a plain text description. For instance, all patients would die. So I just took blood pressure of 150, got classified into that branch. And so at the level of individual components, it is interpretable. The model can look at each level and interpret all the. There are some caveats requires that the inputs themselves should be interpretable, and that disqualifies some models clearly as well. And then finally, we have this idea of algorithmic transparency, which relates to the level of the training algorithm and where we seek to prove that the training of the algorithm will converge to a unique solution, even for previously unseen datasets. And this can provide some confidence how the model will behave in such a situation where it is making online decisions on previously unseen data and modern deep learning methods do lack this kind of algorithmic transparency. And it can, of course, be quite tricky to achieve with those kinds of deep learning models. Okay, so let's do another quick, more honest discussion amongst yourself. Now that we talked about these three types of transparency, think of how it could be in your chatbot. So think about your chatbot that you're working on for your coursework and think of how that is interpretable to someone who's trying to understand how it works. With regards to those three transparency types, think about how your chat bots could be interpretable or made into a transfer to someone who was trying to understand how it works. Okay. Discuss for 3 minutes and share with the class. Oh, no. Mm hmm. I wish I did that. I am. I've seen it given the answer way as well. Mm hmm.

UNKNOWN
Yeah, Yeah, Yeah. Uh. I guess. Yes.

SPEAKER 0
Huh? Nice to meet. You could find that. I think laptops have really improved as well. I mean, PC laptops like in the last few years, I've seen trucks, you know, being able to catch up better with Macs. I used to be such a guy. Yeah. And if you wanted to get a good PC laptop, it was just as expensive, if not more. Usually more most. I don't have many more slides and many more slides after this. Another five sites where. Yeah, I think so. Okay. Okay. Sufficiently confused. Any ideas? Anything to share the back? Now I should.

SPEAKER 1
To be with me.

SPEAKER 0
Yeah. The whole process for the chocolate business, what to do can be shown as a treat. Yeah, so you can.

UNKNOWN
I didn't know you.

SPEAKER 0
Then also take on what I already said in matching between. And what's the use of that? Yeah. Yeah. Correct. And, and yeah, there are different ways, you know, to achieve that. So this certainly is a good explanation for where, where, where the chat bot is, as could, you know, could provide transparency to someone trying to understand that other ideas. Yeah. Yeah, You are raising an interesting point because I think evaluation is about the goals of evaluation are a little bit different. But you were raising an interesting parallel between evaluation and something like being able to step through or decomposing or being able to step through it. So I think the goals are different a little bit in that I would say that evaluation we typically take on, take the perspective of the finished product. You want to test it, how well does it work for the user and stepping through different configuration, different things. The user might say, What happens if I try to break and say gibberish to it? How will it respond? These are the kind of things I would try to do in an evaluation, for example, and obviously other things as well. But with something like honest and that's can be similar to kind of trying to simulate it. So I think when you're developing, I think as a developer you have kind of use of living between those two worlds because you're trying to you're obviously developing the chatbot and you as if you're testing it. You are simulating different parts of the chatbot. You're trying to test, if that makes sense. So there is there is a parallel there. But I think the yeah, the I hope that explains that a little bit. It's I think evaluation comes from a different world where we want to test and evaluate systems as as and when they are finished. The developer says this is a finished system. Here you go and then we can evaluate the system. Whereas something like simulates abilities, Composability is much more about understanding things like level of the components of a system which is typically beyond what you might do an evaluation. Yeah.

SPEAKER 1
So. Maybe just by saying like, Oh, if you want a trip that goes to this area, this area, but you want to understand the price. You could be like, oh, they look this other place that similar to this place with the recommendation of it.

SPEAKER 0
Mm hmm. Yeah. So I think what you're saying is, is how it's almost giving advice to the user, providing explanations of how they could game the system in a way or how they could achieve a better price in the this kind of positive model. So it's kind of explaining the model in the way in which they might recommend flights based on price to the user. Right? So that's what we would call a post hoc explanation. And now we are I suppose it has elements of that because it's. I was giving. Yeah. It's kind of. It's not making the pass. It's not really doing any of those three things, but it's doing the other kinds of transparency, which I'm going to talk more about in a second, which is not transparency, which is post hoc explanations. Yeah. Any other thoughts? I think one one more thing to note here is that I sort of left it ambiguous so much as someone. I think it matters who that is, doesn't it? So is that a user of your product, In which case maybe you could talk more about like, I think your idea of providing something in the output to the user that might make some something understandable about how it works would be more appropriate. But then that someone could also be me. Yeah. Or Jeremy, which obviously when we're mocking your systems and your code, we obviously need to understand it in those in those ways. But also if you're working in a team with other developers, it's the same thing. So if you're working with others on the code together and they want to understand how it works or, you know, for educational purposes, whatever it might be, or your shareholders in the company you work for. Right? They might, might actually try to understand all of it. So it depends a lot on who that someone is and whether this is something you do by showing and talking through the code or just having good commented code. Right. Or whether that's by by making it by making it transparent to the user. So I think these matters. Okay. So post-hoc, interpretability then as I mentioned, the other kind of interpretability that Lipton talks about here. Right. Is giving explanations of what else the model can tell me. And there are different kinds of these types of explanations. There are text explanations that offer justifications of a decision in natural language like the Amazon example. You know, you're seeing this because somebody else bought this visualisation. You could also use that to show why, you know, why classification is is being done in that way. There are local explanations which refers to something that goes on within a neural network and tries to basically break apart a little bit the different components or the different stages of what goes on within a neural network. There's explanations by example showing other examples but are most similar with respect to the model, and I've already given some examples like that to my classification. I think this is Juma because of these other five examples. So I know what you meant because experts have validated them. Okay. So stepping through those and a little bit more detail with text explanations that there you might have a language model, such a recurrent neural network, and that is that you you you use in addition to another model that generates the predictions and you're using that additional language model to generate explanations for that other model. So you can become quite overengineered very quickly. I think if you're trying to trying to to do this kind of provide those kinds of explanations. And of course, this doesn't tell you anything about the correctness of an explanation. And that is where this post hoc explanations differ from the idea of model transparency. Right. The idea of that is you can go and inspect the model itself. So you can also know something about the correctness of the model. Whereas with explanations, that's something that's being done after the fact. You don't know whether the model is correct. You don't actually know whether the explanation is correct. Visualisation pictures. That's more than a thousand words. Clearly, if I show you this, I can very, very quickly you can very quickly grasp why why these things are classified into three clusters regardless of what they are. But I just show you the picture and I don't really need to say anything any much more than that. Especially popular in computer vision type problems where you can offer explanations with graphical data. But however, this also lacks apparently a rigorous standard of correctness as well. If you show if you show this picture, for instance. I mean, there are other ways of slicing these. Who is to say that just because I call it this in this way, that's the correct way of doing it. There's this idea of local explanations. And I think this is best, best understood by using an example like this one. This is from an influential paper that that is about the line model, ally, Amy. And so what this does is it uses, for instance, here well, this image of the original. Image. And then these are examples of explaining the concept of an electric guitar. Electric guitar. Right. So for that, it's it's then highlighting the region of the image that it associates with the concept of electric guitar. The next one is explaining the concept of an acoustic guitar and then highlighting the regions of the image that are associated with that concept, and then finally explaining Labrador and then highlighting those regions of the image, which is an example of providing local explanations. Why is it seeing an electric guitar in this picture that doesn't have an electric guitar on it? Right? So these kinds of explanations can be quite effective. I'd also understand, Eris, why? Why is it making errors in its classification and a given model that pictures also I think the papers also on Moodle. If you want to have a look at some of the more some more of the detail with regards to two lines, what else can the model tell me? So explanations by example. We've discussed this. So how would you do this in a in a kind of machine learning, for instance, model, you could train a new model for the task, not only for the predictions, but also to provide the learned representations. Right. So instead of just just giving you a label saying, yes, it's cancer or not, it also gives you, you know, that sort of a this is an unrelated image for illustration only. But it could also give you a range of images that it shows you that show something that that we know is the ground truth. Right. Then you could use the activations of hidden layers to identify K nearest neighbours. And and then again, we could show those those near those nearest neighbours as examples of similar types of issues. And this is this is how humans do it typically, right? They might make an explanation of something they see based on some other examples. We know this, this, this is cancer on these five images. So we can justify it in that way. It's how humans do this by analogy a lot of the time. So just kind of finally wrapping up, there are some caveats, as always with any types of techniques and approaches. And these the linear models are not necessarily strictly more interpretable than deep neural networks. It depends on the kind of transparency, why it's likely that it's true for algorithmic transparency, but there can also be high dimensional features. So know then you also have the problem with linear models that they use simulated ability and decomposed ability as well. E You can probably make claims about interpretability quite easily, but they should also really be qualified. It's not. It's sort of again, it's sort of depends on the context of the individual application and the dataset and so on. And it's not kind of a monolithic concept. And in some cases transparency may also be at odds with other kind of values or other kinds of objectives of the system. For instance, it might this short term goal here of of trying to build trust with the doctors by making models more interpretable or transparent, might actually clash with the longer term goal of improving health care, for instance. Right. And I think quite often transparency is also critiqued for potentially being problematic for privacy. So when you have a lot of confidentiality, if there is data that that is being, you know, used by the system, that is that is personal data, for instance, then that has, you know, obviously you can't make all of that transparent because that would invade someone's privacy potentially and and be against data protection laws by. So you've got to kind of way of the trade-offs of these types of approaches. And then the issue that you have with post hoc interpretations or explanations that they may be misleading and you might be deliberately or not optimising algorithms to present misleading but plausible explanations, just because something is plausible doesn't actually mean it is correct because there's this disconnection in post hoc explanations. Okay, that's it. So what's next to this lab we have this week is going to be related to coursework stuff, and we have another quiz next week. All right. See you soon.
