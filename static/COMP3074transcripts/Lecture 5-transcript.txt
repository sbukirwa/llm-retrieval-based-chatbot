SPEAKER 0
We can. Welcome back to lecture number five of Human Interaction. Today we're going to talk about some of the issues, some of the problems, concerns that we have with AIDS, particularly around different kinds of biases. I'm going to talk about gender and racial bias and a few moments in particular. And so this lecture is going to be split into three parts. I wish I didn't have to talk about it, but certainly I do. And there are issues so such as racism, algorithmic bias in relation to and racial bias in relation to technology, in relation to AI driven systems in particular, that we need to talk about as we can understand and avoid them in the future. In part two, I'll talk about I'll show different kinds of research that is examining algorithmic, racial and gender bias in in in different kinds of machine learning models. And I know these systems are particularly looking at facial recognition, advertising and checkbox there. And finally, I'll talk about with embedding some bias and modern voting system thing that you'll encounter as a person of working in natural language processing as well. When we talk about how word embeddings reflect bias and stereotypes as part of that. So before I get into it, I'd like you to talk to the person next to you for 3 minutes. Can you think of some examples of algorithmic bias and what causes inspires? Do you think so? Yeah. Talk to each other. Talk to your own, your neighbours for 3 minutes about this question and then we'll have a quick chat about it. Okay. Carrying on here as you're watching this recording I just posted for the discussion. Um, so John McCain's good. So another definition of algorithmic bias is here at the top of the slide. Systematic errors or systemic errors in computer systems that lead to unfair outcomes of judgements. Here are some examples of the kinds of biases that exist racial bias, sexist ages, ableist and religious bias. So I think when we think about these kinds of biases, we can also notice the the similarities and the parallel parallels to the fact that what is enshrined in law in the UK in the Equality Act 2010 is this notion of protected character characteristics. Have you heard of that? Have you heard that phrase before? Protected characteristics probably have is quite common to use. Here is a list of protected characteristics and I think if you think about problems with A.I. systems, they they sometimes inadvertently because it's not. Why? Why is another question. But they discriminate against one of these protected characteristics. And if that is the case, then we can talk about a system that has inherently algorithmic bias. Okay. So you can think about all of these protects current characteristics age, disability, sex, gender reassignment, race, religion, belief, sexual orientation, marriage, civil partnership, pregnancy, maternity. These are all protected characteristics in law in the U.K. and I think it's useful to think about when we think about algorithmic bias and the parallels two to the two to this set of characteristics. Yes, please. Isn't this bias.

UNKNOWN
And the result of that data? Why is it called bias?

SPEAKER 0
Well, I think it's I think it's you can treat it as a sort of a collection term for a number of issues that can exist. Right. We can we can also just talk about different kinds of bias. And Yeah. And you're saying is that yeah, instead of we can it's probably more accurate to say the bias stems from something like bias training data. Right. But I think the collection term that's often used in the literature is algorithm bias. Okay. But it's not that doesn't necessarily mean it's always the correct term. Right. It's just like a label under which we can understand a range of problems, the range of biases that exist within the AI driven systems. So, um, talk about one of these issues around racial bias. So when we talk about race, we need to understand that it's a multi dimensional and multidimensional concept really. Um, phenotype and physical features of how, you know, how someone looks is certainly part of that. But also things like self-defining identity, um, and racial ancestry also play into that. And so it's not just about how someone looks. Right. But I think most importantly, race is also socially constructed. So it involves material and concrete consequences for people living in societies. And here's another here's an example of what is a definition of racism. Race and compliance comprises attitudes, actions and institutions which contribute to relative disadvantages for members of racial groups. And I put the reference to this paper here, Critical Race Theory for HCI. If you want to read more about some of these issues, we're going to have some some more in-depth kind of look at some of these problems within our area of interest, which is not processing racism in the UK. Sadly, you know, even though you might look at the UK as a as quite a liberal country where racism is at least one of its it's a crime. And also it's you know, it's it's kind of seen to be perhaps less of an issue than in other countries, but it is actually something that people experience. And so something that needs to be recognised. And so this sort of racial bias is actually widespread. Yeah. Even in progressive societies like the UK is an example of a survey that was done by The Guardian. And you can follow that name from your PDFs and look at that in more detail. But just to pick out one of the examples here is that 43% of people from black, Asian and minority ethnic backgrounds, Amy, have been overlooked for work promotion and 38% have also been wrongly suspects of shoplifting, compared to 40% of those who identify as white. And you can see picking out some of these examples that, for instance, here someone, someone at work university on a school trip treating differently summer clothing passed on with general appearance. We can see that within Vietnamese respondents. The number of those that that agree with the statement is higher than. Then the white response down here, and this is broken down by all respondents from Vietnamese backgrounds, women and men. You can see that women basically are more often subject of this kind of discrimination. And with white respondents, you have got much lower percentages. So these are these are concerns that exist in the society as well. So we shouldn't take them lightly. And so let's talk a bit more about some of the some of the issues we have with technology and racial bias. Different kinds of biases exists. And these range from algorithmic bias to bias issues with sensors as well. So issues with the actual hardware. So that includes things like system behaviour such as errors and faults and that lead to unfair outcomes or judgements due to a person's race. So this I think, you know, something like it doesn't really matter at this point whether this is accidentally design into the system or on purpose, which you know, would be malicious and terrible, but it doesn't matter if it's still racially biased and it's going to still be seen as a racist type of thing. As I mentioned, this can be about the physical dimension of race, such as darker skin tones, not being accurate, actually recognised by sensors and algorithms and problems in computer vision and image processing. And we can see some of those examples as well. And but these issues can also be due to race based correlations, for instance. And so we think about targeted advertising. There's an example I'm going to show from that and also issues with filter bubbles who are going to talk about filter bubbles more in a future lecture. And as we already discussed, it can also be the result of bias training data. And often this is the case because of historical reasons. So a lot of the time systems, as we learned to that data sets from the web and from, you know, the collection of texts, many of which go back decades at least. And so there are some there are more apparent biases in those kinds of texts because the societies were different. And the ways in which things were talked about were quite different to how they are today. So that's important to think about when we are using training data, what is what is the nature of the the types of statements that are encoded in the training data that we use? So we as I mentioned as an example, I think of the that shows the physical dimensions and it's not being recognised by censors can be a problem. I think some of you may have seen this. Very nice. Okay. Normally you're training. I'm trying to block the block without doing it, but margins are low, nor do I worry about a duplicate today. Who look to you after the listing is here. It seems that Bizarro went the set. Nothing short of it. Again, we have Canoga Park in Lower Manhattan. Why it's a good thing you can laugh about these things as well as sometimes it's the only thing you can do. It's so terrible. It's it's laughable because they keep jumping back to the first lines. Okay, where was I? Here. And so with computer vision, we also have clearly that system that we just saw that that soap dispenser doesn't is not business is a simple system that just uses a sensor. And, you know, there there isn't a computer in there. Probably it's just a soap dispenser. But with computer vision, we have similar kinds of issues where systems found to misidentify those with darker skin. For instance, one example here from the slides is a paper that found that autonomous vehicles, the idea that you have a system that that uses computer vision for collision avoidance to avoid obstacles in the road tends to think that those those types of systems might basically be biased against people with darker skin is a problem, right? Because well, it's an autonomous vehicle. And that might mean that the collision avoidance system doesn't work as well for people darker skin, therefore potentially harming those people. And of course, we also have problems with facial recognition in identity matching. And we've seen the example in in the first lecture I've given, for example, of Chuck Williams, who was wrongfully arrested because an image of him was, was matched, uh, by error where there was an error and but his image was matched with that from CCTV of a burglar going into the shop. So let's take another couple of minutes to discuss now that we've seen some more examples and what could the developers do to avoid this kind of bias, let's say So talk to your neighbours again. So talk to our folks to 2 minutes because next to you, what could the developers do to avoid this kind of bias? If that was not the right buttons for us. What is this? Never seen before. Okay. And it goes back to the beginning. I've done something now. Okay. Sorry about that. Trying to be clever. Four outsmarted by Microsoft one more time. So, yeah, in part two, I want to talk and give you some examples of research that looks at algorithmic and racial and gender bias in some more detail. So this is a piece of work that looked at issues with facial recognition, and now they did something that is is they did they did this to prove a point, not because these researchers said, oh, this is a good idea. Right? There are generally, in my opinion, there are problems, but you should not throw away items and machine learning. You should simply not try to do this kind of thing with a AI. It's a bad idea to try and do something like gender classification based on facial recognition, right. That has all sorts of ethical issues. But these authors are doing this here to prove that this is problematic. Okay. They're not saying this is something this is a problem where this is that's where A.I. or computer vision is a good solution. They are saying they're doing this to demonstrate that it's a bad idea. Okay. So what they did was they used three so-called state of the art commercial gender classifiers. These are algorithmic algorithms that are available commercially, and they sort of treat them as a black box. Right. So they can't just apply the algorithms and to see what happens. And what they found is on the dataset that the benchmark dataset. So I'll come talk about in a second is that darker skinned females were most misclassified. 35% error. Right? And light skinned males were were least misclassified with 0.8%. Right. So you can clearly see in the data how this bias turns out in and turns into error in the system. But so you might you might ask, was it down to the test data? This was the point that they actually filled a in inclusive benchmark dataset, as they called it. And this is this is this is what they really tried to build a dataset that has a whole range of phenotypic and demographic representation so that they had a test dataset that really represented, um, yeah, a wide cross-section, uh, across the kind of spectrum of phenotype phenotypic appearance of people's faces. You can see that here, and it is with this dataset that they were able to show that that these, these areas exist with these commercial gender classifiers. Okay. And another example of the racial bias in this time, targeted advertising. And the story here is and this is it's important to take the context. This is Google in 2013 or when it was published in 2013. Right. Google has changed since then. Right. But it's because of research like this that Google has changed them. So what Google used to do about ten years or so ago is that if you search for a name in Google, it would give you targeted ads that that use the same name that you just searched for like this. So which would show the knobs on the sign. You know, next you search results for the person's name and then it would you know in this case, you would show these ads. This is in some sense. This is some kind of free social network type website where you could look up people. Right. Whatever. And that doesn't mean that people were necessarily in agreement that this was happening. Right. But people's names were used. And so you have something like this with this person. Search for the Tanya Sweeny here, and then this would obviously turn into this kind of clickbait, right? So Tonya Sweeny, arrested question and you know, so click on it and whatever it would take you to this instant checkmate website. And then on the other side, you have another example of a target that we found some hiring contact personally for you and for Congresswoman address some of this is a people smart dot com. Again some sort of free social network. I'm not sure whether it still exists or not. And what they actually did was they used different they they used they use they basically constructed an experiment to see what would happen when they were using more black on black, identifying first names compared with whites. Identifying first names and their conduct. The word arrest would appear 60% of the time, with black identifying first names like this in comparison with white first names where Blood was on 48%. We have here, for instance, someone searched for this person in the times we which actually did not have a criminal record. So there isn't there isn't an empirical justification for why this would come up. Right. There's no criminal records, but it is a black name. On the other hand, there is a person named Kristen Harrington who has a criminal record. But, you know, this is why they didn't find it. So what they found is is again, another example of. Racial bias in in this in this in this case, how Google and Google actually show targeted that's offensive is because of research like this that they are not doing this anymore. They Google does not display ads for person search results anymore. You can go and tested on Google right now and you will see that there will not be ads displayed in that case. And this is the reason why. So we've come across already this is the 2016 chatbot that Microsoft developed and which was released on Twitter and it became racist or it starts to produce racist language and within a day, within 24 hours. Wow. And so clearly, you know, Mexicans take it off offline very quickly. But then there's quite a lot of interesting research into what actually happens. This this article here forced you to read my slating, I thought talks about the blacklist. And so the blacklist is is a way in which you can avoid in any way to learn. You know, avoid it. Keep it. Stop it from learning words that are on the blacklist and responding to those words. So it's a crude message. And and this is what this paper talks about. It is crude because it's it means that, you know, you are effectively censoring certain words from from being able to be processed or being able to be used in generation A person. Kazemi here was a creator of films of Blacklist, said that he's willing to lose a few words like Homogenous in pop song in order to avoid false negatives. If you see what potential words that are blacklisted here, which of which are thought these words here, which are slurs that would, you know, would be probably be on a blacklist if you were to employ that method. And the paper also says, you know, by deleting the N-word, we do not eliminate racism. So this is, I guess, a point, you know, when you have these kinds of methods to target issues with, for instance, hate speech, it is quite difficult to do. And if you have a tool like the Blacklist, it is really quite blunt to that will just filter out the these kinds of words and is probably if you try to to do sort of hate speech, talk with a lot of the modern chatbots, they still just operate on a blacklist model and they will just not respond to those words if they are on the blacklist. So this these authors here slicing at all the PDF is on the move. They should suggest that instead of just deleting words, can we and how can we build chatbots that behave differently and that can handle sensitive topics like race, power and justice? And they kind of go through three areas of checkpoint architecture that's more relevant for generating better responses, as I say. So they discuss these three page caucuses, corpora, language, understanding and learning. Just talk about just want to highlight some of the ideas that have here. Talk about the data corpora. Well, in other words, the humble database. What can you do with the database when you have especially you have these issues with such as with Tay that you have a dynamic, potentially dynamic and continuously growing corpus, mostly the whole idea of which is still, you know, in some ways is still this is still something that is, is desirable about your checkbox to, you know, to be able to learn from from the language that people use to talk to it. So it gets more intelligent as time goes past so. The authors are saying that rather than to assume you can avoid this kind of language, you should instead anticipate that race talk happens and try to collect and aggregate dialogues to train chocolates and more respectful race, culture, whatever that might mean. And one of the things that they said would be important has to do is to strive to for a diverse representation of cultural, cultural references across ethnicities. So again, this comes down to what we discussed earlier. So having a better training dataset, for instance, when you want to be able to talk to your chatbot without artists or authors, books, music, public figures. And, you know, it's important to have some diversity in that and not a spectrum of, you know, the knowledge base that you are effectively coding into by means of database or other kinds of ways of data representation into your into your training data. So it's important to think about who builds the database. And so you need to have a diverse team probably, or it will be a lot easier to get this diverse representation and cultural references into your data. If you have a diverse team of people who are building the database, right, Because they say, Well, there's all this and all these artists that I know about but are not part of this database that you're building. So they should go in. So diversity in in the teams that are building these these. These systems, this important, important way of addressing and avoiding some of these issues from occurring in the first place. And yet. So you should strive for I think this was your point earlier as well, to have people with different native languages, accents and dialects, ethnicities and cultural contexts as part of the themes of building systems. So next, the next survivor, they talk about, as is the language understanding side of things. And we've already encountered and talked about the use of probabilistic language models. So Jeremy spent quite a bit of the last lecture on that. And in fact, you have started to build your own probabilistic language models in Lab one. And now it's important to just kind of conceptual level. Remember that this is, you know, based on which words or tokens are likely to appear one after the other, right? We've got these engrams playgrounds, universe programs and so on. And this is a powerful way to generate valid statements, of course, But it doesn't tell you anything about whether or not the statements are racist, sexist, untrue or otherwise problematic. Right. So I think that's a really important thing to bear in mind, is that building a probabilistic language model, you know, can can actually ingrain some of these problems. And it doesn't it doesn't help you at all and in figuring that out. So these authors suggest that we need this capacity for meaning in context and natural language processing. The errors of semantics and pragmatics become much more important. Right. So what they, for instance, suggest is rather than having to use general chatbots can perhaps, you know, when the goal is that they can just respond to any kind of query you may have. Instead, you may have a network of many domain specific boss and you have got you put a lot more effort into building a bar that is very good at handling a certain domain of language, for instance, travel or music or or, you know, books, whatever it might be. And you kind of make sure that that box is an expert at handling queries that are in that sort of domain. And so when you build a chatbot, you end up actually building multiple chatbots when you just they're in the background. And when a user asks to query you, you pass a query on to the expert and the expert handles that woman. And you know, it's a different way of thinking about how we failed to chatbots this kind of architecture that I just outlined that is is being built into what I called more experimental chatbots like Alana. A lot of say a bot, you can look it up there now company. They won the grand challenge a few years ago. So they're a team of researchers from the University of Heriot-watt and they built this this chatbot and it works very consistently. And you might might see it as an example in Alexa Lecture as well. And the final point they're talking about is turning. So of course we're talking about machine learning. Yeah, in the context of chatbots. And they're asking how accountable are the models, the machine learning models. And of course, with machine learning you have this issue of the black box, right, where you know, this this idea is that you throw some data at it and this black box stuff happens. You don't know exactly. It's hard to explain what exactly is going on and you have some sort of output. So how can you trust it? How can you trust in A.I. like neural network? How can you how can you trust it? So there's some issues with machine learning models. Here's one example from research that talks about this idea of internal and external context and how that's a problem for machine learning. And so what the research did say was there was a bunch of research where they looked at clinical data, data from people who were in hospital, and they tried to predict the probability of death from pneumonia from that data. So they had historic data of people who had been in hospital. Some had passed away, some had survived. Right. And they tried to predict on the basis that that was what was their profile, what you know, what's what is it within that kind of that that that sort of health status that makes them more or less likely to die from pneumonia? They all had pneumonia. And so they tried to predict whether somebody would be a high risk patient or low risk patient. Um, and the models actually predicted with high accuracy that asthma patients so people who suffer from asthma, respiratory disease would not die of pneumonia and therefore. The VA system classified them as low risk. So if you had a scan, the system would classify you at low risk. Why would it do that? Well, we all know intuitively, I think even if we as non-health experts know that if you have asthma, probably pneumonia. Is that thing for you, Right? Because you already have you have an issue with your respiratory. You have respiratory disease. If you have pneumonia infection, then you're probably that's pretty bad for you. So the issue was that the models failed to account for the fact that people with asthma were hospitalised quickly because the doctors for those who, you know, health care workers in the system knew that they were at risk, somebody had asthma and presents with pneumonia. They're much more likely to be hospitalised quickly and earlier on and therefore not die because action has been taken early on to save them because that makes sense. So, so this is this idea of external context. The machine or the machine learning model did not represent that external context or the model that was the data, which was the, you know, the health status of someone in hospital. You know, this external context was more part of it and therefore could not be taken into account. And therefore, we've got to be careful about the metrics of performance. We've got some machine learning. So accuracy is the number one metric that keeps being bandied around to show that machine learning models are good, right? In this case, this would have a high accuracy, but it's actually harmful if you were to use it for decision making. So if you were to rely on this model for decision making, you would not you would just be classified as low risk and you would not disclose them. So the reason why they're low risk is because they have been hospitalised quickly and early. So that makes sense. So it's important to question sometimes the nature of what is encoded in the models. Of course also I think the takeaway message here on accuracy itself, it's not a good measure to evaluate machine learning models alone and explainable. I this this notion of making A.I. more explainable actually starts from this premise, from understanding exactly these kinds of issues with machine learning models and black box A.I.. I will return to this later on in another lecture. But the idea of explainable AIS provide transparency or interpretability of the inner workings of an AI, and this may or may not involve what I just talked about, which is kind of internal and external context as well. I'm going to wrap up. I've gone on too long, so I'm going to just touch on this and then we'll come back to this the beginning of the next lecture. So word embeddings. So word embeddings is something you're going to come you're going to come across practically within the labs as well. And word embeddings are a way to represent text data as vectors. So you can represent data, you can represent words, aspects like this. And you have word, you have got the word dog, reds and cats. These three words here represented are this in respect of space. And to to do that, this will get into the maps of how you represent text as vectors. But this allows you to do things like similarity and to to compute similarity between different words and association. And you do this by co-occurrence like words co-occurrence together, and you've already seen that kind of in the unique rhymes and backgrounds in the probabilistic language models that you've been building. And what in natural language processing happens is that this idea of coherence is conflated with meaning, right? There's something about these things are similar, so they must kind of mean a similar thing, right? So they are conflated with meaning. So that that is a problem that we need to bear in mind. But really, these things are based on statistics. They're not based on semantics. Okay? And words are considered to have similar meanings If the vectors, the closer together and differences represent relation between between different words. So what you can then do is vector arithmetic. You can do arithmetic with vectors to understand how how things are connected. Or maybe or maybe not. So this paper then shows how. What about exclusive in that bias? Right. So when we do vector arithmetic, that is also essentially prone to bias. They did this another puzzle here in this paper. This is also. It is also a moral minus to king as woman is to x, and that you can denote like this and then it solves to queen. Right. So minus to king as woman is to queen is the best answer, because the difference between the vectors man and woman is roughly equivalent to the difference between the vectors king and queen. Okay. And then that is problematic because then there's also encode sexist associations as well. For instance, the same vectors difference is also roughly equivalent to the difference between computer programmer, homemaker, wife. You can see how these are job descriptions, occupations that are traditionally more associated and stereotypically more associated with with women than men the other way around. Yeah, I hope you you're going to prove us all wrong and show how it's different in future. So they go on to basically look at these X, what they call extreme occupations. So these are occupations both closest to she and to the vectors for the prepositions in this word to vector you said, and which which has these embeddings. And they were strongly correlated with Travis austerity, the quality of the occupations. So these are stereotypically occupations that are associated with women, and these are occupations stereotypically associated with men. And they're basically showing that the embeddings contain biases in that geometry. And there's some other research that basically shows that over 100 years of data, um, they look at how bias towards in this case bias towards Asian people changed over time and is this is a form of quantitative sort of science right where you can take the large datasets in this case over a hundred years and examine how bias changes towards social groups. There's another example that basically, again, just shows us when confirms that these biases in White Embeddings exist. And so they say that they show that this relates to how people's basically implicit associations, which are sort of what you might call unconscious bias, and they relate to that as well. And so I just want to wrap up this lecture by just summarising what we can do about it, because clearly we've highlighted a lot of issues and we talked a little bit about solutions and talk more about them. Things start by understanding it as the education point. Change starts with understanding, and so there are lots of different approaches to mitigate bias. Some of them we touched on, some others we will hear more about in the next lecture. There are technical solutions. There are solutions to provide transparency and openness with regards to data, being aware of the limitations of the training data, for instance, having an open training dataset and not a black box algorithm. We have no idea what data was trained on, for instance, and having diversity and inclusion in the models themselves. We get to get to that in more detail. Just importantly, there's a quiz next week. So the question will be during the hours to make an announcement on and the coursework will be released, we see some.
