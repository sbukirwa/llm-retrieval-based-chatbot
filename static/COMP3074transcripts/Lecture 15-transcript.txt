SPEAKER 0
Lecture 15 which means we are we haven't got many likes is left now actually before we head for the Christmas break. And there won't be any lectures after the Christmas break either. And so we're in part two now of the module firmly where we're looking at. We're focusing on on voice user interface design and conversational design. And as one of the topics that is important here is automatic speech recognition or ACR, which we often have kind of mentioned, talked about a little bit as well. But today's lecture is going to be focusing on that entirely. So we're going to talk about some of the key features of ASR before coming on to discussing some of the key technical challenges related to S.R. So as is also known as machine transcription or speech to text. So this is really the recognition and transcription of spoken language into text. And of course it's a crucial component to turn any chatbot into a voice user interface. We think about it as possibly arguably one of the core distinguishing features, if not the one, you know, in technical terms, distinguishing feature, distinguishing text based chat bots from voice based or speech based voice user interfaces. There are other terms, of course, as we know, for we like, you know, personal assistant, your Alexa's, your series and so on. And there are a number of popular engines that allow a developer to make use of an app store that's already been developed in much the same way as using software libraries and so on. You can use different closed source libraries or engine sorry, such as Google's Cloud Speech API and IBM's Watson speech to text Amazon's Transcribe and Alexa Skills kit, which is what you use essentially to build any skill for an Alexa device. And there are also open source alternatives, much, of course, lesser known names you probably wouldn't have heard of any of these names probably was okay. And things are some of them. And these so these are much less known, but they are open source. So they have advanced edges. There are also a number of Python libraries that allow you to essentially use some some features that are built for Python, specifically like speech recognition, Google Cloud speech, which is obviously for the Google Cloud Speech API and built as a built out as a Python library. What's in developer cloud? The same for the IBM Watson and so on. So these these kind of make kind of the work to link to the glue between the engines and your python code much easier because somebody else has already done the work essentially. So if you wanted to build something in Python, then these are some of the places where you could go. Of course, this is not part, of course, one at all. So I'm just saying that here for any future projects you might be working on and so let's get together and discuss for 3 minutes. If you had to choose for your project between a closed source and an open source engine, which would you choose and why? And can you think of some of the trade-offs either choice tests? Yeah, discuss with each other. I've listed the engines there again, discuss with each other. And let's chat in 3 minutes. Recording again. So. Some other things and some of these things were being brought up already in this discussion just now. So when you are choosing an engine, some of the key considerations are the robustness of the data set and the accuracy of the data set. And as we kind of already mentioned, you know, in this case, size really matters. And quite large companies such as Google do have an advantage because they have huge datasets available. And there are other things that you may want to consider, such as end point detection performance. So that's something when I talk about what we mean by that, this endpoint detection is the computer, the device recognising when you finish speaking, when you begin and when you finish speaking, and the performance of that is very important for the perceived usability and yeah, ease of use of the, of the voice interface that you are building on the basis of the score. And there are also advanced features that you may want for your system, but not all engines have them. There are things like end lists. We're going to talk about those and other kinds of parameters that you might like to set, like end of speech time outs. You might want to be able to manipulate the timing associated with those, and you may want to have customised vocabularies and things like that. And I think all of these things here as you want them in your to build your voice interface, then you may need to look at open source systems because they allow you to build those things in. Whereas often with closed source systems, it's a black box is essentially you throw some audio at it, hopefully speech and you get some transcription back and you don't and that's it. You don't really get anything else. And it'll only work for an predefined set of languages and accents and dialects and so on. But it'll work very well for those. But the trade offs are you don't have a lot of customisability and there isn't actually a lot of clever, clever stuff you can do based on the output that you get back. Such as and best lists, which is a different kind of output we're going to look at today. Some some other things that are important about as is barge in detection. So that's a bargain is what you what you call when a user starts speaking while the device is still outputting speech, you can and basically, you know, you just you don't wait for it to finish. You can barge in. There are timeouts, different kinds of timeouts you need to consider with ASR, including end of speech, no speech and too much speech. I guess these are all self-explanatory, but we're going to go into them in a little bit more detail. And then there are endless lists. And I'll also cover those. So with Bhajans, this is, as I mentioned, to allow the user to interrupt the system while the system is still talking. So while the system is still generating output, the the user is allowed to or is able to barge in different options to implement margins, you could potentially implement something where the system immediately stops when it detects speech. Right. This makes sense when you have an IVR. So these are and in God, what is that IVR stands for? It's basically telephone systems, right? So interactive voice s sorry, I'm sure I'm not sure what in Texas. Voice recognition, maybe. Yeah, I think I think so. Something like a voice response. Yeah, in terms of voice response, maybe. That's right. And that is basically the old school kind of telephone system that, you know, from when you phone your bank and it says, you know, to proceed with option, say, one, two or three, that kind of thing. So so for example, you have a banking IVR, you can transfer money, check your account balances and pay, and then the user barges in check my account balance, right? So the user just goes because they've already heard that option. That's what they want to do, right? You can transfer money, check your account, and then it'll continue reading out all the options. So it's nice to be able to just say what you want to do rather than having to wait until the end when the IVR finishes. So this avoids long menus or lists of options. But a lot can go wrong as well. For instance, the voice system might say, What would you like to do? Then there's a bit of a silence and then it starts speaking and you, the user says, I would. And then. The system continues calm and then stops because the user has passed and hit. Right. And you get these kinds of awkward stop start situations where this, you know, where and ways breaks down and has to be started again. The interaction has to be started again. And so you have this issue that when the the V asks the question and then it produces a silence, this encourages the user to speak prematurely. And so from a design standpoint, it's always better to ask the question last. So present the options first, then ask the questions like this. You can check your balance, transfer funds or speak to an agent. What would you like to do? That's a better way of presenting that information. So what? Of course, how it is implemented in the voice interfaces that we talk about a lot like Alexa or Siri, is that it stops when it detects the wake word. So you interrupted by by saying the wake word and barging in in that way usually. So the wake word being the Alexa in this case, for Alexa, for instance. So you say, for instance, Alexa, stop. Or you can use specific keywords. For instance, next, just skip and so on. I think this this type of reliance on keywords makes sense when you have long running actions that are typical on smart speakers because of course things that you know, the smart speaker when we say play BBC Radio six music, it will then play the radio until you say something to it and you don't. Every time it detects speech, you don't want to stop the output of the voice interface. Instead, you have therefore, those systems rely on the keyword or Lakewood to be heard. What is it? That's what it listens for. And this is or shouldn't be an excuse for overly long prompts. So from a design point of view, when you prompt the user to say something just because you are able to, the user is able to barge in, you shouldn't produce as a designer, these overly long prompts, that's generally about design decision. So it's more important to be concise and remember that speech is ephemeral. So ephemeral meaning that no, once it said it's gone, it's out in the aether, it's not written down like text. So if the user look at it and go, Oh, there's all the seven options in front of me, I can just pick one. It was a bad idea to to read out seven options, right? You want to break it down more because people can't write. You know, you produce a sound, you produce speech. It's relying on people's memory. Right. So you don't want to read out seven options at once, but you want to break it down more. Then you have the end of speech time out, which is also known as end point detection. This is to detect when the user stops speaking, of course, and some as our engines allow the developer. So someone like yourself to adjust this. But there's a but typically if you use a closed source engine, it will not there will not be a way to do that to adjust that. But 1.5 seconds is a kind of a rule of thumb, but it could be shorter for user initiated interactions where you know that, you know, the user has has uttered the commands to the device. They when they finish, they're likely to be to be finished. And also for things like prompts that ask for a simple yes or no response, the end of speech time. It could also be shorter and it may need to be longer for certain prompts which require the user to think for longer. So if you ask a complicated question of the user in some way allows them to reflect how their day runs, then you may need to wait longer to make sure they're finished what they wanted to say. And there's the no speech timeout. So that's clearly when the when there is no speech detected, right? So if there's a prompt from the device and this therefore should be longer than a end of speech. So this is you know, this is for instance, if there is a prompt and then there's no speech for forthcoming, again, you might have asked the user a complicated question, and therefore you need to wait longer for the user to figure out what they want to say. And so usually this would be around 10 seconds. And this can results. If there's if there is a no speech time that can result in different kinds of actions, do nothing. So just go into a stand back into a kind of standby mode, which is possibly most common in devices when you say Alexa. And then it starts to listen, doesn't it, and prompts you basically. But say something else. But then if nothing comes forward, the user doesn't say anything else. Alexa will just go away and be back in standby. But in some other cases, you also may want to read from the user. For instance, if you have this kind of situation where this looks like the the ISP, whatever the Internet service provider Zoey, that you interacting with in this exchange and you clearly there's some sort of interaction already going on and the device prompts you what is your account number, Right. And then the sign is that's produced. Then, of course, is that while the user is trying to look up what their account number is, Right. So but if that silence is overly long, then you might do a response like like this. Sorry, I didn't get that. Your account number can be found at the top of your statement. Please say or type it in or say I don't know it. So it's actually what you can see here is the report is actually quite useful because it provides some additional information to help the user in the context that they're in to try and like in this case, to locate the account number. So this is a good kind of use of a prompt, which we talk about a bit more on Thursday as well. There are different strategies for error handling where the the voice interface would produce more successfully, more information to try and help the user along the way. And then you can see here what happens. This user just say, I don't know it and the voice comes back with we can look it up, no problem. We can look at look it up with your phone number and address instead. So it provides an alternative solution to the problem of locating the account number. So it's not just a dead end, but the interaction can continue. There are different ways to forget to proceed with the interaction. So it's helpful for things like this, also helpful for things like system analysis for for you as a developer because it shows where there are problems as well. So where, where, where are these No speech timeouts. You can you can probably identify that somehow the prompt that preceded that is causing problems because there are lots of non speech timeouts could be down to trigger accidental triggering or problems finding the right response to the prompt of course as well. So the too much speech timeout is clearly kind of triggered when the user actually talks for too long without pausing, which is quite hard to do. And, and people don't normally speak like that. So that's quite rare. But it may be useful for things like applications and skills within which the user is encouraged to talk for a long time about something like that as well. So endless lists are important. So these are lists with the end most likely queries the user might have said so endless. So the context is of course something like is a transcription, right? So the context is the this here's an example for a dialogue. So my favourite animal who would prompt the user. So really I want to know more about what animals you love, What's your favourite and the user response. Well, I think at the moment my favourite is going to be kitty Cats. Okay. And then so the the V, the AC tries to transcribe that. Right. So that's the task as has of course it hears this is recorded as a bit of audio, you know, gets sent off to be transcribed. That's a joke. Yes. And, and it produces five candidates and five candidate transcriptions of what the user said. Okay. So and this is what's called an end best list and in this case, five. Right. And it's just it could be ten, it could be 20, it could be 100 in this case, five, five Best list returned by the as ordered by confidence. So the so this is important. So this notion of confidence. So there's something that the system produces, you know, a confidence number between zero and one usually which says you know, between between zero confidence and 100% confidence that the. Assure that the uses that this is the most confident one is? Well, I think at the moment my favourites got to be fit and fat. That's the one that has us the most confident about. And well, I think the moment my faves got to be busy cats. So that's the correct one. And then there are my fans of the kitty and so on, off the kitty bat. Got to be kissin cats. So it sounds similar. Which is? Which explains why the ASO produces the must candidate transcriptions. So now you're the developer here and you know you are the developer of your favourite animals, Louie. And you getting back this endless list. So what strategy would you implement for Ruby to pick from, for your view to pick from the best list and why? Okay, so chat amongst yourselves for 2 minutes and then we'll hear your responses. Mm hmm. So that's the pause, I think. I don't.

UNKNOWN
They tried to take a dramatic pause.

SPEAKER 0
We're going to present a starting. Okay, so. Uh huh. Well, you've asked that question, haven't you? So, you know, what's your favourite? You ask the question of what's your favourite animal? So you know what kind of responses you are expecting. You are expecting an animal name on you. So you can look for that. So the context, the context of what you asked for can then drive the selection in addition to the confidence level, which of course you'll be using. That's our confidence level. But you can do something on top of that. So going through your engram list, that takes into consideration. And there are different ways of doing that, of course. And you all have some interesting ideas to achieve that. But the important point is, you know, there's a context to what you've asked the user. So you can expect a certain kind of response. And so therefore, you can search the engram list for something that fits that kind of response that you are expecting. And let's also help with course correction. For instance, here's an example of what you should not do. You have a travel agent rule that says, What city are you starting from? The user says Boston Travel. Who says, Was that Allston? No, Boston. Was that Allston or Boston was that you know, and it never stops and you just you delete the chapter guy never used again. So the what they could have done here is actually avoiding suggesting the same incorrect option over and over again, even though your engram list suggests you know the top one, right. Always the one with the most confidence. For some reason, the highest confidence level the top one so that every time the user says Boston because I don't know because maybe with their accent it gets transcribed transcribers Austen with the highest confidence level. However, the user does also say no, right? No Boston. So that could set a reject flag when the user says no to the prompt. So you know, you're going through the inverse list. You've presented an option from the inverse list. What you hear in response is a no. So clearly that option that you presented from the endless list was wrong. So you can take that into consideration and then present another option from the inverse list. So taking into consideration the user response in what you do next can also be important. So you move to the next item in the list when you detect something like a note. Okay. Well, let's talk about some of the technical challenges with RSA. I mean, these are some of the things you can probably guess, like I've listed them here. And so with noise, the issue is that, you know, you have background noise. It's just one of them. You have multiple speakers where people talk over each other and things like that. You have music in the background, You have, you know, animals, dogs, barking, whatever. This issue of site size speech, which is, you know, when the user says something and then says something to the device and then says something to the person next to them because the device has no way of knowing. Perhaps there's actually some clever things you can do, perhaps with microphone, with direction of sound, with directional microphones that you have now in these devices, the device there is some something that that could be done to to differentiate whether the user might might have just turned their face the other way. And therefore the sound sounds different. The sound sounds different to be able to distinguish that. So there's some things you might be able to do, but maybe not a lot. There are other things you can do, such as enriching the training data sets with noisy examples. So that's a job for those who are actually developing the ACR engines. Developing ACE engines is is, as you can imagine, very time consuming, relies on a lot of data and so on. So for a developer, developers like yourselves, you might you know, this is the kind of the choice between, well, picking an open source. One would hope to maybe train it with some additional examples for your context or using a black box closed source one because, you know, Google built it and so it's going to work, right? But if you are building your own assigns and then you may start from scratch and you may just think. It's like building data sets that deliberately have noisy backgrounds in them. Another thing you can do is reproduce. And this is the strategy that makes a lot of sense in a lot of different cases, providing you may be able to provide hints that it was too noisy to understand as well, because you may be able to distinguish too from your ESR because you have a low confidence level, for example, associated with the with the candidate transcription that is returning to you. You may be able to say, well, this will low confidence level, which can mean a number of things, including could be too noisy. So you just ask the user to re the issue with multiple devices triggered by the same phrase. Some of you may have had that. You know, when you say wake word, it triggers everyone's phone. Now, it doesn't happen anymore, really, because like Siri, for instance, if Apple have done some some work to basically attune the recognition of Awake to the user's voice. So when you first use your Siri, it asks you to say, Hey, Siri to us was tries to learn the sound of your voice so that other voices don't trigger it. However, it doesn't, it's not infallible. So if you have, you know, in general, like male voices, you know, they have a they can sound similar, a female voice can sound. So it's definitely not infallible. And it definitely happens that you can trigger someone else's phone. Multiple speakers talking to the same device as it's not very well equipped to handle what's called overlapping talk. So when yeah, when people talk at the same time, that's called overlapping talk or over talking. People finish each other, set each of the sentences they don't enunciate properly, etc.. So there are lots of challenges as well with that. When you have multiple devices that could respond, which device should respond? When I say, Hey, Siri, to be in my kitchen and sometimes my iPhone and sometimes my iPad responds and actually I wanted to be one, but it's sometimes the other and I don't know why and I don't know how to change it. So for the user, it can be quite frustrating. But I think for the developer it's also quite challenging to decide even for Apple, because they still get it wrong as well. Children are a problem because as I struggle with children's voices, they have obviously got more high pitched voices because they have shorter vocal cords, but there's also less and there's less training data, of course, because, well, you just don't have a lot of good reason to put a lot of children into a room and recorded a lot of things into a microphone. So you have low accuracy because of that. But also there's this thing called child's talk. You know, young children talk funny, don't they? They don't talk like we do, like adults do, and not very grammatical a lot of the time and they stutter. Damian They have long pauses. They repeat themselves and so on. That causes a lot of problems for us as well. Here is an example, you know. Sorry, these examples are from, from the book. So sorry to plug that, but hello Barbie or whatever says, What would you like to be when you grow up? So as a child, as the child gives some sort of response, doesn't really matter what it is or the strategy is just to say, sounds good, I want to be blah. So sometimes the strategy is if you are targeting a certain audience and the response they give may not really matter. It's just kind of more about the interaction. They have said something. You can just you can, as a developer of the assets, produce some sort of output. So the child has said something great. You can say you can say something else next just to keep it going. Other issues are around names and spelling and alphanumeric characters. Well, we all know they can be difficult when you hear someone's name too, to imagine what it would be spelt like, right? Because names are from different cultures, different countries, different languages and so on, but the context can help. So here's an example where the user says, What is the the what is Adele's latest album? So I just kind of live transcribing it as a sort of example is transcribing What is that? It tells what is Adele's is transcribing, that is. What is that There's. Of course, a transcription error here. But then I say the latest album. Now the context of latest album then associates what I said before with an artist. So now I know. Oh, they probably said an artist's name, so I can then look for, you know, with my, my audio, my full name that that I have and my acoustic model, I can then look for artist names that sound like that. And then all of a sudden you get a doubt. So, you know, when you say latest album, it can then of course correct what came before and other known data can help and other examples for instance if you're asking for credit card number, there's only a finite number of valid credit cards and you can tell whether it's valid by using something called the checksum. And so you can run that in the background and you can say if you have some confidence issue, you can, well, pick the one that's valid. If you're you know, you have your own best guy and you have your own best list. And so you can iterate through that and you can check against the credit card checksum. You have registered username. So if you are asking, you know, if the user is saying the username and you can use that, you can use post codes and so on. Since he's closest to the current location and so on. So the point is context. You can use that sometimes in clever ways to essentially to rule out certain candidate responses that the assigns produced. Some good practices around data privacy, as you can imagine, with ACR. And as we've covered before, because this records the person's voice, it can record all sorts of things that are potentially impeding on their privacy. If you are not store else, you are storing it or if you are storing for too long, or if you let people listen to it and so on. So don't store anything or upload anything I said or that was said before the wait for it is detected. Just don't store data. For instance, transcription of user queries longer than needed. Allow the user to control what and for how long it should be kept and anonymized data before storing it, for instance, through that user information. So if it's just a transcript, it's it's anonymized. However, if it's a voice recording, someone's voice can, you know, it's like someone's fingerprint. It can be it can be faked, but it's actually very well linked linkable to someone's identity. So storing data security and all those things, of course, matter as hearings and relevant protection law, data protection laws like GDPR and GDPR. Very briefly regulates how people can access information about them, and limits for organisations can do with personal data. There are seven principles with four GDPR that I've listed here and GDPR, even though the, you know, Britain has left the EU is still is law in this country, although they are figuring out what to do with that. Right. I just wanted to mention this, so you should be setting up your voice flow accounts. I, I probably make an announcement by email and on teams about this as well. So head over to this, this URL, It's very important that you go there because it's a promo sign up thing where you have to create your account and input the coupon code. Otherwise it won't know to create you the right kind of account that you need. And you just get a basic account and make sure you use your Nottingham username and you just do your peers that or peers or whatever it is username at Nottingham without the email in it and that's it. And then so set up your account. Then we're going to be starting by following the labs this week and then coursework one. As I said, the new deadline is the 9th of December. See you all on Thursday.
