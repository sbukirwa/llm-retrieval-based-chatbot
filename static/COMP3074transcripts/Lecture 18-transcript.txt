SPEAKER 0
A team of human interaction. Today, we're going to be talking about user testing. So, Luis, I've got quite a few slides split into three parts based on Chapter six and seven from Castro's book with design. And and I'll cover the basics at first and I'll talk about early stage testing, and then I'll talk about different kinds of testing in the final part before we get going. User testing, right. You all have some kind of idea about what user testing is. So now for 3 minutes, discuss with each other for user testing for who is what. What are some of the things you may want to test specifically when you do user testing for Luis? Okay. So talk amongst yourselves for two or 3 minutes and then share with the class.

UNKNOWN
But. I was. What are some. What do you think?

SPEAKER 0
Okay. Yeah. So what are some of the things you might want to test for? When you test when you do user testing for voice user interfaces? What are some ideas you might have? Yeah. But I just wonder whether whether they can understand the responses of the grief. Yeah. Very good. What are some of the other things we should be looking at when we do user testing for voice? You know the ideas and there's a lot of things you can look up. There's a lot of cosmonauts you can get and they will all be correct. Any other ideas? Yeah. The testing of the users. Test the X and so test whether the. And the flip side so test whether the V understands the user and tests what are the different accents can be understood by. Yeah you know the thoughts yeah that's the kind of thing as well. So we assume that every time we. Like they say. Check the logic using something like, like I say, fake data out of something, scenarios that are supposed to test a different path, paths through your application, the different branches, so that all the branches or the logic sort of gets tested. So what you get? Yeah. Good for the error handling and recovery and test testing those. Yeah. Very important results. Yeah. Any other thoughts? Yeah. Testing budget. When you do budgets. Yeah. Yeah, that's another good idea. And obviously it depends whether we've implemented whether the voice interface supports conscience. Um, so yeah. And you know the thoughts. Okay, well, that's good stuff. Um, so here are some other things, some special considerations for testing, for user testing for movies. Um, yeah. Do users understand that they can talk to the system or what they can say to the system? This relates obviously to the discoverability problem we talked about on Monday. Um, and important with regards to that is prompt design. So how you design what the system says to the user so that it's understood, understood that the user can take their cues from what the voice interface is prompting with um, does the we understand the way people actually talk to it. Again, this is some things some of you may have already mentioned. And what are the kinds of things people say? And what are the words people use as well and who needs to recognise them? So there may be different ways of phrasing the same thing. So you know that how how do we design that in voice flow? How do we accommodate that in voice flow? What's that called again? Yeah, sorry. How do we help in voice flow specifically? How do we allow people to say the same thing in different words and utterances? Thank you. Exactly. So you specify the utterances that for for a single intent, you can have as many utterances as you want. So the same thing leads to the different, different ways of phrasing. The same thing leads to the same outcome. For instance, um, we had this example when we on Monday, when we looked at the family and how they try to invoke a family quiz on Alexa and they have been trying to use the keyword set set up from the quiz and it would respond to throughout, you know, had the designers incorporated different utterances, then it would have been able to respond to that. And then we can look at the dialogue management side of things and whether that's effective in your voice user interface. You know, all the things we we discussed when it comes comes to designing, um, you know, broadly following the design principles for voices and devices. The overarching question you might ask is do people get the things done with the device that they're supposed to get done with it Still a bit loud. So how well do you implement the strategies such as error, recovery, confirmation and immigration and so on? How well do these strategies that you've implemented actually work? So again, as you suggested, that testing error, recovery and error handling as part of what you can do in user testing for voice user interfaces. So why do we test with real users? Why do we even bother? So that's what I want to cover and talk about a bit. Talking about different practices of testing, thinking about what is involved in designing a study, designing an actual user study. How do you also define the tasks of the user study and in which order? What are considerations regarding participants? The data collection and analysis. So why do we test with real users in the first place? So first of all, this shouldn't come as a surprise because this is part and parcel of what we do when we do human centred design or user centred design is another way of putting the same thing. We do this because we need to find problems with the interaction with the with the technology early on in the process. Why? Because we need to fix them. And the earlier we find the problems or issues with it, the less it costs. So there's also a cost argument, you know, when you are working against a budget, working against the deadline and all those real world constraints that companies have that provide technology for people to interact with as products. Having problems with it obviously has cost implications. And later down the line you find these problems, the more expensive it would be for the companies to fix them. The other obvious thing is that most technology is designed to be used by and useful for people, right? So even if you, you know, might not have a product that is kind of user facing, there's most likely going to be some kind of business unit or whatever that uses that piece of technology. But of course, even more so. That is the case when you have something like a voice interface which is directly facing an end user and therefore you need to test it with them. People draw on prior experience when it comes to interacting with any kind of technology. That prior experience can help them or it can hinder them because they come with with preconceptions. You can only find those out by testing your voice interface because you as a designer and your team, you know, there's perhaps only a few of you coming up with the design of it and thinking about what utterances and so on. You come can bring your own preconceptions to that. So that is going to be not representative of the whole population, that is potentially going to be your users, your system. Therefore, testing also helps with understanding, you know, how do people actually talk to it? And more than just the people like your small circle of friends and colleagues. Testing helps improve your product as a result and therefore makes more money. So there's also an economic argument, of course, with always these things and you have more satisfied customers and so on and so on, and people have expert knowledge or local knowledge as well. So local knowledge in the sense of their kind of cultural circumstances, where they're from and so on. And that can also help you design a better voice interface, especially if they themselves are the users that you end up designing for. So what are some of the other sort of purposes we can what we test for, why we, you know, what are we doing when we're doing testing? So we want to measure. Typically you want to measure things. So we want to measure things that are a little bit difficult to measure, such as user experience. Right. This is kind of a concept. So we need to find ways of rationalising. What do we mean by user experience and how do we measure that? And we can measure more heart things more easily, like task performance. Performance is something that you can measure in terms of measuring time, right? So you can measure the response time. You can measure the accuracy. These are things you can really measure. And you want to understand the successes and the pain points of the system. You can typically do measuring in we we distinguish kind of two broad ways of measuring subjective and objective. Often that relates to things like ratings. So when we have subjective measurements, we ask people, how do you find this on the scale of and so on? And we have, whereas with objective measurements, we have things like time that you can measure elapsed time. We can measure, you know, success rate and failure rates and error rates and so on. So counting things. And we, in other words, have self-reported data. On the one hand side and on the other hand side, we have observational data or behavioural data. So data that pertains to things we can observe but doesn't rely on and people telling us about about it. This can be used to inform design decisions in something that we call a formative evaluation. And so typically that necessarily is something you do early on in the design process or later mid-stage as well. And typically when we think about human centred design, we think about an iterative design process where you do design, design and testing. You do several loops or iterations of that throughout the lifetime of the product. You don't just develop it and never test it and spend lots of time and money into developing it and then at the end you realise it doesn't work. That's a bad idea for the aforementioned reasons. There's a thing called a summative evaluation as well, which is at the end as as the name suggests. The idea is to evaluate the final, final version of the thing you're building. You may also want to do this user testing to report success measures to others who are implicated and interested in it, such as your clients or if you are writing research paper. This is what you use to talk about successes and failures. Or you may have other extrinsic reasons, such as, you know, reporting to your funders that give you money to do this work in the first place. So when we design and use a study, it's important to have a goal in mind. What is the goal or purpose for? A study. The same is true for research studies. You need to have a research question. This goal can be driven by your research questions or hypotheses. Or it can be more exploratory as well. If you don't have a strict hypotheses as such, you need to understand whether you're studying user experience, performance, or exactly what are you looking at. You need to define the tasks that you're asking the user to do as part of it. If it's task based, it could also, of course, be that you just you put your product out there and you let people use it and you kind of treat that as a test, sort of different kind of framing, which is perhaps doesn't involve a task as such that you give to participants. When it comes to recruiting participants. And some of you have some experience in that. You need to follow an ethics procedure. You need to have. You need to follow the principles of informed consent so that people understand what they're taking part in and how the data you're collecting about what they're doing is used and stored. And and that data protection goals are being followed in this and so on. And think about the instructions you give to participants. You need to think about Do you pay them for that? Think about sample size. How many people do you need to take part in the study? When it comes to data collection and analysis, you need to answer the questions around how are you collecting data and what kind of data you're collecting. Do you do interviews, surveys, measurements of performance and so on. How? And then you've got to think about things like what you're measuring. Does it actually measure what you want to measure? So is it valid? Are these measurements valid? How are you analysing the data? And you use quantitative types of methods, relate to quantitative data or to use data to analyse quantitative sorry methods, to analyse data that is qualitative. And when it comes to that, so let's talk about some of these these issues becomes the task definition. You need to think about how you design these tasks to basically exercise the parts of the system that you wish to test. So how do you do that? So clearly needs to be the task you're giving it to be relevant for your goals and what you want wanting to measure. And with various, you may want to focus on primary dialogue paths, features that are likely to be used most frequently, make sense to test those first. And you also may want to focus on areas of high risk. So if some of what is high risk mean in the context of voice interface, something like confirming a payment, you know, if it is high risk in the sense that you want to use this, confirm the payment, the money will leave that account and so on. So you want to make sure that these areas of high risk are tested. First, the task to address the major goals and design criteria identified in the requirements specification of the study. So writing the task definitions carefully also kind of can help with avoiding bias and bias in the sense that you may inadvertently bias your participants to perform very well because of the way you instructed them and you've given them kind of you you've given the key words away in your instruction, right? You say, can you test in this in this task, you will need to order a flight. And, you know, they just have to say order a flight. But if they use a different term like book flight, then you might not work. But you've, you know, you biased and towards using the correct keyword that you've intended for them to use, you need to describe the goal of the task without mentioning the command. That's what I just said. And for instance, you may want to say like the example we had with the feed me up, where we looked at, where we did our discoverability study, we asked them to please use this voice interface to order two dishes from the restaurant where, yeah, you need to be careful not to include the keywords. And so when it comes to ordering the tasks, we need to avoid order effects. There's a thing in psychology called primacy and recency effects. So if I give you a list of seven, you're most likely to remember the first and the last item on the list of seven that I've given you. So to do to avoid that so that if you people, if you give your participants the task always in the same order, they're going to perform best in the first task or in the last task depending for instance. Right. Or if it's about if it's about remembering, ask us. You ask them questions about their task. They're most likely to remember the first. The last task. So to avoid any kind of order effects, you randomise the tasks using what's called a Latin square design, where you have each task in every position as possible. And if you're also using conditions like, for instance, you're testing two different kinds of voice interfaces, you also need to counterbalance them because you're always testing, you know, design a first and then design B By the time you get to design be, they might be a bit tired or distracted. So you always you're introducing a bias because you have introduce order effects into your design. Participants You need to think about a whole lot of thing or lots of things that starting with that person characteristics like demographics. Does that matter to you? Are you trying to are you trying to get a representative sample of the population, in which case you know yourselves you are not a representative sample of the population because you're, you know, your students, so you are of a certain age group. So if I ask you to test my prototype, there will also be a bias. There will be an age bias, but there'll also be a bias towards being more highly educated and being good with technology. Right. Compared to the average of the population. So these things are important to think about. There's a thing called a stratified sample size of kind of way of getting a sample where you want to have, you know, ten participants from each each age group, for instance. So you stratify the sample in that way so that you get some sample that has certain characteristics represented equally across the sample. And when you get a representative sample, you you are then allowed to make generalisable statements for the population. You need a large number of participants for that until you need to to sample, take something like the UK. You also need to make sure different regions or participants are coming from kind of all corners of the country and ages, ethnicities, different socioeconomic backgrounds, etc. So you need to do a whole lot of thinking about these kinds of things if you want. If you want know, how many is enough, how many participants do I need? Well, typically if you are just doing usability testing, you start with a very small. And so Yaakov Nielsen's advice is that a test with five users or less is actually good enough to understand any issues that you might have with low fidelity prototypes and then you just do more iterations of the same test. So you can use five five users each time, but you do four tests more frequently. For fewer iterations of tests, you may need some something like 8 to 10 users for prototypes and 15 to 20 for finished products. So these numbers, you know, they're very manageable even for even for yourselves as as a single kind of researcher who might be thinking about doing something like this. Part of the master's thesis, for instance. So how do you collect the data? Different obviously forms, different methods of collecting data. Questionnaires, Surveys and interviews. Probably the most used ones where you've got to understand that with those methods you are gathering self-reported data which have certain biases within them. And there are lots of techniques to both collect and like structure the ways in which you wish you collect that type of such as semi-structured interviews. And there's kind of more open ended interviews and different kinds of questionnaires, different kinds of surveys and so on, different kind of standardised instruments that have been tested to be of a high quality. But you can also use like ask us is that is a survey that's ten questions about the system, usability scale, ten questions. It's been tried and tested as a way of analysing it, give you give you reasonably good results. For instance, though, making use of standardised instruments can make a lot of sense. There are tools that allow you to make your own surveys like Qualtrics survey and so on. There are, you know, there are lots of considerations. Guts. Those Microsoft forms, by the way, is one that allows you to create surveys straight on your office by one drive that you each have and you. Yeah, different considerations with different kinds of questions. Now, when it comes to different the different kind of data, when it comes to observational data and typically you need to put more effort into, into the planning of how you capture that observational data. You may need to instrument your application that you're building with logging functionality so that you log the interactions users are doing with your voice interface. You may also use video or audio clearly, you know, with a voice interface doing, you know, making and collecting data via audio recordings makes a lot of sense. As you've seen on Monday in the lecture, I've used audio recordings that we've captured as part of our research and going through and analysing this kind of observational data. Again, you can do it in a quantitative way where you count things about the data in broad terms, in the obviously statistics on the data, you can count errors, times completions, numbers of words, etc. You can also do a more qualitative analysis where for instance, you try to analyse using, for instance, conversation analysis. This is what we do in our research to understand the interactions with voice and faces. And when it comes to data analysis, there are really myriad approaches, quantitative and qualitative. Quantitative. You have the kind of classic statistical hypothesis testing and which is typically used inferential statistics to prove or disprove your hypothesis depends on the kind of data that you've collected, what kind of statistics you should run on it. You also have different kinds of algorithmic or mathematical kinds of analysis that you can run. Now, some of you have well, you all have obviously encountered precision and recall and F1 scores as part of thinking about as part of the and of benchmarking. And then Jeremy talks about that is that is relevant for testing your chat both qualitative on the qualitative side, there are again thousands of different ways in which you can analyse qualitative data. Somatic analysis is one that is is quite popular amongst students, but also amongst researchers which allow you to find themes in the data. So if you do interviews, you really want to find out what are the issues, what people actually talking about within that. So the massive analysis where allows you to do that conversation analysis and you've seen me do a little bit of that on Monday is a way of focusing on the talk and interaction and you transcribe the data in a certain very detailed way, following certain kinds of formalisms that allow you to do this type of analysis. Interaction analysis is similar to conversation analysis, but also includes nonverbal interactions, multimodal types of interactions, and, and a lot more. So in part two, I want to talk about early stage and usability testing and a little bit more detail. So when you do early stage testing and and what you do with in voice interfaces in particular, this is the time when you test concepts of your voice interface and the dialogflow. So you're kind of promising that you have envisioned in the design process. There are different ways of doing that. Jeremy covered this in one of the lectures on basics of voice interface design table reads. So sample dialogues that you can, you know, write the kind of back and forth between voice and face and the user. You can kind of that help you to think about how the voice interface should respond, what the user might say, and actually doing a table read, you know, like you would do when you're practising for a play. Sounds a bit silly, but actually if you read it out loud, you know, it's, it can sound odd. All of a sudden you realise, well that's not really how a person would say that. So it gives you a way to rephrase the way in which a person might address your voice interface. And it's also good for spotting if you have too many translations with the same wording. So for instance, if you always have fun. Thank you. Thank you. Thank you. Thank you. This is the voice interface. Very kind of responding. So it helps you to kind of find repetitiveness and things like that. You can also look at mock ups and get some initial reactions of those was it was testing is a kind of different kind of testing that I mentioned as well in the previous lecture where like in the story The Wizard of Oz, there's actually a human behind the curtain, so you don't have a fully functioning voice in face, but it's a simulates a fully working system and it allows you to test certain things again early on. And if you're interested in certain concepts, you may not need a full, fully working voice interface to understand it early on. And so it's realistic, but it's much cheaper and quicker to produce and implement and test with than a fully working system. That's why you do it. And you can also use it for things like elicitation studies as a specific kind of study where you really try to understand how are people addressing the system. So if you have a very specific voice interface that allows you to operate specific machinery, for instance, that an engineer might use, then the language they might use to address the system might be very specialised. And this kind of wisdom testing allows you to elicit what that language is that those people might use. So, um, a few more points about Wizard of Oz. There are some challenges here, which is how do you actually create a realistic simulation? One way which you might do that is through a strict protocol where you might call the Wizards rules, because what you don't want is to create a system that like is basically just a human being that can understand everything the user says to it and just responds, because that doesn't help you to design it, because it doesn't translate to actually when you build a voice interface, right? So you need specific rules so that it is a realistic simulation of the voice interface. So you have you can do that through if rules, for instance. So if the user sets the right keywords and the request is complete, the wizard produces a successful response. Otherwise it produces an error response, prompting the user to say the same thing again, but in different terms. For instance, if. So the challenges with these things is that researchers that want to use the Wizard of Oz method, they need to produce this believable fiction, we call it. Right. It's basically a performance, but it needs to be believable and it needs to also be transferable to actual design as well. If you want to know more about this, I'll paper on The Wizard of Oz goes it goes into more detail on that. So when it comes to usability testing, you do that when you have a working app. So all the testing I talked about up to this point is coming before that. Now you've come to the point where you have working up. Then that's when you can start to do usability testing and you may need to do things like create fake accounts for this. You part of the system may still be hard coded such. So not it's not fully implemented, but you know, most of it is there so that you can start to do usability testing. You can do that to test the dialogue flow and the ease ease of use of the system. And so it's not usability testing isn't generally aimed at recognition testing and is usually done in a lab setting. So a lab by lab setting we mean like a controlled environment where like your office, where you invite people into the office and violence to interact with each with a prototype. And obviously the point of it being that you can record the interaction and learn from any issues and improve your prototype. And it can be done remotely, of course, and nowadays as well. Clearly, you can do that through a team's called a Zoom call and where people have and has the has the advanced people don't have to travel with is cheaper and so on. But and you know may also be happy that they're in their own environment. And but yeah as gravity got downsized they you know don't control their environments. You know they might have their children screaming in the background and so on and that might hinder and affect the way they perform as part of usability testing. Usability testing allows you to measure both performance of both the kind of objective observations as well as the experience, the subjective experience. And both this is is can be important for feasibility testing. So it allows you to focus on the likes and dislikes, but also the the hard things around that you can measure like task completions, errors, recoveries and so on. Yes. Here are some key measurements that you might wanna focus on with testing voice interfaces. These include accuracy and speed, cognitive effort, transparency, confusion, friendliness and voice. Some of the specific things in there that are specific to voice interfaces. When you do the analysis, you want to focus on pain points and you want to focus on recommendations. So what you learn from the usability testing that allows you to improve the the prototype and how to fix it. When it comes to usability testing. Clearly the setting really matters a lot. For instance, are you in the lab or are you in the wild, as we call it? And so in a real world setting, such as a car, right. And with voice interfaces being hands free, ice free cleaning of cars is a very bad of. But it is an environment that lends itself well for voice, interface, voice and voice interaction. What if you do in-car testing? Clearly there are. And the question is, would you have a car simulator or is it the real thing? And if you are in the in a real car, you know, there are some other risks, of course. What's the use of users actually driving at the same time as testing your interface? Need to make sure that that's safe to do so and they don't crash the car. I'm here. This is a sort of prototype approach here. Well, it's just a button that's taped to the steering wheel to trigger the voice interface by pressing the button. So you may do, obviously, with things like prototypes, you may do some creative use of sticky tape and DIY skills maybe needed when you're in the home. You need to balance the setting of that with potential intrusions, with with the benefits of being in the home as well. So generally we think that field trials are really useful because they allow you to test the technology in the setting that you're designing it for. So if you're designing for the home, at some point you're going to have to go into the home and test of that. And also, it shows the messiness of technology in context. Think of our Lexus family that we had on Monday. And, you know, this is really what it's like when these technologies go in into the world and are embedded in the world. So at some point, it's is needed to understand that. And the phrase we use is ecological validity. And so it's that is one of the key qualities of good research studies is that it is ecologically valid. So it takes place in a context that is relevant for yeah, for the context in which that panel is designed to be. So in my final part, I'll go over pre-release and pilot testing different. So kind of later on in the stage of development, developing your voice and face, you will then do. You're ready to release. You think you're ready to release your product. So there are some specific tests you can do and then you can you may want to you know, you don't typically you don't release your product to the world. You release it to a small, a smaller portion of testers, and that's called a pilot test. So in pre-release testing for voice interfaces, you can do this type of testing called dialogue traversal testing. I think this was actually something that one of you mentioned right at the beginning, which is this is where you really test in complete detail in every single branch and every single Yeah. Every single branch in, in the conversation tree that you have implemented for your voice interface, all the transitions, all the error problems, health problems, anything else that could happen at any given state in your dialogues. So you can also, as part of this, try out of premature answers to test the behaviour in response to a recognition reject. So say something ungrammatical to it and see what happens. In other words, try silence the test. No speech timeouts. Try multiple successive errors with dialogue states and so on. Another thing you do at that at this stage is low testing. So how well does actually perform when the system is under a lot of stress for a lot of different concurrent user sessions, you know. So getting overloaded and faltering. As a as a result of that. And you can automate that type of testing with third party services. When it comes to pilot testing, you are releasing it to into the wild, let's say, for a small number of people. And this isn't just about recognition, accuracy. And this is also a a good moment where you start to talk to different departments within the company that might be implicated in the voice interface you're making. So this is where marketing, sales and support and other stakeholders can actually have something to offer. For instance, you may want to understand that 60% of the users who are starting to make a hotel reservation are actually completing it. And this is something even understanding of pilot testing. And you can, you know, here are some other examples of the types of things you can understand on the basis of pilot testing, key measures of success that you should define as part of the pilot testing that allows you to Yeah, to metric to measure the success with regards to these, these aspects that you can actually measure. So task completion rates. So if you know what you're offering is booking a flight. How many of those who start the process actually finish up the dropout rates or the inverse? So how many are actually dropping out throughout the process that you are that you have given people to go through? How much time do they spend? Where do they barge in? So when while the system is producing output, when and where do they barge in? So, so so yeah, so these are these are different types of things that you can go through when it comes to testing. A pilot stage is important. Part of that is logging where without logging, obviously you're not you're not actually recording anything. So it's about recording the types of metrics of measurements that you have to find. This allows you to log the results of the recognition, so the output in your system, what it is here when he's a spoke, this allows you to perhaps also look at the best list that the air song produces. If it does produce it, it's okay. This is also where you might want to collect the audio of the user's utterance and each state. And if the recognition failed, what did it but it matched to something else? What did it actually match to? So what does it sound like? What are the sorts of mismatches that are occurring within the interaction? So the intense that a firing for for a given utterance, what kind of errors are being produced and so on. So nearly at the end of the lecture now the with the the audio of the, the, the user interaction with your voice, you can transcribe it and from your log data and you can do things like examine the domain versus out of domain data. For example, for, for your in domain you have correct access and where the recognise that return correct answer for my favourite colours red, blue returns red but then use that for success and false rejects. So false positives and false negatives. In other words, the way they recognise the returns, an incorrect answer and where they recognise that rejects rather than turns an answer, even though the the the request the user makes is correct, then you have output domain types of utterances which are correctly rejected instead of basically call the kind of situation where the user says something irrelevant. Like I think I would like to book a hotel. So no matches is correct. It's the correct behaviour and you have a false except where the recogniser returns an answer that is wrong because the input was not in the grammar. For instance, I read the paper then gets matched with red, so it's a false accept and so understanding what types of errors your system makes. We got to. This can be insightful. So in conclusion. You should be testing. All the way from early onset to release and beyond the release of your system to get the user experience right. So you get the right language and the flow and different types of recovery, but also to optimise the system performance so that the way in which your system responds recognisers and the latency and things like that are actually acceptable and to monitor how your app is doing and allowing the designer to improve the performance. And overall, this allows you to ensure that you've, you've talked to stakeholders throughout the process as well. Help them to define your success measures, task completion rates and you're logging the right data and so on. Tracking of speech failure points. And this all together allows you to make improvements as you go along an iterative way. The best possible product. Okay. So thank you. We have a quiz next week and of course, with one view and you should be creating your voice flow accounts if you haven't done so already. Okay. So see you soon.
