SPEAKER 0
So in this lecture, we're going to still be quite general. I'm going to talk about natural language processing. And I'm going to be in a first class. I'm going to talk about what it is. And in the second part, I am going over the points. In a second part, I am going to talk about to introduce the concept of an MLP pipeline, which is something that we'll be doing with into that a little bit tomorrow and then from next Friday onwards. So first, a quick overview of natural language processing. This is a topic which is really reserved for the graduate courses themselves. So we won't even scrape the surface off of it. First, a quick warm up question and just just take a minute and think, can someone give me a quick estimates of how many eyes are in this room? Anybody want to guess?

UNKNOWN
You want to play the price? Well.

SPEAKER 0
That's the question, isn't it? Just as how do your best guess was it was your own assumption. And then tell me.

UNKNOWN
How this works.

SPEAKER 0
Here. And so I can speak other instruments.

UNKNOWN
I understand diamonds now better known as.

SPEAKER 0
Okay. That's what we saw for time. Yeah.

UNKNOWN
You know, you must give us serious stuff. Nasty process to stake out your bones.

SPEAKER 0
Yeah. Oh, so we have, like you said, 40, 50, 80. Sorry. Working on this. Okay. I think I have no estimate on the winner, but I get your points. Basically, in the most students, you say a bit more than the most students. Anybody else?

UNKNOWN
John, I'm surprised.

SPEAKER 0
Right. I mean, you're getting to the point that I was trying to get at basically both of you, which is that nowadays. Right. Most smartphones have actually quite a bit of meaning inside them. Right. And you mentioned Siri. That's an excellent point. But it's not the only thing. So if you have that sort of engine, which is like what you consider to be Siri, but that's faint, contains its own kind of subsystems to make a speech. And then when you take a picture, your camera has some AI inside to correct pictures. And then when you start that into your photography app, then there is some A.I. recognising images, right? You should go into your photo app. You can say like, Oh, I want to search for cats, and then it's going to get you pictures of cats. Well, the phone doesn't know what a cat is, right? There's a some system specialise on that thing to do with it. If you use things like Google Translate, then you also have a translation system. In Eminem's video. If you sum up a map, then there is a very complex route optimisation incitement. So basically you can think how animal phones and computers and tablets they are and then multiply that by maybe 5 to 10 different subsystems always running and trying to basically get something for you and just not even counting the fing m individual smartwatches or it's like converts or whatever you have. And a certain number of these, you know, always language, right? You have a translations metacognition, you have to some extent image cognition, deals with image with text as well, because if you run the search for cats to understand what a cat is for, there is a step in there. And the reason for this is because language is at the key of interaction. Like when you interact between people speaking or sign language or writing, when you interact with machines. And that's common on interfaces, voice interfaces, interaction between machines, stuff like networking protocols, all languages, the formal languages, but it's two languages. The way you interact with your cats on your door is also a form of knowledge. So language is really the key of how we interact with almost anything. And now, thanks to the recent advances in machine learning, deep learning and all that, we brought natural language back as a natural way of interacting with computers. So you wouldn't know that because you are very young, but used to be that you couldn't speak to the phone, you had to type things. It was time. But nowadays you can talk to it and most ask can be automated that way. I want a precise bonus. In natural language, that means languages, which naturally evolved over time. So this is in contrast to something like Pacman or C++, which are not natural language to artificial in full warning, which is designed for specific purpose. The field of natural language processing tries to merge insights on many other fields. So at its core, you know, language you can think of as do some linguistics. And that is true to some extent, but it's very subtle parts of linguistics. Mostly it is things like computer science from the computational aspect of it. We want to automate things with a machine. And I follow autonomous aspect. Right. We want to be able to extract insights from the text but want to generate the text. We want the thing to be autonomous as domains of fuel in an upcoming natural language understanding. And then you focus more on finding insights. And that would be the path we see. Hey, Siri, what time is it? And the natural language generation moving forward with Siri answers. It's one 3119. It's not ready yet. So these are the two steps. But usually when we talk about energy, most of the work is done on the understanding and listening parts because that is the hard part, so to speak. That's what I'm standing focussed on, analysing natural language. You might have noticed that we tend to generate a lot of data and a lot of it is text. So this stats were from two years ago. Haven't changed yet, but I'm assuming they can only go higher, right? 5 million tweets sent each day. The size of English. Wikipedia is five terabytes and something. And as humans we are system hoarders of data. We store everything. We keep it. No mic on that any later. Which is true. It is handy. You can actually get insights from that text data. You can figure out how people feel about a movie or product think or sentiment. You can figure out what people are talking about. What are the topics which are in the main public discourse. You can figure out how to translate sentences without having to have someone speak both languages going, and then that in maybe you can figure out how to make people vote for the candidate you want. The goal of propaganda. And we're also using natural language, understanding and so much more. It is a rich field is used in many places, and we'll talk about that in the coming weeks. Natural language generation systems, on the other hand, focus on generating that language. So you might have heard of things called chat bots. Right now, every time you go on the website there is this very annoying pop up on the bottom right? Who says, Hey, can I help you understand what's going on at one of my store? Do you want to buy something? And that's part of it. But the other part of it, when you I don't know, I think the NHS as check points as well or some counselling websites have channels. It's it's, it's everywhere. You can also generate data, generic text from structure and data. The two main ones are financial reporting and weather prediction. So there are a lot in a lot of newspapers, websites. There's another three small news items about the stock market, for instance, which are actually written by us because you don't really need a human to do that. It's very simple. You get the structured data, which is, oh, how the Apple shares do this day, and then you just turn that into sentences and then you report that on the website. So it's faster, more systematic, and it works. You can also generate text from each other, even much newer thing, but getting some traction now. So for instance, you could have a video or an image and you can have I take this as an input and then generate a caption describing what's happening. And that's great for things like accessibility, right? So if you're visually impaired, then you can have it describing what's happening in the picture. And then this way you can access some of that media that would have been inaccessible before. Right. Okay. So I'm going to add a few slides about basically the evolution of an LP. This is not very detailed, but just to give you a rough idea, Ernie and LP Systems focussed on rule based systems and the way rule based system work is that you have an expert writing rules and saying, you know, if you see this word, that means someone is happy. If you see something mean, someone said, you said this won't be too part to the police and things like that. And then you have to think those rules and then programming and yeah, basically extra system out of those rules. And then we'll just send the system back to the experts and we'll just test it and say, This is terrible, this is good, whatever, and change this and this and so on and so forth, rule by systems, although like making fun of them right now, actually very useful. And they use quite a lot. You can use things to like extracting elements from text, like dates, names with things you must have seen over going to expressions and then you can do classifying text, using stuff or lexicons, which are just lists of terms. And then you can use this and then your immense programming not know how to kind of build reports or do whatever you want on it. We still use our systems these days because they're very easy to build. You don't need any data to start with, and they're very fast usually. Then we ended up having a lot more data and compositional power. I mean, you want to do math and this is the the era of the classical machine learning algorithms. You might have heard things like to machines. This is in trees, logistic regression, linear regression and stuff. That's those classical machine learning algorithms are still used today. If I'm good, that good train off of like, oh, it takes half an hour to build and you guys have good performance on what you want to do and things like that. And it built in from this an obsession with benchmarking. So as I know if you run off Cardinal Andres Echo Line and it's all right I'm surprised at what he does but more than this cardinal is a website basically with machine learning computations where you just basically submit your algorithm and then it benchmarks it against a lot of other other items. And the winner gets, I don't know, $10,000 or something. Oh. So it builds kind of obsession with benchmarking, comparing, and then a new obsession with building a very strict and full evaluation methodology for all systems based on data. And that will make a lot more sense in a few lectures. What I'm going to talk about this must have anybody here has at least heard of deep learning who hasn't. Oh, okay. Then there was the drowning craze. We have so much detail. We have so much competition. You know, jeeps are cheap, and, uh, and. And that's it. And this is the current state of of energy research. Right. A few comments of feeling of an LP you might encounter if you open a textbook. And I'm going to talk about basically what I focus on. And it's just to give you an idea of the range of uses that an opiate or them have. First of all, in the one of the simplest one is oxygen mining or cinema, and that is this if anyone reading and for example, they will be reading a movie review and acting with someone liked all of these things. Now this is very useful. For instance, if you're doing product market research and then you're trying to find out, well, should we fund the next avatar everybody loved? That's fine. It makes a lot of money. Whatever. Another one arguments, tense diction. So for given pair of sentences. And if you want to detect whether A's in agreement was B or disagreement was be, that's all very common field extra entitlements is a bit more difficult. Summarisation a very common one as well. So you encounter summarisation on a basis issue type a question in Google and then it's without an answer above all webpages. Google did not figure out the answer. It extracted this from a page, which is, you know, in the in the documents that you're seeing, and then it's just showing it to you directly because it thinks that's what you want to see. So this is one of the most common uses of summarisation. A question and certain kind of related. If you haven't used Siri or Alexa or assistant or whatever, this is what the question answering is. Basically, it has a used in terms of facts and things that you can answer. And then when you the next question, you just finds out what you want to know and spits out the answer. And that's why you should repeat the same question is going to repeat the same answer because you always match the same information retrieval. I must move on here. It's useful, maybe for or some equivalent. I don't know if there's anything people here you should know, but it can be. It is possible. Conversational user interfaces. So those are the very annoying checkbox I was mentioning before the call on all the websites and asking you what they can do to make you buy their stuff. Now another quick question, just as a as a reflection point of how what common and systems do you think you interact with? So I cited more for this. That's kind of an obvious one, but I'm a bit curious to see if any of you. I've used anything else and erased answers. Don't any points? Yes. I'm trying to contact sports. Sorry to text. Oh, yes. Yes. We should explain. Yes, yes, yes. That's all right. Come with us. Yeah. That isn't any of you use to thing the speech and the you generating the text that is most likely considering the audio that was received. Yeah. Yeah, exactly. That's another one. Anything else? Sorry. Google Maps. That's just one of the ones that's very interesting. One, because it's not mapping text to anything. It's mapping text to geographical entities. And it is trying to understand what you're referring to when you say, I want to find, I don't know, Aspire Cafe. Right. Considering what's the closest thing, you know, what time of the day this is at 1250 then responding to this or something else? Yes. So to be the photo ops gamers, do they use text?

UNKNOWN
All right. Okay.

SPEAKER 0
You would us if you would have said that, for instance. That's absolutely like a usage of an LP. And when you hear use. Oh, sorry. Sounds. Context. Optical character recognition. That's also part of it. They'll be sort of. Yeah, I was kind of open. When you scan. What? I was thinking that since some of you. Yeah.

UNKNOWN
Just cause the systems. Right.

SPEAKER 0
I don't think that that counts as an LP. I thought you were.

UNKNOWN
Going to say I was going to ask you about substance abuse.

SPEAKER 0
And the effect of a four year process for you were going to refer to, because that's something which is true, is that in some cases I know who will pay them. It really doesn't. Where? When you receive a. Wasn't there a bill in your emails? It scans it and then kind of remembers it. Right. So it's telling you you have a bill you can send there because you receive these kind of PDA, which is very boring. Oh, yeah. So as anybody heard of Valley Emergency Generation. Yeah, well, that's another use of an LP, right? Because Dolly takes a natural language description of something and then basically tries to match that to a set of images. So anyway, as you can see, right, that's another MLP thing that we are using on that basis. There's more than that. I don't have that minimalist, but if you think of that, you'll find that it's more common than you think. And general question and that's more of a Hendrix kind of thing. How happy are you all understand you and and if anyone speaking to understand them. No, no, I'm already there. Right. I'm going. My kids school assistant to the Mets hit and miss. But I think we are making progress compared to previous years and previous decades, and we are slowly getting there. Yeah, right. So. That was the letter, General. You know, I know people if you must have students and you want to do more work and help you come see me and I'll find you the right direction. But this is just a general history lesson. Now, I'm going to talk about something called the A.P. pipeline, and that's going to be something that's going to follow you for at least until August one. So pay attention some preliminary notions to understand this. When we talk about documents rights, you might think is like a physical document. But when we say document, what we mean is a unit of interest. So if you making an LP system and you want to analyse tweets, then a document is entry. If you analyse books, that document is a box. If you want to analyse the content of a book, then maybe a document should be a chapter, always should be a paragraph or you should be one. Doesn't really matter. It's a unit of interest. When we have these documents and we put them into a collection, we call that a Corpus Montana sense. You may find this two terms kind of equivalent in the literature, depending on whether an off year person or linguistics person wrote it. And the main idea is that it's about a framing on the one thing. The eyes, of course, could be anything, and a document can also be anything. This is an overview of the pre-processing pipeline and AP, which I actually wrote to my producers, and I've been using it since then because I like it so much. So as you can see, right, we are starting from an input. So in this case, a small piece of text from Wikipedia about the 40th time to cover them. Then we are going for a set of steps, basically, which helps us kind of cut things off and then dig into different words and analyse each word so that we have something which can be fed into an AP algorithm at the end. So we start with tokenization and annotation and the motivation of stemming the heart filtering and then whether some of those steps can be skipped, a donation can be sometimes skipped and then motivation can be sometimes skipped and so on and also skipped. But tokenization is the only one that's actually mandatory because composition refers to actually cutting the text into tokens. And if you don't do that, then you won't be done anything. So let's go through each of these steps one by one. A document is made of tokens, hence the name tokenization, and those events can be of arbitrary length. And that depends on what we're trying to analyse. So for instance, if we are trying to if we care about phrases like once you get two bits of text, then tokens could be the dog run in the country. So that's two sub phrases in that sentence. If we care about words, then obviously we're going to put that into words that don't run and have to change them. If we care about cinemas, let's say for instance, we want to analyse syllables to make a poetry generator which makes sense in the right. And the check is the same only for chemical characters because we want to make a spelling correction algorithm, right? Then we just put them into characters d, h, e space, d or g space and so on. I think you get the point. We usually care about words, but sometimes we can. Of pairs of words. Sometimes, but we care about phrases. It really depends on on what we are trying to do. The next step is annotation and by annotation what we mean is that we want to enrich all those tokens with what we know about them. Usually we do it with support of speech after speech being right of the grammatical function of the word in the sentence. So as you might guess, part of speech depends on the language, and it's used right after speech in English, French or Chinese would be very different. Also, even within languages, you will see that there are different parts of speech classifications. So it's it's a very complicated thing. Thankfully, you don't have to understand the the speech. You can just use it. Right. What we use volume speech for is that sometimes we won't help disambiguate meaning using the grammatical context. So, for example, in this sentence, the sailor dogs the hatch, and if you just took dogs separate from anything else, then you would think immediately like this is referring to the animal and you just classify that with the rest of the animals words. Except that is not the case here, because here in dogs is the verb, which means to close the door and secure them in a class position. So you see here context, knowing that it's a verb helps you actually understand the meaning of that word in the context, which helps you down the line in an apartment. Next step. You call it usually more standardisation and usually refers to one or two things, which is either Amakhosi or stemmer. So as we know, words are in during language doing, doing, using. Sorry. So for example here when you have a verb thank be right. You don't say, I'd be happy, I'd be hungry. You infect that, which means it's modified based on the rest of the sentence. You say I am happy or I am angry or is happy or whatever. And the goal of standardisation is to remove this infection. You want to know what is the code word here? Now I'm going to confirm that is referred to by the infected now. So if you have something, which is how you want it to match to the same word, if it's singular, if you have dogs and dog in the same sentence and they're both nouns, you want them to be together. So the way we do this is either anonymization or staring. When we limit these things, we are reducing terms to dilemma, which is the dictionary form. And the way it works is that you have a very convergent algorithm that goes from each word and then based on the path of speech and the word itself says this is referring to the verb, or this is referring to the non dog, or this is referring to whatever. So that's a very complex algorithm. And the other one is stimming, which is what you see in terms of the horns stem. And this is basically a very symbolic or then that just cuts off anything that looks like it's an infection. So it would give you results which are going to be we're going to sound terrible like not that words, but for the sake of the algorithm, it's more than enough. So an example here limitation, one that dogs, dog owners, dog and thunderstorm joint is not racism, but there is on the same word. So it produces very consistent content. But you have this big dictionary thing it's always looking at is very small. And also it requires to be part of speech because it needs the context of the sentence to grammatical context to do its job. On the other hand, extending would map programming to programming programmers, to programmers, to EMS, which is obviously not great, but that's a bit of noise, but at least it works close enough that it actually good enough for the application. So what is on the horizon exists, but it's not faster. And sometimes that's what you mean. And the way it works is just basically you run the bunch of transformation on each token to get that result. Right. Filtering, I think, is the last step. Yes. Not all words matter equally, as you might have guessed. Some words like minutes, for instance, are not really useful for any and not humorous. Like the the talk, for instance. Some of the best is actually give you an information. And what we want to do when we feel and if you want us is that we want to have the smallest model possible that can do the job. And that means removing this kind of noise. So the way we do this is that we have this list of words in a very language called a stop list. And the words all these words. I'm not sure what acronym and any word which is in this list usually gets removed from the text during this last ten. We can also field the words by frequency. So words which are too frequent, which means they are in all documents, are not very useful. And the words which only a few once you really are not use can either. So be credible and we'll see more about this in the coming weeks. This is just like an introduction to give you get acquainted with is the vocabulary you're going to use. And finally, and I mentioned this a bit before, sometimes we care about sets of words and not words himself. So, for example, Big Ben or Empire State Building would lose all meaning if we isolated every single word separately. Right? And then the mind of a dumb thing and in a million something. But if we look at the pair of words and we know what he's referring to as our friend, think Dan's referring to the big building. Right. And in that case, what we do is that we actually extend our thing. And so I'm just looking at how it works. We look at pairs of consecutive words. We call that engrams and unique from being single words by grammars, of being, you know, pairs of once a three words try grams, triples, and you get the idea. Right. And I think I went to this lecture much faster than I thought. And here I was moving on. So reading to do, because I know is one thing, is that students are doing homework. Right. That's driving more paper. Well, I think a lot of I think this is paper called Kleiser. And this is a very important paper because it's one of the first kind of, you know, mainstream and maybe systems and used it was in 1966. I mean, you weren't born. I wasn't born. John wasn't born either. Right. And the stuff that we're doing with computers that are less powerful than your Apple Watch is incredible. And you don't need to read into details. Right. But you should look into Pdaf and have a look at the way they're using transformation rules, text to take the inputs text and then turn that into a into a very convincing output text to imitate an actual person. It's very interesting. You don't have to read the details. It's it's more of a point of interest. And I think that sense for now I'm going to yeah. Should recording the questions on is any questions. Look at me. Just a few more minutes almost to the end, Lori. And any question about. Oh. Yeah. Yes.

UNKNOWN
So what that mean? Obviously, something wrong is going to be read by line.

SPEAKER 0
It is not an intel based system, but it does have an office system for the spelling corrector. And your Microsoft Word is yet another kind of small A.I. embedded into your computer that's telling you, you know, this is wrong. This is wrong. This is wrong.

UNKNOWN
Yeah. I mean, that's the basics. Like what? I think.

SPEAKER 0
I'm sorry. I can't hear.

UNKNOWN
You, and I'll give you a piece of that. I mean, I think I know what.

SPEAKER 0
I want is in the.

UNKNOWN
Background. Yeah. Yeah. My plan is it's not done yet.

SPEAKER 0
It's almost the end. Just, you know, calm down. I'll do the. You know, but. Yeah. No, I mean, you're right that it is in pieces. And you said the to do work. Spelling correction in Microsoft Word now, for instance, is very complex. The one we used to have like ten years ago was just know random like dictionary search. But the one now is actually quite a beast comparatively. Yeah. So you went in there first. So the oversubscribed. I'm sorry. I really missed this one.

UNKNOWN
I wasn't a boxer.

SPEAKER 0
Okay. That's one for the. Know this person. Oh, this is no part of the process. And it's just like an additional not just sometimes we don't actually need the nice eyes on the horse. We are taking them to into pairs and things like that. Yeah. So like, yeah, you're going to see more of this in the labs because learn to deal directly with this. Uh, yeah. Sorry. Would you ever.

UNKNOWN
Do anything more fun with the meeting? Yeah. Um.

SPEAKER 0
Another session before the election? Yes. No. Because when you limited's rates, if you are actually losing, you're losing the government through conflict. Because since you remove the section right, then you remove stuff which helps, you know, what's the role of that word in the sentence. And you need the practice speech to do that in my position. So you can have it before That makes sense, right? Because it is for the parts of speech that don't matter. These are noise that, oh, this is not a verb. So he needs to be limited that way. This is the noun. I think that way. That makes sense. Yeah.

UNKNOWN
Okay. All right.

SPEAKER 0
Any. Any other impressions? Except. Yeah. Oh. Semi. Semi. Okay, so the motivation uses a dictionary, so it's always going to give you the dictionary form of a word. You might be wrong, he might be correct. But the word you guys is is always an actual word which exists. And for that reason, it takes a bit of time in making this other process to analyse and find out what is the most likely dictionary form of that word. On the other hand, the wisdom works, right, is that you have a set of rules which says or is if this things ends with an S cut, the s if you cut the S, and then switch to the next step, and then if those two ends remove one M. So it's a bunch of rules which acts as a correct animal. So it just looks at a set of characters and applies a set of rules to transform it and get you something, which hopefully is the code for the dictionary form of that word.

UNKNOWN
So this is as well as saving time.

SPEAKER 0
Sometimes you don't care if you make like 85% of your fan making mistake because you can process twice as much data in the same amount of time in the end. And all the shortcuts we take in machine learning are about saving time. When we cut off words because they're most frequent enough, we're also trying to save time and you just trying to make the process as easy as possible for the algorithm at the end, because getting doing more in a set amount of time is extremely valuable. You know, sometimes he thinks, well, maybe not these days, but sometime in the days and weeks to train them well. And if you can't cut that by five days, just very nice. Yeah. And also sometimes it doesn't matter because if you have a word for doesn't exist and maybe you don't want it. Yeah. Yes. Yeah. Yeah. I'm sorry to hear that one better. Yeah. Really? Oh. Well, by taking data from all around the web, don't worry. That's going to be part of the lab. Right. So I'm still finishing up some detail in the lab, so I haven't released it yet, but it's always tomorrow from 4 to 6. Best time. Yeah.

UNKNOWN
Come on. The lab? Yeah. I mean.

SPEAKER 0
You know, everything.

UNKNOWN
Is always on the horizon for the National Science One session, so, you know, everything.

SPEAKER 0
Always except the quiz. That's going to be during the month for. Right. All right. You know.
